# BFCL (Berkeley Function Calling Leaderboard) é›†æˆè®¾è®¡

## ğŸ“‹ **è®¾è®¡ç›®æ ‡**

å°†BFCLçš„Function Callingè¯„ä¼°é›†æˆåˆ°Natural Nichesçš„fitnessè¯„ä¼°ä¸­ï¼Œç”¨äºè¯„ä¼°æ¨¡å‹çš„å·¥å…·è°ƒç”¨èƒ½åŠ›ã€‚

---

## ğŸ—ï¸ **æ¶æ„è®¾è®¡**

### 1ï¸âƒ£ **è¯„ä¼°ç»´åº¦**

åœ¨ç°æœ‰çš„GSM8Kè¯„ä¼°åŸºç¡€ä¸Šï¼Œå¢åŠ BFCLè¯„ä¼°ç»´åº¦ï¼š

```python
fitness_scores = {
    "gsm8k_accuracy": 0.75,      # åŸæœ‰çš„æ•°å­¦æ¨ç†èƒ½åŠ›
    "bfcl_accuracy": 0.82,        # æ–°å¢ï¼šå·¥å…·è°ƒç”¨å‡†ç¡®ç‡
    "bfcl_ast_accuracy": 0.78,    # æ–°å¢ï¼šASTè¯­æ³•æ­£ç¡®ç‡
    "bfcl_exec_accuracy": 0.85,   # æ–°å¢ï¼šå¯æ‰§è¡Œæ€§å‡†ç¡®ç‡
}

# åŠ æƒæ€»åˆ†
total_fitness = Î±â‚ * gsm8k + Î±â‚‚ * bfcl + Î±â‚ƒ * sparsity
```

### 2ï¸âƒ£ **BFCLè¯„ä¼°æµç¨‹**

æ ¹æ®[BFCL Leaderboard](https://github.com/ShishirPatil/gorilla/tree/main/berkeley-function-call-leaderboard)çš„è®¾è®¡ï¼š

```
è¾“å…¥: Function Definition + User Query
  â†“
æ¨¡å‹ç”Ÿæˆ: Function Call (JSONæ ¼å¼)
  â†“
è¯„ä¼°æŒ‡æ ‡:
  â”œâ”€ AST Match: è¯­æ³•æ ‘åŒ¹é…ï¼ˆå‚æ•°é¡ºåºæ— å…³ï¼‰
  â”œâ”€ Exec Match: å¯æ‰§è¡Œæ€§åŒ¹é…ï¼ˆå®é™…è°ƒç”¨ç»“æœï¼‰
  â””â”€ Overall: ç»¼åˆå‡†ç¡®ç‡
```

---

## ğŸ’» **å®ç°æ–¹æ¡ˆ**

### æ–¹æ¡ˆAï¼šå¤šä»»åŠ¡è¯„ä¼°ï¼ˆæ¨èï¼‰

**ä¼˜ç‚¹ï¼š**
- âœ… å…¨é¢è¯„ä¼°æ¨¡å‹èƒ½åŠ›ï¼ˆæ•°å­¦ + å·¥å…·è°ƒç”¨ï¼‰
- âœ… ç¬¦åˆçœŸå®åº”ç”¨åœºæ™¯
- âœ… å¯ä»¥ä¸ºä¸åŒä»»åŠ¡è®¾ç½®æƒé‡

**å®ç°ï¼š**

```python
def create_multi_task_evaluation_fn(
    model_skeleton: torch.nn.Module,
    param_shapes: list,
    gsm8k_dataset,      # GSM8Kæ•°æ®é›†
    bfcl_dataset,       # BFCLæ•°æ®é›†
    tokenizer: AutoTokenizer,
    task_weights: dict = {"gsm8k": 0.5, "bfcl": 0.5},
    eval_subset_size: int = None,
):
    """
    å¤šä»»åŠ¡è¯„ä¼°å‡½æ•°ï¼šåŒæ—¶è¯„ä¼°GSM8Kå’ŒBFCL
    
    Args:
        task_weights: ä»»åŠ¡æƒé‡ï¼Œä¾‹å¦‚ {"gsm8k": 0.5, "bfcl": 0.5}
        eval_subset_size: æ¯ä¸ªä»»åŠ¡é‡‡æ ·çš„æ ·æœ¬æ•°
    
    Returns:
        scores: shape (num_samples,) åŒ…å«æ‰€æœ‰ä»»åŠ¡çš„å¾—åˆ†
    """
    
    # åˆ›å»ºä¸¤ä¸ªè¯„ä¼°å‡½æ•°
    gsm8k_eval_fn = create_evaluation_fn_for_llm(
        model_skeleton, param_shapes, gsm8k_dataset, tokenizer,
        eval_subset_size=eval_subset_size
    )
    
    bfcl_eval_fn = create_bfcl_evaluation_fn(
        model_skeleton, param_shapes, bfcl_dataset, tokenizer,
        eval_subset_size=eval_subset_size
    )
    
    def evaluation_fn(flat_params: jnp.ndarray) -> jnp.ndarray:
        # è¯„ä¼°ä¸¤ä¸ªä»»åŠ¡
        gsm8k_scores = gsm8k_eval_fn(flat_params)  # shape: (n1,)
        bfcl_scores = bfcl_eval_fn(flat_params)    # shape: (n2,)
        
        # æ–¹å¼1ï¼šæ‹¼æ¥æ‰€æœ‰åˆ†æ•°ï¼ˆä¿æŒper-sampleç²’åº¦ï¼‰
        all_scores = jnp.concatenate([gsm8k_scores, bfcl_scores])
        
        # æ–¹å¼2ï¼šåŠ æƒå¹³å‡åå¹¿æ’­ï¼ˆå¦‚æœéœ€è¦ç»Ÿä¸€ç»´åº¦ï¼‰
        # weighted_score = (
        #     task_weights["gsm8k"] * jnp.mean(gsm8k_scores) +
        #     task_weights["bfcl"] * jnp.mean(bfcl_scores)
        # )
        # all_scores = jnp.full(len(gsm8k_scores), weighted_score)
        
        return all_scores
    
    return evaluation_fn
```

### æ–¹æ¡ˆBï¼šå•ç‹¬BFCLè¯„ä¼°

**ä¼˜ç‚¹ï¼š**
- âœ… ä¸“æ³¨äºå·¥å…·è°ƒç”¨èƒ½åŠ›
- âœ… å®ç°ç®€å•
- âœ… å¯ä»¥å¿«é€ŸéªŒè¯

**å®ç°ï¼š**

```python
def create_bfcl_evaluation_fn(
    model_skeleton: torch.nn.Module,
    param_shapes: list,
    bfcl_dataset,       # BFCLæµ‹è¯•æ•°æ®
    tokenizer: AutoTokenizer,
    batch_size: int = 4,
    eval_subset_size: int = None,
) -> Callable[[jnp.ndarray], jnp.ndarray]:
    """
    åˆ›å»ºBFCLè¯„ä¼°å‡½æ•°
    
    BFCLè¯„ä¼°æµç¨‹ï¼š
    1. è¾“å…¥ï¼šfunction definitions + user query
    2. æ¨¡å‹ç”Ÿæˆï¼šfunction call (JSONæ ¼å¼)
    3. è¯„ä¼°ï¼š
       - AST Match: å‚æ•°å’Œå‡½æ•°ååŒ¹é…ï¼ˆé¡ºåºæ— å…³ï¼‰
       - Exec Match: å®é™…æ‰§è¡Œç»“æœåŒ¹é…
       - Overall: ç»¼åˆå¾—åˆ†
    
    Returns:
        evaluation_fn: è¿”å›shape (num_samples,) çš„å¾—åˆ†æ•°ç»„
    """
    base_model = (
        model_skeleton.module if hasattr(model_skeleton, "module") else model_skeleton
    )
    device = next(base_model.parameters()).device
    iteration_counter = {'count': 0}
    
    def evaluation_fn(flat_params: jnp.ndarray) -> jnp.ndarray:
        """è¯„ä¼°å•ä¸ªæ¨¡å‹åœ¨BFCLä¸Šçš„è¡¨ç°"""
        
        # 1. åŠ è½½å‚æ•°åˆ°æ¨¡å‹
        load_flat_params_to_model(base_model, flat_params, param_shapes)
        base_model.eval()
        
        # 2. éšæœºé‡‡æ ·æ•°æ®ï¼ˆå¦‚æœå¯ç”¨subset evaluationï¼‰
        if eval_subset_size is not None and eval_subset_size < len(bfcl_dataset):
            import random
            iteration_counter['count'] += 1
            random.seed(42 + iteration_counter['count'])
            indices = random.sample(range(len(bfcl_dataset)), eval_subset_size)
            from torch.utils.data import Subset
            eval_dataset = Subset(bfcl_dataset, indices)
        else:
            eval_dataset = bfcl_dataset
        
        # 3. åˆ›å»ºDataLoader
        from torch.utils.data import DataLoader
        data_loader = DataLoader(
            eval_dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=bfcl_collate_fn
        )
        
        # 4. æ‰¹é‡è¯„ä¼°
        all_scores = []
        
        with torch.no_grad():
            for batch in data_loader:
                # batchåŒ…å«:
                # - input_ids: åŒ…å«function definitions + user query
                # - ground_truth_calls: æ­£ç¡®çš„function call
                
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                ground_truth_calls = batch["ground_truth_calls"]  # List[dict]
                
                # ç”Ÿæˆfunction call
                generated_ids = base_model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=256,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id,
                )
                
                # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
                generated_texts = tokenizer.batch_decode(
                    generated_ids[:, input_ids.shape[1]:],
                    skip_special_tokens=True
                )
                
                # è¯„ä¼°æ¯ä¸ªæ ·æœ¬
                for gen_text, gt_call in zip(generated_texts, ground_truth_calls):
                    # æå–ç”Ÿæˆçš„function callï¼ˆJSONæ ¼å¼ï¼‰
                    pred_call = extract_function_call(gen_text)
                    
                    # è¯„ä¼°å‡†ç¡®æ€§
                    score = evaluate_function_call(pred_call, gt_call)
                    all_scores.append(score)
        
        return jnp.array(all_scores)
    
    return evaluation_fn


def extract_function_call(text: str) -> dict:
    """
    ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–function call
    
    ç¤ºä¾‹è¾“å…¥:
        "I will call the function: calculate_area(length=5, width=3)"
    
    ç¤ºä¾‹è¾“å‡º:
        {
            "name": "calculate_area",
            "arguments": {"length": 5, "width": 3}
        }
    """
    import json
    import re
    
    # å°è¯•æå–JSONæ ¼å¼
    json_match = re.search(r'\{.*\}', text, re.DOTALL)
    if json_match:
        try:
            return json.loads(json_match.group())
        except:
            pass
    
    # å°è¯•æå–å‡½æ•°è°ƒç”¨æ ¼å¼: func_name(arg1=val1, arg2=val2)
    func_match = re.search(r'(\w+)\((.*?)\)', text)
    if func_match:
        func_name = func_match.group(1)
        args_str = func_match.group(2)
        
        # è§£æå‚æ•°
        args = {}
        for arg in args_str.split(','):
            if '=' in arg:
                key, val = arg.split('=', 1)
                args[key.strip()] = eval(val.strip())  # æ³¨æ„ï¼šç”Ÿäº§ç¯å¢ƒéœ€è¦æ›´å®‰å…¨çš„è§£æ
        
        return {
            "name": func_name,
            "arguments": args
        }
    
    return {"name": None, "arguments": {}}


def evaluate_function_call(pred_call: dict, gt_call: dict) -> float:
    """
    è¯„ä¼°function callçš„å‡†ç¡®æ€§
    
    BFCLè¯„ä¼°æ ‡å‡†ï¼š
    1. AST Match: å‡½æ•°åå’Œå‚æ•°åŒ¹é…ï¼ˆå‚æ•°é¡ºåºæ— å…³ï¼‰
    2. Exec Match: å®é™…æ‰§è¡Œç»“æœåŒ¹é…
    
    Args:
        pred_call: é¢„æµ‹çš„function call
        gt_call: ground truth function call
    
    Returns:
        score: 0.0 (é”™è¯¯) æˆ– 1.0 (æ­£ç¡®)
    """
    # æ£€æŸ¥å‡½æ•°å
    if pred_call.get("name") != gt_call.get("name"):
        return 0.0
    
    # æ£€æŸ¥å‚æ•°ï¼ˆé¡ºåºæ— å…³ï¼‰
    pred_args = pred_call.get("arguments", {})
    gt_args = gt_call.get("arguments", {})
    
    # å‚æ•°æ•°é‡å¿…é¡»ä¸€è‡´
    if len(pred_args) != len(gt_args):
        return 0.0
    
    # æ¯ä¸ªå‚æ•°çš„å€¼å¿…é¡»åŒ¹é…
    for key, gt_val in gt_args.items():
        if key not in pred_args:
            return 0.0
        
        pred_val = pred_args[key]
        
        # æ•°å€¼æ¯”è¾ƒï¼ˆå…è®¸æµ®ç‚¹è¯¯å·®ï¼‰
        if isinstance(gt_val, (int, float)) and isinstance(pred_val, (int, float)):
            if abs(pred_val - gt_val) > 1e-6:
                return 0.0
        # å­—ç¬¦ä¸²æ¯”è¾ƒ
        elif str(pred_val).strip() != str(gt_val).strip():
            return 0.0
    
    return 1.0


def bfcl_collate_fn(batch):
    """BFCLæ•°æ®é›†çš„collateå‡½æ•°"""
    import torch
    
    input_ids = torch.stack([torch.tensor(item["input_ids"]) for item in batch])
    attention_mask = torch.stack([torch.tensor(item["attention_mask"]) for item in batch])
    ground_truth_calls = [item["ground_truth_call"] for item in batch]
    
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "ground_truth_calls": ground_truth_calls
    }
```

---

## ğŸ“Š **æ•°æ®å‡†å¤‡**

### BFCLæ•°æ®é›†æ ¼å¼

```python
# æ•°æ®é›†ç¤ºä¾‹
bfcl_data = [
    {
        "functions": [
            {
                "name": "calculate_area",
                "description": "Calculate the area of a rectangle",
                "parameters": {
                    "length": {"type": "number", "description": "Length of rectangle"},
                    "width": {"type": "number", "description": "Width of rectangle"}
                }
            }
        ],
        "user_query": "What is the area of a rectangle with length 5 and width 3?",
        "ground_truth_call": {
            "name": "calculate_area",
            "arguments": {"length": 5, "width": 3}
        }
    },
    # ... more samples
]

# é¢„å¤„ç†ä¸ºHugging Face Datasetæ ¼å¼
from datasets import Dataset

def preprocess_bfcl(example, tokenizer):
    """å°†BFCLæ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼"""
    
    # æ„å»ºè¾“å…¥prompt
    functions_text = format_functions(example["functions"])
    user_query = example["user_query"]
    
    prompt = f"""Available functions:
{functions_text}

User query: {user_query}

Generate the appropriate function call:"""
    
    # Tokenize
    encoded = tokenizer(
        prompt,
        truncation=True,
        max_length=512,
        padding="max_length",
        return_tensors="pt"
    )
    
    return {
        "input_ids": encoded["input_ids"][0],
        "attention_mask": encoded["attention_mask"][0],
        "ground_truth_call": example["ground_truth_call"]
    }

def format_functions(functions):
    """æ ¼å¼åŒ–function definitions"""
    formatted = []
    for func in functions:
        params_str = ", ".join([
            f"{name}: {info['type']}" 
            for name, info in func["parameters"].items()
        ])
        formatted.append(f"- {func['name']}({params_str}): {func['description']}")
    return "\n".join(formatted)

# åŠ è½½å’Œé¢„å¤„ç†
bfcl_dataset = Dataset.from_list(bfcl_data)
tokenized_bfcl = bfcl_dataset.map(
    lambda x: preprocess_bfcl(x, tokenizer),
    remove_columns=bfcl_dataset.column_names
)
```

---

## ğŸ”§ **é›†æˆåˆ°Natural Niches**

### ä¿®æ”¹ä¸»è¯„ä¼°å‡½æ•°

```python
# åœ¨ run_natural_niches_sparsity_aware() ä¸­

# åŸæœ‰çš„GSM8Kè¯„ä¼°
train_eval_fn = create_evaluation_fn_for_llm(
    model_skeleton,
    param_shapes,
    tokenized_train_dataset,  # GSM8K
    tokenizer,
    eval_subset_size=eval_subset_size,
)

# æ–°å¢ï¼šBFCLè¯„ä¼°ï¼ˆå¯é€‰ï¼‰
if use_bfcl_eval:
    bfcl_eval_fn = create_bfcl_evaluation_fn(
        model_skeleton,
        param_shapes,
        tokenized_bfcl_dataset,
        tokenizer,
        eval_subset_size=eval_subset_size,
    )
    
    # ç»„åˆè¯„ä¼°
    def combined_eval_fn(flat_params):
        gsm8k_scores = train_eval_fn(flat_params)
        bfcl_scores = bfcl_eval_fn(flat_params)
        
        # æ‹¼æ¥åˆ†æ•°ï¼ˆä¿æŒper-sampleç²’åº¦ï¼‰
        return jnp.concatenate([gsm8k_scores, bfcl_scores])
    
    train_eval_fn = combined_eval_fn
    num_tasks = len(tokenized_train_dataset) + len(tokenized_bfcl_dataset)
```

---

## âš¡ **æ€§èƒ½ä¼˜åŒ–å»ºè®®**

### 1. ä½¿ç”¨Subset Evaluation

```python
# GSM8K: 30ä¸ªæ ·æœ¬
# BFCL: 30ä¸ªæ ·æœ¬
# æ€»å…±: 60ä¸ªæ ·æœ¬/æ¬¡è¯„ä¼°

eval_subset_size = 30  # æ¯ä¸ªä»»åŠ¡é‡‡æ ·30ä¸ª
```

### 2. æ‰¹é‡å¤„ç†

```python
batch_size = 16  # å¢å¤§batch sizeåŠ é€Ÿ
```

### 3. ç¼“å­˜Function Definitions

```python
# é¢„å…ˆæ ¼å¼åŒ–function definitionsï¼Œé¿å…é‡å¤å¤„ç†
cached_function_prompts = precompute_function_prompts(bfcl_dataset)
```

---

## ğŸ“ˆ **å®éªŒè®¾ç½®å»ºè®®**

### é˜¶æ®µ1ï¼šéªŒè¯BFCLè¯„ä¼°

```bash
# å…ˆå•ç‹¬æµ‹è¯•BFCLè¯„ä¼°
python main_sparsity_aware.py \
    --model1_path models/Qwen2.5-0.5B-Instruct \
    --model2_path models/Qwen2.5-0.5B-Instruct \
    --pop_size 2 \
    --total_forward_passes 100 \
    --eval_subset_size 30 \
    --use_bfcl_eval \
    --bfcl_weight 1.0  # åªç”¨BFCL
```

### é˜¶æ®µ2ï¼šå¤šä»»åŠ¡è”åˆè®­ç»ƒ

```bash
# GSM8K + BFCL è”åˆè¯„ä¼°
python main_sparsity_aware.py \
    --model1_path models/Qwen2.5-0.5B-Instruct \
    --model2_path models/Qwen2.5-0.5B-Instruct \
    --pop_size 5 \
    --total_forward_passes 3000 \
    --eval_subset_size 30 \
    --use_multi_task_eval \
    --gsm8k_weight 0.5 \
    --bfcl_weight 0.5
```

---

## ğŸ¯ **é¢„æœŸæ•ˆæœ**

### Fitness Scoreç»„æˆ

```python
# åŸå§‹fitness (åªæœ‰GSM8K)
fitness_old = accuracy_gsm8k

# æ–°fitness (GSM8K + BFCL)
fitness_new = (
    0.5 * accuracy_gsm8k + 
    0.5 * accuracy_bfcl
)

# åŠ ä¸Šsparsity
total_score = omega * fitness_new + beta * sparsity
```

### é¢„æœŸè®­ç»ƒæ›²çº¿

```
Iteration | GSM8K Acc | BFCL Acc | Total Fitness | Sparsity
---------|-----------|----------|---------------|----------
   100   |   0.15    |   0.10   |     0.125     |   0.35
   500   |   0.25    |   0.20   |     0.225     |   0.28
  1000   |   0.32    |   0.28   |     0.300     |   0.22
  3000   |   0.38    |   0.35   |     0.365     |   0.18
```

---

## ğŸ“ **å‘½ä»¤è¡Œå‚æ•°æ‰©å±•**

```python
# åœ¨ main_sparsity_aware.py ä¸­æ·»åŠ 
parser.add_argument('--use_bfcl_eval', action='store_true',
                   help='Enable BFCL function calling evaluation')
parser.add_argument('--bfcl_data_path', type=str, 
                   default='datasets/bfcl_test.json',
                   help='Path to BFCL test dataset')
parser.add_argument('--gsm8k_weight', type=float, default=0.5,
                   help='Weight for GSM8K task')
parser.add_argument('--bfcl_weight', type=float, default=0.5,
                   help='Weight for BFCL task')
```

---

## ğŸš€ **å®æ–½æ­¥éª¤**

1. **å‡†å¤‡BFCLæ•°æ®é›†**
   - ä¸‹è½½BFCL test set
   - è½¬æ¢ä¸ºHugging Face Datasetæ ¼å¼
   - é¢„å¤„ç†å’Œtokenize

2. **å®ç°è¯„ä¼°å‡½æ•°**
   - `create_bfcl_evaluation_fn()`
   - `extract_function_call()`
   - `evaluate_function_call()`

3. **é›†æˆåˆ°ä¸»å¾ªç¯**
   - ä¿®æ”¹`run_natural_niches_sparsity_aware()`
   - æ·»åŠ å¤šä»»åŠ¡è¯„ä¼°é€»è¾‘

4. **æµ‹è¯•éªŒè¯**
   - å•ç‹¬æµ‹è¯•BFCLè¯„ä¼°
   - éªŒè¯å¤šä»»åŠ¡è¯„ä¼°
   - æ€§èƒ½benchmarking

5. **å…¨é‡å®éªŒ**
   - è¿è¡Œå®Œæ•´çš„è¿›åŒ–ç®—æ³•
   - å¯¹æ¯”å•ä»»åŠ¡vså¤šä»»åŠ¡æ•ˆæœ

---

## ğŸ’¡ **å…³é”®æ³¨æ„äº‹é¡¹**

1. **æ•°æ®ç»´åº¦ä¸€è‡´æ€§**
   - GSM8K: 200ä¸ªæ ·æœ¬ â†’ é‡‡æ ·30ä¸ª
   - BFCL: 100ä¸ªæ ·æœ¬ â†’ é‡‡æ ·30ä¸ª
   - æ€»ä»»åŠ¡æ•°: 60ä¸ªæ ·æœ¬/æ¬¡

2. **è¯„ä¼°é€Ÿåº¦**
   - BFCLç”Ÿæˆå¯èƒ½æ¯”GSM8Kæ…¢ï¼ˆéœ€è¦ç”ŸæˆJSONæ ¼å¼ï¼‰
   - å»ºè®®ä½¿ç”¨è¾ƒå°çš„`max_new_tokens`ï¼ˆ256è¶³å¤Ÿï¼‰

3. **Prompt Engineering**
   - Function definitionsçš„æ ¼å¼å¾ˆé‡è¦
   - éœ€è¦å¼•å¯¼æ¨¡å‹ç”Ÿæˆæ ‡å‡†JSONæ ¼å¼

4. **é”™è¯¯å¤„ç†**
   - æ¨¡å‹å¯èƒ½ç”Ÿæˆä¸åˆæ³•çš„JSON
   - éœ€è¦robustçš„parsingé€»è¾‘


