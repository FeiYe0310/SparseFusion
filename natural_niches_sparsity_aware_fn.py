"""
Natural Niches with Sparsity-Aware Selection and Wanda Pruning

This module extends the original Natural Niches algorithm by:
1. Adding sparsity scoring alongside fitness scoring
2. Integrating Wanda pruning for active sparsification
3. Using Total Score (fitness + sparsity) for selection and archiving
4. Dynamic sparsity scheduling with Cosine Annealing and Warm Restarts

The core architecture and evaluation logic remain IDENTICAL to the original.
"""

import jax

# Enable 64-bit precision for JAX. This is crucial for handling large numbers,
# such as the total parameter count of a 7B model, which exceeds the int32 limit.
jax.config.update("jax_enable_x64", True)
import jax.numpy as jnp
from collections import defaultdict
from tqdm import tqdm
import torch
import numpy as np
from transformers import AutoTokenizer
from datasets import load_from_disk
from typing import Callable, Optional
import os
import math
import sys
import pickle
from contextlib import nullcontext
import os
import json
import random

# Add current directory to path for importing lib/ module
# This ensures the code is portable across different environments
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

try:
    from jax.sharding import Mesh, NamedSharding, PartitionSpec
    from jax import make_array_from_callback
except ImportError:  # pragma: no cover - back-compat with older JAX
    Mesh = None
    NamedSharding = None
    PartitionSpec = None
    make_array_from_callback = None

try:
    default_device_ctx = jax.default_device
except AttributeError:  # pragma: no cover - older JAX
    from contextlib import contextmanager

    def default_device_ctx(_device):
        @contextmanager
        def _noop():
            yield

        return _noop()


# --- Imports for Multi-GPU ---
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler

from helper_fn import (
    crossover,
    crossover_without_splitpoint,
    mutate,
    get_pre_trained_models_and_skeleton,
    jax_flattened_to_pytorch_model,
)
from config import GSM8K_DIR, RESULTS_DIR
from lib.async_shard import AsyncShardCoordinator
from dot_eval_utils import (
    generate_mult_dataset,
    generate_bool_dataset,
    parse_int_from_text,
    parse_bool_from_text,
    dot_collate,
)


def _init_distributed_if_needed() -> tuple[int, int]:
    """Initialise torch.distributed with sensible defaults and backend selection."""

    if not dist.is_available():
        raise RuntimeError(
            "torch.distributed is not available in this build of PyTorch"
        )

    if dist.is_initialized():
        return dist.get_rank(), dist.get_world_size()

    # Provide defaults so that single-process runs do not crash when torchrun env vars are missing.
    os.environ.setdefault("RANK", "0")
    os.environ.setdefault("WORLD_SIZE", "1")
    os.environ.setdefault("MASTER_ADDR", "127.0.0.1")
    os.environ.setdefault("MASTER_PORT", "29500")
    os.environ.setdefault("LOCAL_RANK", os.environ.get("RANK", "0"))

    rank = int(os.environ["RANK"])
    world_size = int(os.environ["WORLD_SIZE"])

    backend = os.environ.get("TORCH_BACKEND")
    if not backend:
        backend = "nccl" if torch.cuda.is_available() else "gloo"

    # Set a very long timeout (7 days) to prevent NCCL timeout errors
    # Default is 10 minutes (600s), which is too short for long computations
    from datetime import timedelta

    timeout = timedelta(days=7)

    dist.init_process_group(
        backend=backend,
        rank=rank,
        world_size=world_size,
        timeout=timeout,
    )

    return rank, world_size


# ==============================================================================
# SPARSITY-RELATED FUNCTIONS (NEW)
# ==============================================================================


def calculate_dynamic_sparsity(
    current_iteration: int, eta_min: float, eta_max: float, t0: int, t_mult: int
) -> float:
    """
    ä½¿ç”¨å¸¦çƒ­é‡å¯çš„ä½™å¼¦é€€ç«ï¼ˆä¿®æ”¹ä¸ºæ­£å¼¦ï¼‰è®¡ç®—åŠ¨æ€å‰ªæç¨€ç–åº¦

    åŸºäºè®ºæ–‡: SGDR: Stochastic Gradient Descent with Warm Restarts (https://arxiv.org/abs/1608.03983)
    ä¿®æ”¹ç‚¹ï¼šä½¿ç”¨ sin(pi/2) æ›¿ä»£åŸæ–‡çš„ cosï¼Œå®ç° warm-up æ•ˆæœ

    å…¬å¼ï¼š
        eta_t = eta_min + 0.5 * (eta_max - eta_min) * (1 + sin(T_cur/T_i * pi/2))

    å…¶ä¸­ï¼š
        - T_cur: å½“å‰å‘¨æœŸå†…å·²ç»è¿‡çš„è¿­ä»£æ¬¡æ•°
        - T_i: å½“å‰å‘¨æœŸçš„æ€»è¿­ä»£æ¬¡æ•°
        - æ¯ä¸ªé‡å¯åï¼ŒT_i *= t_multï¼ˆå‘¨æœŸé•¿åº¦å˜åŒ–ï¼‰

    è®¾è®¡æ€è·¯ï¼š
        1. ä½¿ç”¨ sin(pi/2) è®©ç¨€ç–åº¦ä» eta_min å¹³æ»‘å¢é•¿åˆ° eta_maxï¼ˆwarm-upï¼‰
        2. åœ¨è¿›åŒ–åˆæœŸä½¿ç”¨å°ç¨€ç–åº¦ï¼Œä¿æŠ¤é«˜fitnessä½†ä½sparsityçš„ä¼˜ç§€ä¸ªä½“
        3. éšç€è¿›åŒ–æ¨è¿›ï¼Œé€æ¸æ¢ç´¢æ›´é«˜çš„ç¨€ç–åº¦ç©ºé—´
        4. å‘¨æœŸæ€§é‡å¯é¿å…ç®—æ³•é™·å…¥ç‰¹å®šç¨€ç–åº¦çš„"èˆ’é€‚åŒº"
        5. å¢åŠ ç§ç¾¤åœ¨ä¸åŒç¨€ç–ç”Ÿæ€ä½(sparsity niches)ä¸Šçš„å¤šæ ·æ€§

    Args:
        current_iteration: å½“å‰çš„è¿›åŒ–ä»£æ•°ï¼ˆä»0å¼€å§‹ï¼‰
        eta_min: ç¨€ç–åº¦çš„æœ€å°å€¼ï¼ˆä¾‹å¦‚ 0.1ï¼‰
        eta_max: ç¨€ç–åº¦çš„æœ€å¤§å€¼ï¼ˆä¾‹å¦‚ 0.6ï¼‰
        t0: ç¬¬ä¸€ä¸ªå‘¨æœŸçš„é•¿åº¦ï¼ˆè¿­ä»£æ¬¡æ•°ï¼Œä¾‹å¦‚ 100ï¼‰
        t_mult: æ¯æ¬¡é‡å¯åå‘¨æœŸé•¿åº¦çš„ä¹˜æ•°ï¼ˆä¾‹å¦‚ 2 è¡¨ç¤ºæ¯æ¬¡ç¿»å€ï¼Œ1 è¡¨ç¤ºå›ºå®šå‘¨æœŸï¼‰

    Returns:
        è®¡ç®—å‡ºçš„å½“å‰ç¨€ç–åº¦å€¼ï¼ˆåœ¨ [eta_min, eta_max] èŒƒå›´å†…ï¼‰

    ç¤ºä¾‹ï¼š
        >>> # ç¬¬ä¸€ä¸ªå‘¨æœŸ100æ¬¡è¿­ä»£ï¼Œæ¯æ¬¡å‘¨æœŸç¿»å€ï¼Œç¨€ç–åº¦åœ¨0.1-0.6ä¹‹é—´å˜åŒ–
        >>> for i in range(400):
        ...     sparsity = calculate_dynamic_sparsity(i, 0.1, 0.6, 100, 2)
        ...     print(f"Iter {i}: sparsity={sparsity:.4f}")

        è¾“å‡ºï¼š
        Iter 0: sparsity=0.1000    # ç¬¬1å‘¨æœŸå¼€å§‹ï¼ˆ0-99ï¼‰ï¼Œä»æœ€å°å€¼å¼€å§‹
        Iter 50: sparsity=0.2853   # ç¬¬1å‘¨æœŸä¸­ç‚¹
        Iter 99: sparsity=0.6000   # ç¬¬1å‘¨æœŸç»“æŸï¼Œè¾¾åˆ°æœ€å¤§å€¼
        Iter 100: sparsity=0.1000  # ç¬¬2å‘¨æœŸå¼€å§‹ï¼ˆ100-299ï¼‰ï¼Œé‡å¯ï¼
        Iter 150: sparsity=0.2146  # ç¬¬2å‘¨æœŸ25%å¤„
        Iter 200: sparsity=0.3500  # ç¬¬2å‘¨æœŸ50%å¤„
        Iter 299: sparsity=0.6000  # ç¬¬2å‘¨æœŸç»“æŸ
        Iter 300: sparsity=0.1000  # ç¬¬3å‘¨æœŸå¼€å§‹ï¼ˆ300-699ï¼‰ï¼Œé‡å¯ï¼
    """
    t_i = float(t0)
    t_cur = float(current_iteration)

    # ç¡®å®šå½“å‰æ˜¯ç¬¬å‡ ä¸ªå‘¨æœŸä»¥åŠåœ¨è¯¥å‘¨æœŸå†…çš„ä½ç½®
    while t_cur >= t_i:
        t_cur -= t_i
        t_i *= t_mult

    # åº”ç”¨æ­£å¼¦é¢„çƒ­å…¬å¼
    # T_cur / T_i: ä» 0 å¢é•¿åˆ° 1ï¼ˆè¡¨ç¤ºåœ¨å‘¨æœŸå†…çš„è¿›åº¦ï¼‰
    # * pi/2: æ˜ å°„åˆ° [0, pi/2]
    # sin(...): ä» 0 å¢é•¿åˆ° 1ï¼ˆæ­£å¼¦åœ¨ [0, pi/2] åŒºé—´å•è°ƒé€’å¢ï¼‰
    # (1 + sin(...)): ä» 1 å¢é•¿åˆ° 2
    # 0.5 * (1 + sin(...)): ä» 0.5 å¢é•¿åˆ° 1
    # eta_min + (eta_max - eta_min) * ...: ä» eta_min å¢é•¿åˆ° eta_max
    sparsity_ratio = 0.5 * (1 + math.sin((t_cur / t_i) * math.pi / 2))

    current_sparsity = eta_min + (eta_max - eta_min) * sparsity_ratio

    # å®‰å…¨è¾¹ç•Œæ£€æŸ¥ï¼ˆç†è®ºä¸Šä¸éœ€è¦ï¼Œä½†ä¿é™©èµ·è§ï¼‰
    return max(eta_min, min(current_sparsity, eta_max))


def compute_sparsity(params: jnp.ndarray, epsilon: float = 1e-10) -> float:
    """
    è®¡ç®—å‚æ•°çš„ç¨€ç–åº¦ï¼ˆè¿‘é›¶å‚æ•°çš„æ¯”ä¾‹ï¼‰

    Args:
        params: JAXå‚æ•°æ•°ç»„
        epsilon: åˆ¤æ–­ä¸ºé›¶çš„é˜ˆå€¼

    Returns:
        ç¨€ç–åº¦ï¼ˆ0-1ä¹‹é—´ï¼‰
    """
    near_zero = jnp.abs(params) < epsilon
    return jnp.mean(near_zero.astype(jnp.float32))


def compute_sparsity_scores(
    archive: jnp.ndarray, tau: float, epsilon: float
) -> jnp.ndarray:
    """
    è®¡ç®—æ¡£æ¡ˆä¸­æ‰€æœ‰ä¸ªä½“çš„å½’ä¸€åŒ–ç¨€ç–åº¦å¾—åˆ†

    ä½¿ç”¨Softmaxå½’ä¸€åŒ–ç¡®ä¿ï¼š
    1. æ‰€æœ‰å¾—åˆ†å’Œä¸º1
    2. é¿å…é™¤é›¶é—®é¢˜

    Args:
        archive: æ¡£æ¡ˆå‚æ•°çŸ©é˜µ (pop_size, num_params)
        tau: Softmaxæ¸©åº¦å‚æ•°
        epsilon: é›¶å‚æ•°é˜ˆå€¼

    Returns:
        å½’ä¸€åŒ–çš„ç¨€ç–åº¦å¾—åˆ† (pop_size,)
    """
    pop_size = archive.shape[0]
    sparsities = jnp.array(
        [compute_sparsity(archive[i], epsilon) for i in range(pop_size)]
    )

    # Softmaxå½’ä¸€åŒ–
    exp_sparsities = jnp.exp(sparsities / tau)
    normalized = exp_sparsities / jnp.sum(exp_sparsities)

    return normalized


def compute_normalized_fitness(
    scores: jnp.ndarray, alpha: float, num_tasks: int
) -> jnp.ndarray:
    """
    è®¡ç®—å½’ä¸€åŒ–çš„fitnesså¾—åˆ†

    åŸå§‹fitnessä»…æ±‚å’Œä¸å½’ä¸€åŒ–ï¼Œåœ¨å¼•å…¥sparsity scoreåéœ€è¦å½’ä¸€åŒ–ä»¥å¯¹é½æ•°é‡çº§ã€‚

    Formula: F_i = Î£(f_i,j) / M^Î±

    Args:
        scores: å¾—åˆ†çŸ©é˜µ (pop_size, num_tasks)ï¼Œå·²ç»è¿‡ç«äº‰å½’ä¸€åŒ–
        alpha: å½’ä¸€åŒ–æŒ‡æ•°
        num_tasks: æ€»ä»»åŠ¡æ•°M

    Returns:
        å½’ä¸€åŒ–çš„fitness (pop_size,)
    """
    # ç¡®ä¿scoresæ˜¯floatç±»å‹ï¼ˆå¯èƒ½æ˜¯boolï¼‰
    scores_float = scores.astype(jnp.float32)

    # åº”ç”¨ç«äº‰å½’ä¸€åŒ–ï¼ˆä¸åŸå§‹update_archiveç›¸åŒï¼‰
    z = jnp.sum(scores_float, axis=0) ** alpha
    z = jnp.where(z, z, 1)  # é¿å…é™¤é›¶
    normalized_scores = scores_float / z[None, :]

    # æ±‚å’Œå¹¶é™¤ä»¥M^Î±
    fitness = jnp.sum(normalized_scores, axis=1) / (num_tasks**alpha)

    return fitness


def compute_total_scores(
    archive: jnp.ndarray,
    scores: jnp.ndarray,
    omega: float,
    beta: float,
    tau: float,
    alpha: float,
    num_tasks: int,
    epsilon: float,
) -> jnp.ndarray:
    """
    è®¡ç®—æ€»åˆ† = Ï‰Ã—Fitness + Î²Ã—Sparsity

    **é‡è¦æƒ©ç½šæœºåˆ¶**: å¯¹ç¨€ç–åº¦ > 0.5 çš„ä¸ªä½“ç»™äºˆæä½åˆ†æ•°ï¼ˆ-1e6ï¼‰ï¼Œ
    ç¡®ä¿å®ƒä»¬ä¼šè¢«ä¼˜å…ˆæ›¿æ¢æ‰ï¼Œé¿å…archiveè¢«å…¨0ä¸ªä½“å æ®ã€‚

    Args:
        archive: æ¡£æ¡ˆå‚æ•°çŸ©é˜µ
        scores: æ€§èƒ½å¾—åˆ†çŸ©é˜µ
        omega: Fitnessæƒé‡
        beta: Sparsityæƒé‡
        tau: Softmaxæ¸©åº¦
        alpha: Fitnesså½’ä¸€åŒ–æŒ‡æ•°
        num_tasks: ä»»åŠ¡æ€»æ•°
        epsilon: é›¶å‚æ•°é˜ˆå€¼

    Returns:
        æ€»åˆ† (pop_size,) - è¿‡äºç¨€ç–çš„ä¸ªä½“ä¼šå¾—åˆ°æä½åˆ†
    """
    pop_size = archive.shape[0]

    # è®¡ç®—æ¯ä¸ªä¸ªä½“çš„ç¨€ç–åº¦
    sparsities = jnp.array(
        [compute_sparsity(archive[i], epsilon) for i in range(pop_size)]
    )

    # è®¡ç®—æ­£å¸¸çš„fitnesså’Œsparsity scores
    fitness = compute_normalized_fitness(scores, alpha, num_tasks)
    sparsity_scores = compute_sparsity_scores(archive, tau, epsilon)

    # è®¡ç®—æ€»åˆ†
    total = omega * fitness + beta * sparsity_scores

    # æƒ©ç½šæœºåˆ¶ï¼šç¨€ç–åº¦ > 0.5 çš„ä¸ªä½“ç»™äºˆæä½åˆ†æ•°
    penalty_mask = sparsities > 0.5
    total = jnp.where(penalty_mask, -1e6, total)

    return total


# ==============================================================================
# WANDA PRUNING INTEGRATION (NEW)
# ==============================================================================


def prune_magnitude(
    jax_flat_params: jnp.ndarray, sparsity_ratio: float, sample_size: int = 10000
) -> jnp.ndarray:
    """
    å¿«é€ŸMagnitudeå‰ªæï¼šä½¿ç”¨é‡‡æ ·ä¼°è®¡é˜ˆå€¼
    ä¸éœ€è¦æ’åºæ•´ä¸ªæ•°ç»„ï¼Œé€Ÿåº¦å¿«

    Args:
        jax_flat_params: JAXæ‰å¹³åŒ–å‚æ•°æ•°ç»„
        sparsity_ratio: ç›®æ ‡ç¨€ç–åº¦ (0.0-1.0)
        sample_size: é‡‡æ ·å¤§å°ï¼ˆç”¨äºä¼°è®¡é˜ˆå€¼ï¼‰

    Returns:
        å‰ªæåçš„JAXå‚æ•°æ•°ç»„
    """
    if sparsity_ratio <= 0.0:
        return jax_flat_params

    # ä¿å­˜åŸå§‹dtype
    original_dtype = jax_flat_params.dtype

    # è½¬æ¢ä¸ºfloat32è¿›è¡Œè®¡ç®—
    params_float = jax_flat_params.astype(jnp.float32)
    abs_params = jnp.abs(params_float)

    # å¿«é€Ÿæ–¹æ³•ï¼šé‡‡æ ·ä¼°è®¡é˜ˆå€¼ï¼ˆé¿å…å¯¹å…¨éƒ¨å‚æ•°æ’åºï¼‰
    total_size = abs_params.size
    if total_size > sample_size:
        # éšæœºé‡‡æ ·ä¸€éƒ¨åˆ†å‚æ•°æ¥ä¼°è®¡é˜ˆå€¼
        sample_indices = jnp.linspace(0, total_size - 1, sample_size, dtype=jnp.int32)
        sampled_abs = abs_params.flatten()[sample_indices]
        threshold = jnp.percentile(sampled_abs, sparsity_ratio * 100)
    else:
        # å‚æ•°é‡å°ï¼Œç›´æ¥è®¡ç®—
        threshold = jnp.percentile(abs_params, sparsity_ratio * 100)

    # å‰ªæï¼šå°äºé˜ˆå€¼çš„è®¾ä¸º0
    pruned_params = jnp.where(abs_params < threshold, 0.0, params_float)

    # è½¬å›åŸå§‹dtype
    pruned_params = pruned_params.astype(original_dtype)

    return pruned_params


def prune_model_weights(
    pytorch_model: torch.nn.Module, sparsity_ratio: float
) -> torch.nn.Module:
    """
    ç›´æ¥å¯¹PyTorchæ¨¡å‹çš„æƒé‡è¿›è¡Œå‰ªæï¼ˆåŸºäºmagnitudeï¼‰
    ä¸éœ€è¦æ ¡å‡†æ•°æ®ï¼Œä¸éœ€è¦forward pass
    ä¿ç•™Wandaçš„å±‚çº§éå†é€»è¾‘

    Args:
        pytorch_model: PyTorchæ¨¡å‹
        sparsity_ratio: ç›®æ ‡ç¨€ç–åº¦

    Returns:
        å‰ªæåçš„æ¨¡å‹ï¼ˆin-placeä¿®æ”¹ï¼‰
    """
    import torch.nn as nn

    def find_layers(module, layers=[nn.Linear], name=""):
        if type(module) in layers:
            return {name: module}
        res = {}
        for name1, child in module.named_children():
            res.update(
                find_layers(
                    child,
                    layers=layers,
                    name=name + "." + name1 if name != "" else name1,
                )
            )
        return res

    # éå†æ‰€æœ‰transformerå±‚
    if hasattr(pytorch_model, "model") and hasattr(pytorch_model.model, "layers"):
        layers = pytorch_model.model.layers

        for i in range(len(layers)):
            layer = layers[i]
            subset = find_layers(layer)

            # å¯¹æ¯ä¸ªçº¿æ€§å±‚çš„æƒé‡è¿›è¡Œå‰ªæ
            for name in subset:
                W = subset[name].weight.data
                original_dtype = W.dtype

                # ä¼˜åŒ–ï¼šå°½é‡åœ¨GPUä¸Šè®¡ç®—ï¼Œé¿å…CPU-GPUä¼ è¾“
                # 1. è½¬float32ï¼ˆGPUä¸Šï¼‰
                W_float = W.float()

                # 2. è®¡ç®—magnitudeï¼ˆGPUä¸Šï¼‰
                W_metric = torch.abs(W_float)

                # 3. è®¡ç®—é˜ˆå€¼ï¼ˆä½¿ç”¨torch.kthvalueåœ¨GPUä¸Šï¼Œé¿å…sortï¼‰
                k = int(W.numel() * sparsity_ratio)
                if k > 0 and k < W.numel():
                    # kthvalueåœ¨GPUä¸Šè®¡ç®—ï¼Œæ¯”CPU numpyå¿«å¾ˆå¤š
                    thresh = torch.kthvalue(W_metric.flatten(), k).values
                else:
                    thresh = 0.0

                # 4. åº”ç”¨å‰ªæï¼ˆGPUä¸Šï¼‰
                mask = W_metric <= thresh
                W_float[mask] = 0

                # 5. è½¬å›åŸå§‹dtypeï¼ˆGPUä¸Šï¼‰
                W_pruned = W_float.to(original_dtype)

                # æ›´æ–°æƒé‡ï¼ˆæ— éœ€ç§»åŠ¨è®¾å¤‡ï¼Œå…¨ç¨‹GPUï¼‰
                subset[name].weight.data = W_pruned

    return pytorch_model


def prune_with_wanda(
    jax_flat_params: jnp.ndarray,
    model_skeleton: torch.nn.Module,
    param_shapes: list,
    tokenizer: AutoTokenizer,
    sparsity_ratio: float,
    device: torch.device,
    nsamples: int = 128,
) -> jnp.ndarray:
    """
    å¯¹JAXå‚æ•°è¿›è¡Œå‰ªæï¼ˆçº¯å‰ªæé€»è¾‘ï¼Œæ— æ ¡å‡†ï¼‰

    æµç¨‹ï¼š
    1. JAX flat params â†’ PyTorch model
    2. åº”ç”¨magnitudeå‰ªæï¼ˆWandaé£æ ¼çš„å±‚çº§éå†ï¼‰
    3. PyTorch model â†’ JAX flat params

    Args:
        jax_flat_params: JAXæ‰å¹³åŒ–å‚æ•°æ•°ç»„
        model_skeleton: PyTorchæ¨¡å‹éª¨æ¶
        param_shapes: å‚æ•°å½¢çŠ¶åˆ—è¡¨
        tokenizer: æœªä½¿ç”¨ï¼ˆä¿ç•™æ¥å£å…¼å®¹æ€§ï¼‰
        sparsity_ratio: ç›®æ ‡ç¨€ç–åº¦
        device: è®¡ç®—è®¾å¤‡
        nsamples: æœªä½¿ç”¨ï¼ˆä¿ç•™æ¥å£å…¼å®¹æ€§ï¼‰

    Returns:
        å‰ªæåçš„JAXå‚æ•°æ•°ç»„
    """
    # æ­¥éª¤1: è½¬æ¢ä¸ºPyTorchæ¨¡å‹
    base_model = (
        model_skeleton.module if hasattr(model_skeleton, "module") else model_skeleton
    )
    pytorch_model = jax_flattened_to_pytorch_model(
        jax_flat_params, base_model, param_shapes
    )
    pytorch_model.eval()

    # æ­¥éª¤2: ç›´æ¥å‰ªæï¼ˆä¸éœ€è¦æ ¡å‡†æ•°æ®ï¼‰
    pytorch_model = prune_model_weights(pytorch_model, sparsity_ratio)

    # æ­¥éª¤3: è½¬å›JAXå‚æ•°
    pruned_params = []
    for param in pytorch_model.parameters():
        # å…³é”®ï¼šå…ˆ.float()å†.numpy()ï¼Œé¿å…bfloat16é”™è¯¯
        pruned_params.append(param.detach().cpu().float().numpy().flatten())

    return jnp.array(np.concatenate(pruned_params)).astype(jnp.bfloat16)


# ==============================================================================
# EVALUATION FUNCTION (IDENTICAL TO ORIGINAL)
# ==============================================================================


def extract_answer(text: str) -> str:
    """
    ä»GSM8Kç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–æ•°å­—ç­”æ¡ˆ
    GSM8Kçš„æ ‡å‡†æ ¼å¼ï¼šç­”æ¡ˆåœ¨####åé¢
    """
    import re

    # å°è¯•æ‰¾åˆ°####åçš„ç­”æ¡ˆ
    if "####" in text:
        answer = text.split("####")[-1].strip()
    else:
        # å¦‚æœæ²¡æœ‰####ï¼Œå°è¯•æå–æœ€åä¸€ä¸ªæ•°å­—
        answer = text.strip()

    # æå–æ•°å­—ï¼ˆå¯èƒ½å¸¦é€—å·ã€å°æ•°ç‚¹ï¼‰
    numbers = re.findall(r"-?\d+(?:,\d{3})*(?:\.\d+)?", answer.replace(",", ""))
    if numbers:
        return numbers[-1]  # è¿”å›æœ€åä¸€ä¸ªæ•°å­—
    return ""


def create_evaluation_fn_for_llm(
    model_skeleton: torch.nn.Module,
    param_shapes: list,
    tokenized_dataset,
    tokenizer: AutoTokenizer,
    batch_size: int = 4,
    distributed: bool = False,
    world_size: int = 1,
    rank: int = 0,
    eval_subset_size: int = None,  # æ¯è½®è¯„ä¼°çš„æ ·æœ¬æ•°ï¼ˆNone=ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼‰
    return_subset_only: bool = False,  # å¤šä»»åŠ¡è¯„ä¼°æ—¶è®¾ä¸ºTrueï¼Œä¸æ‰©å±•åˆ°å®Œæ•´æ•°æ®é›†
    gsm8k_qwen_chat: bool = False,
    gsm8k_few_shot_k: int = 3,
    gsm8k_few_shot_dataset=None,
) -> Callable[[jnp.ndarray], jnp.ndarray]:
    """
    Creates an evaluation function for GSM8K using **generation + exact match**.

    çœŸå®è¯„ä¼°æµç¨‹ï¼š
    1. æ¨¡å‹ç”Ÿæˆå®Œæ•´ç­”æ¡ˆï¼ˆä½¿ç”¨ model.generate()ï¼‰
    2. ä»ç”Ÿæˆçš„æ–‡æœ¬ä¸­æå–æ•°å­—ç­”æ¡ˆ
    3. ä¸ground truthè¿›è¡Œç²¾ç¡®åŒ¹é…

    Args:
        eval_subset_size: æ¯è½®éšæœºé‡‡æ ·çš„æ•°æ®ç‚¹æ•°é‡ï¼ˆåŠ é€Ÿè¯„ä¼°ï¼‰
                         - None: ä½¿ç”¨å…¨éƒ¨æ•°æ®
                         - 30: æ¯è½®éšæœºé‡‡æ ·30ä¸ªæ•°æ®ç‚¹
    """
    base_model = (
        model_skeleton.module if hasattr(model_skeleton, "module") else model_skeleton
    )
    device = next(base_model.parameters()).device

    # ç”¨äºç”Ÿæˆä¸åŒéšæœºé‡‡æ ·çš„è¿­ä»£è®¡æ•°å™¨
    iteration_counter = {"count": 0}

    def collate_fn(batch):
        """
        è‡ªå®šä¹‰collateå‡½æ•°ï¼š
        - input_ids, attention_mask -> stackæˆtensor
        - answer_text -> ä¿æŒä¸ºå­—ç¬¦ä¸²åˆ—è¡¨
        """
        import torch

        input_ids = torch.stack([torch.tensor(item["input_ids"]) for item in batch])
        attention_mask = torch.stack(
            [torch.tensor(item["attention_mask"]) for item in batch]
        )
        answer_texts = [item["answer_text"] for item in batch]
        questions = [item.get("question", "") for item in batch]

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "answer_text": answer_texts,
            "question": questions,
        }

    def evaluation_fn(flat_params: jnp.ndarray) -> jnp.ndarray:
        # Get device from model
        device = next(base_model.parameters()).device

        # Restore parameters into the raw model (not the DDP wrapper)
        restored_model = jax_flattened_to_pytorch_model(
            flat_params, base_model, param_shapes
        )
        restored_model.eval()

        # ã€åŠ é€Ÿã€‘éšæœºé‡‡æ ·å­é›†ï¼ˆæ¯è½®ä¸åŒï¼‰
        if eval_subset_size is not None and eval_subset_size < len(tokenized_dataset):
            # ä½¿ç”¨è¿­ä»£è®¡æ•°å™¨ä½œä¸ºéšæœºç§å­ï¼Œç¡®ä¿æ¯è½®é‡‡æ ·ä¸åŒ
            import random

            random.seed(iteration_counter["count"])
            indices = random.sample(range(len(tokenized_dataset)), eval_subset_size)
            indices.sort()  # ä¿æŒé¡ºåºï¼Œä¾¿äºè°ƒè¯•

            # åˆ›å»ºsubset
            from torch.utils.data import Subset

            eval_dataset = Subset(tokenized_dataset, indices)
            iteration_counter["count"] += 1

            if rank == 0 and os.environ.get("VERBOSE_EVAL", "0") == "1":
                print(
                    f"  [Eval] ä½¿ç”¨ {eval_subset_size}/{len(tokenized_dataset)} æ ·æœ¬ (iteration {iteration_counter['count']})"
                )
        else:
            eval_dataset = tokenized_dataset

        # Sampler ensures each GPU gets a different slice of data when distributed
        if distributed:
            sampler = DistributedSampler(
                eval_dataset,
                num_replicas=world_size,
                rank=rank,
                shuffle=False,
            )
            data_loader = DataLoader(
                eval_dataset,
                batch_size=batch_size,
                sampler=sampler,
                collate_fn=collate_fn,
            )
        else:
            data_loader = DataLoader(
                eval_dataset,
                batch_size=batch_size,
                shuffle=False,
                collate_fn=collate_fn,
            )

        local_scores = []
        with torch.no_grad():
            for batch in data_loader:
                # è‹¥å¯ç”¨QwenèŠå¤©æ¨¡æ¿ï¼Œåˆ™åŸºäºquestioné‡å»ºè¾“å…¥
                if gsm8k_qwen_chat and hasattr(tokenizer, "apply_chat_template"):
                    qs = batch.get("question", [""] * len(batch["answer_text"]))
                    texts = []
                    import random as _random

                    # é¢„å–few-shotæ± ä¸­çš„(question, answer)å¯¹
                    few_qas = []
                    if gsm8k_few_shot_dataset is not None:
                        # å‡è®¾å­—æ®µåä¸º 'question' å’Œ 'answer'
                        few_qs = gsm8k_few_shot_dataset["question"]
                        few_as = gsm8k_few_shot_dataset["answer"]
                        few_qas = list(zip(few_qs, few_as))

                    def build_msgs(q: str, exemplars: list[tuple[str, str]]):
                        system_text = (
                            "You are a helpful math problem solver. "
                            "Answer with concise reasoning and put the final numeric answer on the last line as '#### <number>'."
                        )
                        msgs = [{"role": "system", "content": system_text}]
                        for ex_q, ex_a in exemplars:
                            msgs.append({"role": "user", "content": ex_q})
                            msgs.append({"role": "assistant", "content": ex_a})
                        msgs.append({"role": "user", "content": q})
                        return msgs

                    for idx, q in enumerate(qs):
                        k = max(0, int(gsm8k_few_shot_k))
                        exemplars = []
                        if few_qas and k > 0:
                            rnd = _random.Random((iteration_counter["count"] + idx) * 1315423911)
                            exemplars = rnd.sample(few_qas, min(k, len(few_qas)))
                        msgs = build_msgs(q, exemplars)
                        text = tokenizer.apply_chat_template(
                            msgs, tokenize=False, add_generation_prompt=True
                        )
                        texts.append(text)

                    enc = tokenizer(
                        texts,
                        padding=True,
                        truncation=True,
                        max_length=1024,
                        return_tensors="pt",
                    )
                    input_ids = enc["input_ids"].to(device)
                    attention_mask = enc["attention_mask"].to(device)
                else:
                    input_ids = batch["input_ids"].to(device)
                # åŸå§‹ç­”æ¡ˆæ–‡æœ¬ï¼ˆç”¨äºæå–ground truthï¼‰
                answer_texts = batch["answer_text"]

                # ç”Ÿæˆç­”æ¡ˆï¼ˆæœ€å¤š256ä¸ªtokenï¼Œä¿è¯å®Œæ•´æ¨ç†è¿‡ç¨‹ï¼‰
                generated_ids = restored_model.generate(
                    input_ids,
                    max_new_tokens=256,
                    do_sample=False,  # è´ªå©ªè§£ç ï¼Œä¿è¯å¯é‡å¤
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )

                # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
                generated_texts = tokenizer.batch_decode(
                    generated_ids[:, input_ids.shape[1] :],  # åªè¦æ–°ç”Ÿæˆçš„éƒ¨åˆ†
                    skip_special_tokens=True,
                )

                # å¯¹æ¯”é¢„æµ‹ç­”æ¡ˆå’Œground truth
                batch_scores = []
                for gen_text, gt_text in zip(generated_texts, answer_texts):
                    # ä»ç”Ÿæˆæ–‡æœ¬æå–é¢„æµ‹ç­”æ¡ˆ
                    pred_answer = extract_answer(gen_text)

                    # ä»ground truthæ–‡æœ¬æå–ç­”æ¡ˆ
                    gt_answer = extract_answer(gt_text)

                    # ç²¾ç¡®åŒ¹é…
                    is_correct = (pred_answer == gt_answer) and (pred_answer != "")
                    batch_scores.append(1.0 if is_correct else 0.0)

                local_scores.extend(batch_scores)

        if not local_scores:
            return jnp.zeros(len(tokenized_dataset))

        # è½¬ä¸ºtensor
        local_results_tensor = torch.tensor(local_scores, dtype=torch.float32)

        if distributed:
            # Move to GPU for NCCL backend
            local_results_tensor = local_results_tensor.to(device)
            gathered_tensors = [
                torch.empty_like(local_results_tensor) for _ in range(world_size)
            ]
            dist.all_gather(gathered_tensors, local_results_tensor)
            full_results_tensor = torch.cat(gathered_tensors)[: len(eval_dataset)]
            # Move back to CPU for numpy conversion
            full_results_tensor = full_results_tensor.cpu()
        else:
            full_results_tensor = local_results_tensor[: len(eval_dataset)]

        # ã€åŠ é€Ÿã€‘å¦‚æœä½¿ç”¨å­é›†è¯„ä¼°ï¼Œæ ¹æ®return_subset_onlyå†³å®šæ˜¯å¦æ‰©å±•
        if eval_subset_size is not None and eval_subset_size < len(tokenized_dataset):
            subset_scores = full_results_tensor.numpy()

            if return_subset_only:
                # å¤šä»»åŠ¡è¯„ä¼°ï¼šç›´æ¥è¿”å›å­é›†åˆ†æ•°ï¼Œä¸æ‰©å±•
                return jnp.array(subset_scores)
            else:
                # å•ä»»åŠ¡è¯„ä¼°ï¼šæ‰©å±•åˆ°å®Œæ•´æ•°æ®é›†å¤§å°
                avg_score = (
                    float(subset_scores.mean()) if len(subset_scores) > 0 else 0.0
                )
                full_scores = np.full(
                    len(tokenized_dataset), avg_score, dtype=np.float32
                )
                full_scores[indices] = subset_scores
                return jnp.array(full_scores)
        else:
            return jnp.array(full_results_tensor.numpy())

    return evaluation_fn


# ==============================================================================
# PARENT SELECTION (MODIFIED TO USE TOTAL SCORE)
# ==============================================================================


def sample_parents_with_sparsity(
    archive: jnp.ndarray,
    scores: jnp.ndarray,
    rand_key: jnp.ndarray,
    alpha: float,
    num_tasks: int,
    omega: float,
    beta: float,
    tau: float,
    use_matchmaker: bool,
    epsilon: float,
) -> tuple[jnp.ndarray, jnp.ndarray, int, int]:
    """
    é€‰æ‹©çˆ¶ä»£ï¼ˆåŸºäºTotal Score = Ï‰Ã—Fitness + Î²Ã—Sparsityï¼‰

    ä¸åŸå§‹sample_parentsçš„åŒºåˆ«ï¼šä½¿ç”¨total_scoresä»£æ›¿fitness

    Args:
        archive: æ¡£æ¡ˆå‚æ•°çŸ©é˜µ
        scores: æ€§èƒ½å¾—åˆ†çŸ©é˜µ
        rand_key: JAXéšæœºå¯†é’¥
        alpha: Fitnesså½’ä¸€åŒ–æŒ‡æ•°
        num_tasks: ä»»åŠ¡æ€»æ•°
        omega, beta, tau: æ€»åˆ†è®¡ç®—å‚æ•°
        use_matchmaker: æ˜¯å¦ä½¿ç”¨matchmaker
        epsilon: é›¶å‚æ•°é˜ˆå€¼

    Returns:
        ä¸¤ä¸ªçˆ¶ä»£å‚æ•°
    """
    k1, k2 = jax.random.split(rand_key)

    # è®¡ç®—Total Score
    total_scores = compute_total_scores(
        archive, scores, omega, beta, tau, alpha, num_tasks, epsilon
    )

    # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡
    probs = total_scores / jnp.sum(total_scores)

    # ç¬¬ä¸€ä¸ªçˆ¶ä»£
    if use_matchmaker:
        parent_1_idx = jax.random.choice(k1, probs.size, shape=(1,), p=probs)[0]

        # ç¬¬äºŒä¸ªçˆ¶ä»£ï¼šåŸºäºä¸ç¬¬ä¸€ä¸ªçˆ¶ä»£çš„äº’è¡¥æ€§
        # è®¡ç®—æ¯ä¸ªä¸ªä½“ä¸parent_1åœ¨å„ä»»åŠ¡ä¸Šçš„äº’è¡¥å¾—åˆ†
        z = scores.sum(axis=0)
        z = jnp.where(z, z, 1) ** alpha
        fitness_matrix = scores.astype(jnp.float32) / z[None, :]

        match_score = jnp.maximum(
            0, fitness_matrix - fitness_matrix[parent_1_idx, :]
        ).sum(axis=1)

        match_probs = match_score / jnp.sum(match_score)
        parent_2_idx = jax.random.choice(
            k2, match_probs.size, shape=(1,), p=match_probs
        )[0]
    else:
        parent_2_idx, parent_1_idx = jax.random.choice(
            k1, probs.size, shape=(2,), p=probs
        )
    # è¿”å›çˆ¶ä»£å‚æ•°ä»¥åŠå…¶ç´¢å¼•ï¼Œä¾¿äºè®°å½•è°±ç³»
    return archive[parent_1_idx], archive[parent_2_idx], int(parent_1_idx), int(parent_2_idx)


# ==============================================================================
# ARCHIVE UPDATE (MODIFIED TO USE TOTAL SCORE)
# ==============================================================================


def update_archive_with_sparsity(
    score: jnp.ndarray,
    param: jnp.ndarray,
    archive: jnp.ndarray,
    scores: jnp.ndarray,
    alpha: float,
    omega: float,
    beta: float,
    tau: float,
    num_tasks: int,
    epsilon: float,
) -> tuple[jnp.ndarray, jnp.ndarray]:
    """
    æ›´æ–°æ¡£æ¡ˆï¼ˆåŸºäºTotal Scoreï¼‰

    ä¸åŸå§‹update_archiveçš„åŒºåˆ«ï¼šä½¿ç”¨total_scoresä»£æ›¿fitness

    Args:
        score: æ–°ä¸ªä½“çš„æ€§èƒ½å¾—åˆ†
        param: æ–°ä¸ªä½“çš„å‚æ•°
        archive: å½“å‰æ¡£æ¡ˆ
        scores: å½“å‰æ€§èƒ½å¾—åˆ†çŸ©é˜µ
        alpha: Fitnesså½’ä¸€åŒ–æŒ‡æ•°
        omega, beta, tau: æ€»åˆ†è®¡ç®—å‚æ•°
        num_tasks: ä»»åŠ¡æ€»æ•°
        epsilon: é›¶å‚æ•°é˜ˆå€¼

    Returns:
        æ›´æ–°åçš„æ¡£æ¡ˆå’Œå¾—åˆ†çŸ©é˜µ
    """
    # æ‰©å±•æ¡£æ¡ˆä»¥åŒ…å«æ–°ä¸ªä½“
    ext_archive = jnp.concatenate([archive, param[None, ...]], axis=0)
    ext_scores = jnp.concatenate([scores, score[None, ...]], axis=0)

    # è®¡ç®—æ‰€æœ‰ä¸ªä½“çš„Total Score
    total_scores = compute_total_scores(
        ext_archive, ext_scores, omega, beta, tau, alpha, num_tasks, epsilon
    )

    # æ‰¾åˆ°æœ€å·®ä¸ªä½“
    worst_ix = jnp.asarray(jnp.argmin(total_scores), dtype=jnp.int32)
    scores_len = jnp.asarray(scores.shape[0], dtype=jnp.int32)
    update_mask = worst_ix < scores_len

    # æ›¿æ¢æœ€å·®ä¸ªä½“
    row_selector = (
        jnp.arange(archive.shape[0], dtype=jnp.int32) == worst_ix
    ) & update_mask
    row_selector = row_selector[:, None]

    archive = jnp.where(row_selector, param[None, :], archive)
    scores = jnp.where(row_selector[: scores.shape[0], :], score[None, :], scores)

    return archive, scores


# ==============================================================================
# MAIN EVOLUTION FUNCTION (MODIFIED TO INTEGRATE PRUNING)
# ==============================================================================


def run_natural_niches_sparsity_aware(
    runs: int,
    pop_size: int,
    total_forward_passes: int,
    store_train_results: bool,
    no_matchmaker: bool,
    no_crossover: bool,
    no_splitpoint: bool,
    alpha: float = 1.0,
    omega: float = 0.5,
    beta: float = 0.5,
    tau: float = 1.0,
    epsilon: float = 1e-10,
    pruning_sparsity: float = 0.0,
    pruning_method: str = "wanda",
    use_pre_trained: bool = False,
    model1_path: str = "",
    model2_path: str = "",
    distributed: bool = False,
    archive_backend: str = "gpu",
    log_sparsity_stats: bool = False,
    eval_subset_size: int = None,  # ğŸš€ NEW: æ¯è½®è¯„ä¼°çš„æ ·æœ¬æ•°ï¼ˆåŠ é€Ÿï¼‰
    use_bfcl_eval: bool = False,  # ğŸ¯ BFCL: æ˜¯å¦å¯ç”¨BFCLå¤šä»»åŠ¡è¯„ä¼°
    bfcl_data_path: str = "bfcl/data/bfcl_test_200.json",  # BFCLæ•°æ®è·¯å¾„
    gsm8k_weight: float = 0.5,  # GSM8Kä»»åŠ¡æƒé‡
    bfcl_weight: float = 0.5,  # BFCLä»»åŠ¡æƒé‡
    # GSM8K Qwen few-shot
    gsm8k_qwen_chat: bool = False,
    gsm8k_few_shot_k: int = 3,
    gsm8k_few_shot_split: str = "train",
    # ğŸ¯ MBPP: MBPPä»£ç ç”Ÿæˆè¯„ä¼°
    use_mbpp_eval: bool = False,  # æ˜¯å¦å¯ç”¨MBPPè¯„ä¼°
    mbpp_data_path: str = "mbpp/data/mbpp_test.json",  # MBPPæ•°æ®è·¯å¾„
    mbpp_weight: float = 0.33,  # MBPPä»»åŠ¡æƒé‡
    # MBPP Qwen few-shotï¼ˆä¸main_sparsity_awareä¿æŒä¸€è‡´ï¼Œå½“å‰ç‰ˆæœ¬ä¸å¼ºåˆ¶ä½¿ç”¨ï¼‰
    mbpp_qwen_chat: bool = False,
    mbpp_few_shot_k: int = 3,
    mbpp_few_shot_split: str = "train",
    # ğŸ”„ NEW: Dynamic Sparsity with Warm Restarts
    use_dynamic_sparsity: bool = False,  # æ˜¯å¦å¯ç”¨åŠ¨æ€ç¨€ç–åº¦è°ƒåº¦
    sparsity_min: float = 0.1,  # æœ€å°ç¨€ç–åº¦ (eta_min)
    sparsity_max: float = 0.6,  # æœ€å¤§ç¨€ç–åº¦ (eta_max)
    sparsity_t0: int = 100,  # ç¬¬ä¸€ä¸ªå‘¨æœŸçš„è¿­ä»£æ¬¡æ•°
    sparsity_t_mult: int = 2,  # å‘¨æœŸé•¿åº¦ä¹˜æ•°ï¼ˆ1=å›ºå®šå‘¨æœŸï¼Œ2=æ¯æ¬¡ç¿»å€ï¼‰
    async_num_nodes: Optional[int] = None,
    async_sync_interval: int = 10,
    # DoT tasks optional
    use_mult4_eval: bool = False,
    use_mult5_eval: bool = False,
    use_bool_eval: bool = False,
    mult4_weight: float = 0.0,
    mult5_weight: float = 0.0,
    bool_weight: float = 0.0,
    save_best_model: bool = True,
) -> list:
    """
    Run Natural Niches with Sparsity-Aware Selection and Wanda Pruning

    æ–°å¢å‚æ•°:
        omega: Fitnessæƒé‡ (default: 0.5)
        beta: Sparsityæƒé‡ (default: 0.5)
        tau: Softmaxæ¸©åº¦ (default: 1.0)
        epsilon: é›¶å‚æ•°é˜ˆå€¼ (default: 1e-10)
        pruning_sparsity: Wandaå‰ªæç›®æ ‡ç¨€ç–åº¦ (default: 0.0 = ä¸å‰ªæ)
        pruning_method: å‰ªææ–¹æ³• 'wanda' æˆ– 'magnitude' (default: 'wanda')
        log_sparsity_stats: æ˜¯å¦è®°å½•ç¨€ç–åº¦ç»Ÿè®¡ (default: False)
        eval_subset_size: æ¯è½®è¯„ä¼°çš„æ ·æœ¬æ•° (None=å…¨éƒ¨æ•°æ®, 30=éšæœºé‡‡æ ·30ä¸ª)
                         ã€åŠ é€Ÿã€‘å¯æ˜¾è‘—å‡å°‘è¯„ä¼°æ—¶é—´

        ğŸ”„ åŠ¨æ€ç¨€ç–åº¦è°ƒåº¦å‚æ•°ï¼ˆåŸºäºCosine Annealing with Warm Restartsï¼‰:
        use_dynamic_sparsity: å¯ç”¨åŠ¨æ€ç¨€ç–åº¦è°ƒåº¦ (default: False)
                             è‹¥å¯ç”¨ï¼Œå°†å¿½ç•¥ pruning_sparsity å‚æ•°
        sparsity_min: ç¨€ç–åº¦æœ€å°å€¼ (default: 0.1)
        sparsity_max: ç¨€ç–åº¦æœ€å¤§å€¼ (default: 0.6)
        sparsity_t0: ç¬¬ä¸€ä¸ªå‘¨æœŸçš„è¿­ä»£æ¬¡æ•° (default: 100)
        sparsity_t_mult: å‘¨æœŸé•¿åº¦ä¹˜æ•° (default: 2, å³æ¯æ¬¡ç¿»å€; 1=å›ºå®šå‘¨æœŸ)

    å…¶ä»–å‚æ•°ä¸åŸå§‹run_natural_nichesç›¸åŒã€‚
    """
    archive_backend = archive_backend.lower()
    if archive_backend not in {"gpu", "cpu"}:
        raise ValueError("archive_backend must be 'gpu' or 'cpu'")

    use_matchmaker, use_crossover, use_splitpoint = (
        not no_matchmaker,
        not no_crossover,
        not no_splitpoint,
    )

    # ç¡®å®šæ˜¯å¦å¯ç”¨å‰ªæï¼šåŠ¨æ€ç¨€ç–åº¦æˆ–é™æ€ç¨€ç–åº¦ä»»ä¸€å¯ç”¨å³å¯
    enable_pruning = (pruning_sparsity > 0.0) or use_dynamic_sparsity

    # --- Multi-GPU Distributed Setup (IDENTICAL TO ORIGINAL) ---
    if distributed:
        rank, world_size = _init_distributed_if_needed()
        local_rank = int(os.environ.get("LOCAL_RANK", rank))
    else:
        rank, world_size = 0, 1
        local_rank = 0

    if torch.cuda.is_available():
        torch.cuda.set_device(local_rank)
        device = torch.device("cuda", local_rank)
    else:
        device = torch.device("cpu")

    is_main_process = rank == 0
    dist_enabled = distributed and world_size > 1

    # --- LLM & Data Loading (IDENTICAL TO ORIGINAL) ---
    if is_main_process:
        print("Loading tokenizer and models...")

    (
        model_1,
        model_2,
        param_shapes,
        tokenizer,
        model_skeleton,
    ) = get_pre_trained_models_and_skeleton(
        model1_path,
        model2_path,
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # å¯¹äºdecoder-onlyæ¨¡å‹ï¼Œç”Ÿæˆæ—¶å¿…é¡»ç”¨å·¦å¡«å……
    tokenizer.padding_side = "left"

    # æ¸…é™¤æ¨¡å‹é»˜è®¤çš„é‡‡æ ·å‚æ•°ï¼ˆæˆ‘ä»¬ä½¿ç”¨è´ªå©ªè§£ç ï¼‰
    if hasattr(model_skeleton, "generation_config"):
        model_skeleton.generation_config.temperature = None
        model_skeleton.generation_config.top_p = None
        model_skeleton.generation_config.top_k = None

    num_params_llm = model_1.shape[0]

    if async_num_nodes is None:
        async_num_nodes = world_size if distributed else 1
    async_num_nodes = max(async_num_nodes, 1)
    async_enabled = async_num_nodes > 1
    async_coordinator: Optional[AsyncShardCoordinator] = None
    if async_enabled and is_main_process:
        async_coordinator = AsyncShardCoordinator(
            num_params=num_params_llm,
            num_nodes=async_num_nodes,
            sync_interval=async_sync_interval,
        )
        async_coordinator.bootstrap(model_1)

    if is_main_process:
        print("Loading and preprocessing GSM8K dataset from local directory...")
        if dist_enabled:
            load_from_disk(GSM8K_DIR)

    if dist_enabled:
        dist.barrier()

    dataset = load_from_disk(GSM8K_DIR)

    def preprocess_function(examples):
        """
        é¢„å¤„ç†GSM8Kæ•°æ®ç”¨äºç”Ÿæˆå¼è¯„ä¼°ï¼š
        - input_ids: åªåŒ…å«é—®é¢˜ï¼ˆç”¨äºmodel.generate()ï¼‰
        - answer_text: åŸå§‹ç­”æ¡ˆæ–‡æœ¬ï¼ˆç”¨äºæå–ground truthï¼‰
        """
        # åªtokenizeé—®é¢˜éƒ¨åˆ†
        questions = examples["question"]
        model_inputs = tokenizer(
            questions,
            max_length=256,  # GSM8Ké—®é¢˜æœ‰æ—¶è¾ƒé•¿
            padding="max_length",
            truncation=True,
        )

        # ä¿å­˜åŸå§‹ç­”æ¡ˆæ–‡æœ¬ï¼ˆä¸tokenizeï¼Œåç»­ç›´æ¥ç”¨äºæå–ç­”æ¡ˆï¼‰
        model_inputs["answer_text"] = examples["answer"]
        # ä¹Ÿä¿ç•™åŸå§‹é—®é¢˜æ–‡æœ¬ï¼ˆç”¨äºQwenèŠå¤©æ¨¡æ¿é‡å»ºpromptï¼‰
        model_inputs["question"] = questions

        return model_inputs

    # è®­ç»ƒé›†ï¼šå‰200ä¸ªæ ·æœ¬ï¼ˆå¿«é€Ÿå®éªŒï¼‰ï¼›æµ‹è¯•é›†ï¼šå‰50ä¸ªæ ·æœ¬ï¼ˆå¿«é€Ÿè¯„ä¼°ï¼‰
    tokenized_train_dataset = (
        dataset["train"]
        .select(range(200))
        .map(preprocess_function, batched=True, remove_columns=["question", "answer"])
    )
    tokenized_test_dataset = (
        dataset["test"]
        .select(range(50))
        .map(preprocess_function, batched=True, remove_columns=["question", "answer"])
    )

    # Few-shotæ± ï¼šç”¨äºGSM8K QwenèŠå¤©æ¨¡æ¿çš„ç¤ºä¾‹é‡‡æ ·
    fewshot_split = gsm8k_few_shot_split if gsm8k_few_shot_split in dataset else "train"
    gsm8k_fewshot_pool = dataset[fewshot_split].select(range(200)).to_dict()  # åŒ…å«question/answer
    # ä¸ä½¿ç”¨set_formatï¼Œä¿æŒåŸå§‹æ ¼å¼
    # DataLoaderä¼šè‡ªåŠ¨å°†input_idsç­‰è½¬ä¸ºtensorï¼Œanswer_textä¿æŒä¸ºå­—ç¬¦ä¸²åˆ—è¡¨

    # ============================================================================
    # ğŸ¯ BFCL Data Loading (if enabled)
    # ============================================================================
    bfcl_dataset = None
    if use_bfcl_eval:
        if is_main_process:
            print(f"\nğŸ¯ Loading BFCL dataset: {bfcl_data_path}")
        try:
            from bfcl_data_utils import load_bfcl_dataset

            bfcl_dataset = load_bfcl_dataset(bfcl_data_path, tokenizer)
            if is_main_process:
                print(f"âœ… BFCL dataset loaded: {len(bfcl_dataset)} samples")
        except Exception as e:
            if is_main_process:
                print(f"âŒ Failed to load BFCL dataset: {e}")
                print("Continuing with GSM8K only...")
            bfcl_dataset = None
            use_bfcl_eval = False

    # ============================================================================
    # ğŸ¯ MBPP Data Loading (if enabled)
    # ============================================================================
    mbpp_dataset = None
    if use_mbpp_eval:
        if is_main_process:
            print(f"\nğŸ¯ Loading MBPP dataset: {mbpp_data_path}")
        try:
            from mbpp_data_utils import MBPPDataset
            mbpp_dataset = MBPPDataset(mbpp_data_path, tokenizer)
            if is_main_process:
                print(f"âœ… MBPP dataset loaded: {len(mbpp_dataset)} samples")
        except Exception as e:
            if is_main_process:
                print(f"âŒ Failed to load MBPP dataset: {e}")
                print("Continuing without MBPP...")
            mbpp_dataset = None
            use_mbpp_eval = False
    
    # åˆå§‹num_tasksè®¾ç½®ï¼ˆåç»­ä¼šæ ¹æ®æ˜¯å¦ä½¿ç”¨BFCL/MBPPå’Œåˆ†å¸ƒå¼è°ƒæ•´ï¼‰
    # é»˜è®¤ç”¨trainï¼›è‹¥éœ€åœ¨è¿­ä»£ä¸­è¯„ä¼°testå­é›†ï¼Œå¯åœ¨ä¸‹æ–¹æ›¿æ¢
    iter_tokenized_dataset = tokenized_train_dataset
    num_tasks = len(iter_tokenized_dataset)
    if dist_enabled and world_size > 1:
        num_tasks = num_tasks * world_size  # åˆ†å¸ƒå¼èšåˆåçš„æ€»ä»»åŠ¡æ•°

    # --- Evaluation Setup (IDENTICAL TO ORIGINAL) ---
        if is_main_process and os.environ.get("VERBOSE_EVAL", "0") == "1":
            print("Setting up evaluation environment...")
    model_skeleton.to(device)

    if dist_enabled:
        model_skeleton.to(torch.float32)
        ddp_kwargs = {}
        if device.type == "cuda":
            ddp_kwargs.update(device_ids=[device.index], output_device=device.index)
        model_skeleton = DDP(model_skeleton, **ddp_kwargs)
        model_skeleton.to(torch.bfloat16)
    else:
        model_skeleton.to(torch.bfloat16)

    model_skeleton.eval()

    # ============================================================================
    # ğŸ¯ Create Evaluation Functions (GSM8K or Multi-Task)
    # ============================================================================
    if (use_bfcl_eval and bfcl_dataset is not None) or (use_mbpp_eval and mbpp_dataset is not None) or (use_mult4_eval or use_mult5_eval or use_bool_eval):
        # Multi-task evaluation: GSM8K + (BFCL) + (MBPP) + (DoT)
        if is_main_process and os.environ.get("VERBOSE_EVAL", "0") == "1":
            task_names = ["GSM8K"]
            if bfcl_dataset is not None and use_bfcl_eval:
                task_names.append("BFCL")
            if mbpp_dataset is not None and use_mbpp_eval:
                task_names.append("MBPP")
            if use_mult4_eval:
                task_names.append("4x4 Mult.")
            if use_mult5_eval:
                task_names.append("5x5 Mult.")
            if use_bool_eval:
                task_names.append("Boolean")
            print(f"\nğŸ¯ Creating Multi-Task Evaluation ({' + '.join(task_names)})")
            print(f"  GSM8K weight: {gsm8k_weight}")
            if use_bfcl_eval and bfcl_dataset is not None:
                print(f"  BFCL weight: {bfcl_weight}")
            if use_mbpp_eval and mbpp_dataset is not None:
                print(f"  MBPP weight: {mbpp_weight}")
            if use_mult4_eval:
                print(f"  4x4 Mult. weight: {mult4_weight}")
            if use_mult5_eval:
                print(f"  5x5 Mult. weight: {mult5_weight}")
            if use_bool_eval:
                print(f"  Boolean weight: {bool_weight}")
        
        # ä»»åŠ¡æƒé‡å­—å…¸ï¼ˆå¦‚éœ€ç”¨åˆ°ï¼‰
        task_weights_dict = {"gsm8k": gsm8k_weight}
        if use_bfcl_eval and bfcl_dataset is not None:
            task_weights_dict["bfcl"] = bfcl_weight
        if use_mbpp_eval and mbpp_dataset is not None:
            task_weights_dict["mbpp"] = mbpp_weight
        if use_mult4_eval:
            task_weights_dict["mult4"] = mult4_weight
        if use_mult5_eval:
            task_weights_dict["mult5"] = mult5_weight
        if use_bool_eval:
            task_weights_dict["bool"] = bool_weight

        # è‹¥æŒ‡å®šä½¿ç”¨testå­é›†è¿›è¡Œè¿­ä»£è¯„ä¼°
        if eval_on_test_subset:
            iter_tokenized_dataset = tokenized_test_dataset
            num_tasks = len(iter_tokenized_dataset)

        train_eval_fn = create_multi_task_evaluation_fn(
            model_skeleton,
            param_shapes,
            iter_tokenized_dataset,
            bfcl_dataset,
            tokenizer,
            task_weights=task_weights_dict,
            distributed=dist_enabled,
            world_size=world_size,
            mbpp_dataset=mbpp_dataset,
            rank=rank,
            eval_subset_size=eval_subset_size,
            use_mult4_eval=use_mult4_eval,
            use_mult5_eval=use_mult5_eval,
            use_bool_eval=use_bool_eval,
            gsm8k_qwen_chat=gsm8k_qwen_chat,
            gsm8k_few_shot_k=gsm8k_few_shot_k,
            gsm8k_few_shot_dataset=gsm8k_fewshot_pool,
        )

        # Test evaluation: GSM8K only (for compatibility)
        test_eval_fn = create_evaluation_fn_for_llm(
            model_skeleton,
            param_shapes,
            tokenized_test_dataset,
            tokenizer,
            distributed=dist_enabled,
            world_size=world_size,
            rank=rank,
            eval_subset_size=None,
            gsm8k_qwen_chat=gsm8k_qwen_chat,
            gsm8k_few_shot_k=gsm8k_few_shot_k,
            gsm8k_few_shot_dataset=gsm8k_fewshot_pool,
        )

        # Update num_tasks for competitive normalization
        # Multi-task: sum of actual samples per task (considering eval_subset_size and dataset size)
        # åªè®¡ç®—æƒé‡>0çš„ä»»åŠ¡
        if eval_subset_size is not None:
            num_tasks = 0
            if task_weights_dict.get('gsm8k', 0.0) > 0:
                num_tasks += min(eval_subset_size, len(tokenized_train_dataset))
            if use_bfcl_eval and bfcl_dataset is not None and task_weights_dict.get('bfcl', 0.0) > 0:
                num_tasks += min(eval_subset_size, len(bfcl_dataset))
            if use_mbpp_eval and mbpp_dataset is not None and task_weights_dict.get('mbpp', 0.0) > 0:
                num_tasks += min(eval_subset_size, len(mbpp_dataset))
            if use_mult4_eval and task_weights_dict.get('mult4', 0.0) > 0:
                num_tasks += eval_subset_size  # DoTä»»åŠ¡åœ¨çº¿ç”Ÿæˆï¼Œæ€»æ˜¯eval_subset_size
            if use_mult5_eval and task_weights_dict.get('mult5', 0.0) > 0:
                num_tasks += eval_subset_size
            if use_bool_eval and task_weights_dict.get('bool', 0.0) > 0:
                num_tasks += eval_subset_size
        else:
            # åªè®¡ç®—æƒé‡>0çš„ä»»åŠ¡çš„æ ·æœ¬æ•°
            num_tasks = 0
            if task_weights_dict.get('gsm8k', 0.0) > 0:
                num_tasks += len(tokenized_train_dataset)
            if use_bfcl_eval and bfcl_dataset is not None and task_weights_dict.get('bfcl', 0.0) > 0:
                num_tasks += len(bfcl_dataset)
            if use_mbpp_eval and mbpp_dataset is not None and task_weights_dict.get('mbpp', 0.0) > 0:
                num_tasks += len(mbpp_dataset)
            # DoTåœ¨çº¿ä»»åŠ¡ï¼šè‹¥æœªé‡‡æ ·å­é›†ï¼ŒæŒ‰è¯„ä¼°é»˜è®¤æ•°é‡ï¼ˆä¸eval_subset_sizeç­‰åŒæˆ–20ï¼‰
            default_dot = 20
            if use_mult4_eval and task_weights_dict.get('mult4', 0.0) > 0:
                num_tasks += default_dot
            if use_mult5_eval and task_weights_dict.get('mult5', 0.0) > 0:
                num_tasks += default_dot
            if use_bool_eval and task_weights_dict.get('bool', 0.0) > 0:
                num_tasks += default_dot
    else:
        # Single-task evaluation: GSM8K only
        if is_main_process and os.environ.get("VERBOSE_EVAL", "0") == "1":
            print("\nğŸ“Š Creating GSM8K-only Evaluation")

        # è‹¥æŒ‡å®šä½¿ç”¨testå­é›†è¿›è¡Œè¿­ä»£è¯„ä¼°
        if eval_on_test_subset:
            iter_tokenized_dataset = tokenized_test_dataset
            num_tasks = len(iter_tokenized_dataset)

        train_eval_fn = create_evaluation_fn_for_llm(
            model_skeleton,
            param_shapes,
            iter_tokenized_dataset,
            tokenizer,
            distributed=dist_enabled,
            world_size=world_size,
            rank=rank,
            eval_subset_size=eval_subset_size,
            gsm8k_qwen_chat=gsm8k_qwen_chat,
            gsm8k_few_shot_k=gsm8k_few_shot_k,
            gsm8k_few_shot_dataset=gsm8k_fewshot_pool,
        )
        test_eval_fn = create_evaluation_fn_for_llm(
            model_skeleton,
            param_shapes,
            tokenized_test_dataset,
            tokenizer,
            distributed=dist_enabled,
            world_size=world_size,
            rank=rank,
            eval_subset_size=None,
            gsm8k_qwen_chat=gsm8k_qwen_chat,
            gsm8k_few_shot_k=gsm8k_few_shot_k,
            gsm8k_few_shot_dataset=gsm8k_fewshot_pool,
        )

        # num_taskså·²ç»åœ¨å‰é¢è®¾ç½®ä¸ºlen(tokenized_train_dataset)

    # --- Archive Sharding Setup (IDENTICAL TO ORIGINAL) ---
    archive_sharding = None
    scores_sharding = None
    mesh_context = nullcontext()
    cpu_archive_device = None

    if archive_backend == "gpu" and Mesh and NamedSharding and PartitionSpec:
        available_jax_devices = jax.devices()
        if len(available_jax_devices) > 1 and pop_size > 1:
            shard_axis_size = min(len(available_jax_devices), pop_size)
            if pop_size % shard_axis_size != 0:
                shard_axis_size = math.gcd(pop_size, shard_axis_size)

            if shard_axis_size and shard_axis_size > 1:
                shard_mesh = Mesh(
                    np.array(available_jax_devices[:shard_axis_size]),
                    ("archive_shard",),
                )
                archive_sharding = NamedSharding(
                    shard_mesh, PartitionSpec("archive_shard", None)
                )
                scores_sharding = NamedSharding(
                    shard_mesh, PartitionSpec("archive_shard", None)
                )
                mesh_context = shard_mesh
                if is_main_process:
                    print(f"Sharding archive across {shard_axis_size} JAX devices.")
    elif archive_backend == "cpu":
        cpu_devices = jax.devices("cpu")
        if not cpu_devices:
            raise RuntimeError("No CPU devices available for archive_backend='cpu'")
        cpu_archive_device = cpu_devices[0]

    with mesh_context:
        if is_main_process:
            print("Setup complete. Starting evolution with Sparsity-Aware Selection.")
            if enable_pruning:
                if use_dynamic_sparsity:
                    print(f"ğŸ”„ Dynamic Sparsity ENABLED: method={pruning_method}")
                    print(
                        f"   Sparsity range: [{sparsity_min:.2f}, {sparsity_max:.2f}]"
                    )
                    print(f"   First cycle: {sparsity_t0} iterations")
                    print(f"   Cycle multiplier: {sparsity_t_mult}x")
                else:
                    print(
                        f"ğŸ”ª Pruning ENABLED: method={pruning_method}, target_sparsity={pruning_sparsity}"
                    )
            print(f"ğŸ“Š Scoring weights: Ï‰={omega} (fitness), Î²={beta} (sparsity)")

        # --- JIT Compilation of Update Function ---
        if archive_sharding is not None:
            update_archive_fn = jax.jit(
                update_archive_with_sparsity,
                static_argnums=(
                    4,
                    5,
                    6,
                    7,
                    8,
                    9,
                ),  # alpha, omega, beta, tau, num_tasks, epsilon
                in_shardings=(
                    None,
                    None,
                    archive_sharding,
                    scores_sharding,
                    None,
                    None,
                    None,
                    None,
                    None,
                    None,
                ),
                out_shardings=(archive_sharding, scores_sharding),
            )
        elif archive_backend == "cpu":
            update_archive_fn = jax.jit(
                update_archive_with_sparsity,
                static_argnums=(4, 5, 6, 7, 8, 9),
                backend="cpu",
            )
        else:
            update_archive_fn = jax.jit(
                update_archive_with_sparsity, static_argnums=(4, 5, 6, 7, 8, 9)
            )

        results = []

        if is_main_process:
            # --- Archive Initialization (Main Process Only) ---
            print(f"--- Initializing Archive on Main Process (Rank {rank}) ---")
            archive_shape = (pop_size, num_params_llm)
            scores_shape = (pop_size, num_tasks)  # ä½¿ç”¨num_tasksï¼Œæ”¯æŒå•ä»»åŠ¡å’Œå¤šä»»åŠ¡
            if archive_sharding is not None:
                archive = _sharded_zeros(archive_shape, jnp.bfloat16, archive_sharding)
                scores = _sharded_zeros(scores_shape, jnp.float32, scores_sharding)
            elif archive_backend == "cpu":
                with default_device_ctx(cpu_archive_device):
                    archive = jnp.zeros(archive_shape, dtype=jnp.bfloat16)
                    scores = jnp.zeros(scores_shape, dtype=jnp.float32)
            else:
                archive = jnp.zeros(archive_shape, dtype=jnp.bfloat16)
                scores = jnp.zeros(scores_shape, dtype=jnp.float32)

            # Evaluate initial models to populate the archive
            for seed_idx, model in enumerate((model_1, model_2)):
                score = train_eval_fn(model)
                archive, scores = update_archive_fn(
                    score,
                    model,
                    archive,
                    scores,
                    alpha,
                    omega,
                    beta,
                    tau,
                    num_tasks,
                    epsilon,
                )
                if async_coordinator is not None:
                    owner_idx = seed_idx % async_coordinator.num_nodes
                    async_coordinator.commit(model, owner_idx)
                    if async_coordinator.synced():
                        archive = _apply_async_sync_to_matrix(
                            archive, async_coordinator
                        )
        else:
            archive = None
            scores = None
            if dist_enabled:
                for model in (model_1, model_2):
                    train_eval_fn(model)

        if dist_enabled:
            dist.barrier()

        # --- Main Evolution Loop ---
        for run in range(runs):
            if is_main_process:
                print(f"--- Starting Run {run+1}/{runs} ---")
                results.append(defaultdict(list))

            seed = 42 + run
            key = jax.random.PRNGKey(seed)

            progress_bar = tqdm(
                range(total_forward_passes),
                desc="Forward passes",
                disable=not is_main_process,
            )

            for i in progress_bar:
                k1, k2, k3, key = jax.random.split(key, 4)
                shard_owner = None
                if async_coordinator is not None:
                    shard_owner = i % async_coordinator.num_nodes

                # --- Child Generation (Main Process Only) ---
                if is_main_process:
                    # Monitor JAX computation time to detect hangs
                    import time

                    start_time = time.time()

                    # 1. Select parents based on Total Score
                    parents_bf16 = sample_parents_with_sparsity(
                        archive,
                        scores,
                        k1,
                        alpha,
                        num_tasks,
                        omega,
                        beta,
                        tau,
                        use_matchmaker,
                        epsilon,
                    )
                    p1_idx = parents_bf16[2]
                    p2_idx = parents_bf16[3]
                    parents_f32 = (
                        parents_bf16[0].astype(jnp.float32),
                        parents_bf16[1].astype(jnp.float32),
                    )

                    # Check if JAX computation is taking too long
                    jax_time = time.time() - start_time
                    if jax_time > 60:  # Warning if JAX computation takes >60 seconds
                        print(
                            f"âš ï¸  WARNING: JAX parent selection took {jax_time:.1f}s (may cause timeout)"
                        )

                    # 2. Apply pruning to parents (NEW)
                    # ä½¿ç”¨PyTorch+NumPyçš„å‰ªææ–¹æ¡ˆï¼ˆå·²ä¿®å¤bfloat16é—®é¢˜ï¼‰
                    if enable_pruning:
                        # ğŸ”„ åŠ¨æ€è®¡ç®—å½“å‰è¿­ä»£çš„ç¨€ç–åº¦
                        if use_dynamic_sparsity:
                            current_pruning_sparsity = calculate_dynamic_sparsity(
                                current_iteration=i,
                                eta_min=sparsity_min,
                                eta_max=sparsity_max,
                                t0=sparsity_t0,
                                t_mult=sparsity_t_mult,
                            )
                            # æ—¥å¿—ï¼šæ¯10æ­¥æç¤ºä¸€æ¬¡ï¼Œå¹¶åœ¨æ¯ä¸ªå‘¨æœŸèµ·ç‚¹æç¤ºé‡å¯
                            need_periodic_log = i % 10 == 0
                            # è®¡ç®—å½“å‰å‘¨æœŸé•¿åº¦ï¼Œç”¨äºæ£€æµ‹é‡å¯ç‚¹ï¼ˆWarm Restartï¼‰
                            ti_len = int(sparsity_t0)
                            remain = int(i)
                            while remain >= ti_len:
                                remain -= ti_len
                                ti_len *= int(sparsity_t_mult)
                            is_restart_step = remain == 0
                            if need_periodic_log or is_restart_step:
                                prefix = "[Restart] " if is_restart_step else ""
                                print(
                                    f"\nğŸ”„ {prefix}[Iter {i}] Dynamic sparsity: {current_pruning_sparsity:.4f}"
                                )
                        else:
                            current_pruning_sparsity = pruning_sparsity

                        try:
                            parents_f32 = (
                                prune_with_wanda(
                                    parents_f32[0].astype(jnp.bfloat16),
                                    model_skeleton,
                                    param_shapes,
                                    tokenizer,
                                    current_pruning_sparsity,
                                    device,
                                ).astype(jnp.float32),
                                prune_with_wanda(
                                    parents_f32[1].astype(jnp.bfloat16),
                                    model_skeleton,
                                    param_shapes,
                                    tokenizer,
                                    current_pruning_sparsity,
                                    device,
                                ).astype(jnp.float32),
                            )
                        except Exception as e:
                            print(f"âš ï¸  Pruning failed at iteration {i}: {e}")
                            # Continue without pruning

                    # 3. Crossover
                    if use_crossover:
                        if use_splitpoint:
                            child_f32 = crossover(parents_f32, k2)
                        else:
                            child_f32 = crossover_without_splitpoint(parents_f32, k2)
                    else:
                        child_f32 = parents_f32[0]

                    # 4. Mutation
                    child_f32 = mutate(child_f32, k3)
                    child_bf16_main = child_f32.astype(jnp.bfloat16)
                    if async_coordinator is not None and shard_owner is not None:
                        child_bf16_main = async_coordinator.prepare_candidate(
                            child_bf16_main, shard_owner
                        )

                    # Log sparsity if requested
                    if log_sparsity_stats and (i + 1) % 10 == 0:
                        child_sparsity = compute_sparsity(child_bf16_main, epsilon)
                        print(f"\n[Iter {i+1}] Child sparsity: {child_sparsity:.4f}")
                else:
                    child_bf16_main = None

                # --- Broadcast Child (IDENTICAL TO ORIGINAL) ---
                if dist_enabled:
                    if is_main_process:
                        child_tensor = torch.from_numpy(
                            np.array(child_bf16_main.astype(jnp.float32))
                        )
                    else:
                        child_tensor = torch.empty(num_params_llm, dtype=torch.float32)

                    # Move to GPU for NCCL backend
                    child_tensor = child_tensor.to(device)
                    dist.broadcast(child_tensor, src=0)
                    # Move back to CPU for numpy conversion
                    child_tensor = child_tensor.cpu()
                    child_bf16 = jnp.array(child_tensor.numpy()).astype(jnp.bfloat16)
                else:
                    child_bf16 = child_bf16_main

                # --- Evaluation (All Processes) ---
                score = train_eval_fn(child_bf16)

                # --- Archive Update (Main Process Only) ---
                if is_main_process:
                    archive, scores = update_archive_fn(
                        score,
                        child_bf16,
                        archive,
                        scores,
                        alpha,
                        omega,
                        beta,
                        tau,
                        num_tasks,
                        epsilon,
                    )
                    if async_coordinator is not None and shard_owner is not None:
                        async_coordinator.commit(child_bf16, shard_owner)
                        if async_coordinator.synced():
                            archive = _apply_async_sync_to_matrix(
                                archive, async_coordinator
                            )

                    # Record iteration statistics (every 10 steps to reduce overhead)
                    if (i + 1) % 10 == 0 or i == 0:
                        # Compute all archive statistics efficiently
                        archive_fitness_vals = jnp.mean(scores, axis=1)
                        archive_sparsity_vals = jnp.array(
                            [
                                compute_sparsity(archive[j], epsilon)
                                for j in range(pop_size)
                            ]
                        )
                        archive_total_scores = compute_total_scores(
                            archive, scores, omega, beta, tau, alpha, num_tasks, epsilon
                        )

                        iteration_stats = {
                            "iteration": i + 1,
                            "child_fitness": float(jnp.mean(score)),
                            "child_sparsity": float(
                                compute_sparsity(child_bf16, epsilon)
                            ),
                            "parent_indices": [int(p1_idx), int(p2_idx)],
                            "fitness_vector": [float(x) for x in jnp.array(score)],
                            "archive_fitness_mean": float(
                                jnp.mean(archive_fitness_vals)
                            ),
                            "archive_fitness_max": float(jnp.max(archive_fitness_vals)),
                            "archive_sparsity_mean": float(
                                jnp.mean(archive_sparsity_vals)
                            ),
                            "archive_sparsity_min": float(
                                jnp.min(archive_sparsity_vals)
                            ),
                            "archive_total_score_mean": float(
                                jnp.mean(archive_total_scores)
                            ),
                            "archive_total_score_max": float(
                                jnp.max(archive_total_scores)
                            ),
                        }
                        results[run]["iterations"].append(iteration_stats)

                        # æŒ‰æ­¥æŒä¹…åŒ–å®Œæ•´fitnessè®°å½•åˆ°ç‹¬ç«‹æ–‡ä»¶ï¼ˆæ¯10æ­¥ï¼‰
                        try:
                            log_dir = os.path.join(RESULTS_DIR, "fitness_logs")
                            os.makedirs(log_dir, exist_ok=True)
                            # å½’ä¸€åŒ–åçš„archive fitnessï¼ˆå½“å‰scoresçŸ©é˜µï¼‰
                            fitness_vector = compute_normalized_fitness(
                                scores, alpha, num_tasks
                            )
                            log_path = os.path.join(
                                log_dir, f"fitness_run{run+1}_step{i+1}.npz"
                            )
                            np.savez(
                                log_path,
                                iteration=i + 1,
                                run=run + 1,
                                parent_indices=np.array([int(p1_idx), int(p2_idx)], dtype=np.int32),
                                child_scores=np.array(score, dtype=np.float32),
                                archive_fitness_normalized=np.array(
                                    fitness_vector, dtype=np.float32
                                ),
                                archive_total_scores=np.array(
                                    archive_total_scores, dtype=np.float32
                                ),
                                archive_scores=np.array(scores, dtype=np.float32),
                            )
                        except Exception as _e:
                            # å®‰å…¨å¤±è´¥ï¼Œä¸ä¸­æ–­ä¸»æµç¨‹
                            pass

                # --- GPU Memory Cleanup (Every 100 steps to prevent memory leak) ---
                if (i + 1) % 100 == 0:
                    if is_main_process:
                        print(f"[Step {i+1}] Cleaning GPU memory...")
                    torch.cuda.empty_cache()
                    if dist_enabled:
                        dist.barrier()  # Ensure all processes clean memory together

                if dist_enabled:
                    dist.barrier()

                # --- Periodic Full Archive Reporting: print fitness instead of GSM8K accuracy ---
                if (i + 1) % 10 == 0:
                    if is_main_process:
                        print(
                            f"\n--- [Step {i+1}/{total_forward_passes}] Archive fitness snapshot ---"
                        )

                        # Compute per-individual fitness (normalized) on current training scores
                        fitness_vector = compute_normalized_fitness(
                            scores, alpha, num_tasks
                        )

                        for j in range(pop_size):
                            ind_fitness = float(fitness_vector[j])
                            if log_sparsity_stats:
                                ind_sparsity = compute_sparsity(archive[j], epsilon)
                                print(
                                    f"  > Archive Individual {j+1}/{pop_size} | "
                                    f"Fitness: {ind_fitness:.4f} | Sparsity: {ind_sparsity:.4f}"
                                )
                            else:
                                print(
                                    f"  > Archive Individual {j+1}/{pop_size} | Fitness: {ind_fitness:.4f}"
                                )

                            # Record fitness snapshot (for analysis)
                            if "fitness_evaluations" not in results[run]:
                                results[run]["fitness_evaluations"] = []

                            results[run]["fitness_evaluations"].append(
                                {
                                    "iteration": i + 1,
                                    "individual": j + 1,
                                    "fitness": ind_fitness,
                                    "sparsity": (
                                        float(compute_sparsity(archive[j], epsilon))
                                        if log_sparsity_stats
                                        else None
                                    ),
                                }
                            )

                # --- Periodic Checkpoint Save (Every 2500 steps) ---
                if (i + 1) % 2500 == 0 and is_main_process and os.environ.get("VERBOSE_EVAL", "0") == "1":
                    from datetime import datetime

                    checkpoint_dir = os.path.join(RESULTS_DIR, "checkpoints")
                    os.makedirs(checkpoint_dir, exist_ok=True)

                    checkpoint_path = os.path.join(
                        checkpoint_dir,
                        f"checkpoint_run{run+1}_step{i+1}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl",
                    )

                    checkpoint_data = {
                        "iteration": i + 1,
                        "run": run + 1,
                        "archive": np.array(archive),
                        "scores": np.array(scores),
                        "results": results,
                    }

                    with open(checkpoint_path, "wb") as f:
                        pickle.dump(checkpoint_data, f)

                    print(f"ğŸ’¾ Checkpoint saved: {checkpoint_path}")

        if dist_enabled:
            dist.barrier()

        # --- Save Final Best Model (Main Process Only, optional) ---
        if is_main_process and save_best_model:
            if runs > 0:
                # Find best based on Total Score
                total_scores = compute_total_scores(
                    archive, scores, omega, beta, tau, alpha, num_tasks, epsilon
                )
                best_individual_idx = jnp.argmax(total_scores)
                best_params = archive[best_individual_idx]

                os.makedirs(RESULTS_DIR, exist_ok=True)

                from datetime import datetime

                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                # Compact tags for filename for easier experiment identification
                tags = [
                    f"pop{pop_size}",
                    f"fp{total_forward_passes}",
                    f"runs{runs}",
                    f"w{omega:.2f}",
                    f"b{beta:.2f}",
                    f"t{tau:.2f}",
                ]
                if eval_subset_size is not None:
                    tags.append(f"subset{eval_subset_size}")
                # Task weights
                tags.append(f"gsm{gsm8k_weight:.2f}")
                if use_bfcl_eval and bfcl_dataset is not None:
                    tags.append(f"bfcl{bfcl_weight:.2f}")
                if use_mbpp_eval and mbpp_dataset is not None:
                    tags.append(f"mbpp{mbpp_weight:.2f}")
                if use_mult4_eval:
                    tags.append(f"m4{mult4_weight:.2f}")
                if use_mult5_eval:
                    tags.append(f"m5{mult5_weight:.2f}")
                if use_bool_eval:
                    tags.append(f"bool{bool_weight:.2f}")
                # Pruning/dynamic sparsity
                if use_dynamic_sparsity:
                    tags.append(f"dyn{sparsity_min:.2f}-{sparsity_max:.2f}")
                elif pruning_sparsity > 0:
                    tags.append(f"prune_{pruning_method}_{pruning_sparsity:.2f}")

                tag_str = "_".join(tags)
                save_path = os.path.join(
                    RESULTS_DIR,
                    f"best_model_{tag_str}_{timestamp}.npz",
                )

                print(
                    f"\nğŸ† Saving the best model (by Total Score) from the last run to: {save_path}"
                )

                # Log final statistics
                best_sparsity = compute_sparsity(best_params, epsilon)
                best_fitness = float(jnp.mean(scores[best_individual_idx]))
                best_total_score = float(total_scores[best_individual_idx])

                print(f"   Final fitness: {best_fitness:.4f}")
                print(f"   Final sparsity: {best_sparsity:.4f}")
                print(f"   Final total score: {best_total_score:.4f}")

                # Record final best model info
                results[run]["final_best_model"] = {
                    "save_path": save_path,
                    "fitness": best_fitness,
                    "sparsity": float(best_sparsity),
                    "total_score": best_total_score,
                    "individual_idx": int(best_individual_idx),
                }

                jnp.savez(save_path, params=best_params)
                print("âœ… Model saved successfully.")

        return results


def _apply_async_sync_to_matrix(
    archive: jnp.ndarray, coordinator: AsyncShardCoordinator
) -> jnp.ndarray:
    """Broadcast synchronised shard slices across the population archive."""
    if archive is None:
        return archive

    global_params = coordinator.global_params()
    if global_params is None:
        return archive

    dtype = archive.dtype
    synced_nodes = coordinator.synced_nodes()
    updated = archive
    for idx in synced_nodes:
        start, end = coordinator.shard_slices[idx]
        shard_vals = jnp.asarray(global_params[start:end], dtype=dtype)
        shard_vals = jnp.broadcast_to(shard_vals, (archive.shape[0], end - start))
        updated = updated.at[:, start:end].set(shard_vals)
    return updated


def _sharded_zeros(shape: tuple[int, ...], dtype, sharding):
    """Helper function for sharded array initialization (IDENTICAL TO ORIGINAL)"""
    if sharding is None or make_array_from_callback is None:
        return jnp.zeros(shape, dtype=dtype)

    def _cb(index):
        slice_dims = []
        for dim_idx, dim_size in zip(index, shape):
            if isinstance(dim_idx, slice):
                start = 0 if dim_idx.start is None else dim_idx.start
                stop = dim_size if dim_idx.stop is None else dim_idx.stop
                slice_dims.append(stop - start)
            else:
                slice_dims.append(1)
        return jnp.zeros(tuple(slice_dims), dtype=dtype)

    return make_array_from_callback(shape, sharding, _cb)


def _sharded_zeros(shape: tuple[int, ...], dtype, sharding):
    """Helper function for sharded array initialization (IDENTICAL TO ORIGINAL)"""
    if sharding is None or make_array_from_callback is None:
        return jnp.zeros(shape, dtype=dtype)

    def _cb(index):
        slice_dims = []
        for dim_idx, dim_size in zip(index, shape):
            if isinstance(dim_idx, slice):
                start = 0 if dim_idx.start is None else dim_idx.start
                stop = dim_size if dim_idx.stop is None else dim_idx.stop
                slice_dims.append(stop - start)
            else:
                slice_dims.append(1)
        return jnp.zeros(tuple(slice_dims), dtype=dtype)

    return make_array_from_callback(shape, sharding, _cb)


# ========== BFCLè¯„ä¼°å‡½æ•° ==========


# ========== BFCLè¯„ä¼°å‡½æ•° ==========
def create_bfcl_evaluation_fn(
    model_skeleton,
    param_shapes,
    bfcl_dataset,
    tokenizer: AutoTokenizer,
    batch_size: int = 4,
    distributed: bool = False,
    world_size: int = 1,
    rank: int = 0,
    eval_subset_size: int = None,
    return_subset_only: bool = False,  # å¤šä»»åŠ¡è¯„ä¼°æ—¶è®¾ä¸ºTrueï¼Œä¸è¿›è¡Œåˆ†å¸ƒå¼èšåˆ
):
    """
    åˆ›å»ºBFCL (Berkeley Function Calling Leaderboard) è¯„ä¼°å‡½æ•°

    è¯„ä¼°å‡½æ•°è°ƒç”¨èƒ½åŠ›ï¼š
    1. ç»™å®šuser queryå’Œå¯ç”¨functions
    2. æ¨¡å‹ç”Ÿæˆfunction callï¼ˆJSONæ ¼å¼ï¼‰
    3. ä½¿ç”¨AST matchingè¯„ä¼°æ­£ç¡®æ€§

    Returns:
        evaluation_fn: è¿”å›æ¯ä¸ªæ ·æœ¬çš„å¾—åˆ† (1.0=æ­£ç¡®, 0.0=é”™è¯¯)
    """
    from bfcl_eval_utils import extract_function_call, evaluate_function_call
    from bfcl_data_utils import bfcl_collate_fn
    from torch.utils.data import DataLoader, Subset
    import random

    device = next(model_skeleton.parameters()).device
    iteration_counter = {"count": 0}

    def evaluation_fn(flat_params: jnp.ndarray) -> jnp.ndarray:
        """è¯„ä¼°BFCLä»»åŠ¡"""
        iteration_counter["count"] += 1

        # é‡‡æ ·å­é›†
        if eval_subset_size is not None and eval_subset_size < len(bfcl_dataset):
            indices = random.sample(range(len(bfcl_dataset)), eval_subset_size)
            eval_dataset = Subset(bfcl_dataset, indices)
            if rank == 0:
                print(f"  [BFCL] é‡‡æ · {eval_subset_size}/{len(bfcl_dataset)} æ ·æœ¬")
        else:
            eval_dataset = bfcl_dataset

        # DataLoader (ä½¿ç”¨BFCLä¸“ç”¨çš„collate_fn)
        dataloader = DataLoader(
            eval_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=0,
            collate_fn=bfcl_collate_fn,
        )

        # é‡å»ºæ¨¡å‹å‚æ•°ï¼ˆä½¿ç”¨å’ŒGSM8Kç›¸åŒçš„æ–¹å¼ï¼‰
        base_model = (
            model_skeleton.module
            if hasattr(model_skeleton, "module")
            else model_skeleton
        )
        restored_model = jax_flattened_to_pytorch_model(
            flat_params, base_model, param_shapes
        )
        restored_model.eval()

        # è¯„ä¼°
        all_scores = []
        with torch.no_grad():
            for batch in dataloader:
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                ground_truth_calls = batch["ground_truth"]

                # Generate
                generated_ids = restored_model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=256,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )

                # Decode
                generated_texts = tokenizer.batch_decode(
                    generated_ids[:, input_ids.shape[1] :], skip_special_tokens=True
                )

                # Evaluate each sample
                for gen_text, gt_call in zip(generated_texts, ground_truth_calls):
                    try:
                        # Extract function call from generated text
                        pred_call = extract_function_call(gen_text)
                        # Evaluate using AST matching
                        is_correct = evaluate_function_call(pred_call, gt_call)
                        all_scores.append(1.0 if is_correct else 0.0)
                    except Exception:
                        all_scores.append(0.0)  # Parse error = incorrect

        # åˆ†å¸ƒå¼èšåˆï¼ˆä¸GSM8Kè¯„ä¼°å‡½æ•°ä¿æŒä¸€è‡´ï¼‰
        if distributed and world_size > 1:
            scores_tensor = torch.tensor(all_scores, dtype=torch.float32, device=device)
            gathered = [torch.zeros_like(scores_tensor) for _ in range(world_size)]
            torch.distributed.all_gather(gathered, scores_tensor)
            # æˆªæ–­åˆ°eval_datasetçš„é•¿åº¦ï¼ˆä¸GSM8Kè¯„ä¼°å‡½æ•°ä¿æŒä¸€è‡´ï¼‰
            all_scores = torch.cat(gathered)[: len(eval_dataset)].cpu().numpy().tolist()

        return jnp.array(all_scores, dtype=jnp.float32)

    return evaluation_fn


# ========== MBPPè¯„ä¼°å‡½æ•° ==========
def create_mbpp_evaluation_fn(
    model_skeleton,
    param_shapes,
    mbpp_dataset,
    tokenizer: AutoTokenizer,
    batch_size: int = 4,
    distributed: bool = False,
    world_size: int = 1,
    rank: int = 0,
    eval_subset_size: int = None,
    return_subset_only: bool = False,  # å¤šä»»åŠ¡è¯„ä¼°æ—¶è®¾ä¸ºTrueï¼Œä¸è¿›è¡Œåˆ†å¸ƒå¼èšåˆ
    mbpp_qwen_chat: bool = False,
    mbpp_few_shot_k: int = 3,
    mbpp_few_shot_dataset=None,
):
    """
    åˆ›å»ºMBPP (Mostly Basic Python Problems) è¯„ä¼°å‡½æ•°
    
    è¯„ä¼°ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼š
    1. ç»™å®šé—®é¢˜æè¿°
    2. æ¨¡å‹ç”ŸæˆPythonä»£ç 
    3. æ‰§è¡Œå•å…ƒæµ‹è¯•éªŒè¯æ­£ç¡®æ€§
    
    Returns:
        evaluation_fn: è¿”å›æ¯ä¸ªæ ·æœ¬çš„å¾—åˆ† (1.0=æ‰€æœ‰æµ‹è¯•é€šè¿‡, 0.0=å¤±è´¥)
    """
    from mbpp_data_utils import mbpp_collate_fn
    from torch.utils.data import DataLoader, Subset
    import random
    import subprocess
    import tempfile
    import uuid
    
    device = next(model_skeleton.parameters()).device
    iteration_counter = {'count': 0}
    
    def safe_execute_code(code: str, tests: list, setup_code: str = "", timeout: int = 10) -> bool:
        """
        å®‰å…¨æ‰§è¡Œä»£ç å¹¶è¿è¡Œæµ‹è¯•
        
        Args:
            code: ç”Ÿæˆçš„ä»£ç 
            tests: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ï¼ˆassertè¯­å¥ï¼‰
            setup_code: æµ‹è¯•å‰ç½®ä»£ç 
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        
        Returns:
            æ˜¯å¦æ‰€æœ‰æµ‹è¯•é€šè¿‡
        """
        # æ„å»ºå®Œæ•´çš„æµ‹è¯•ç¨‹åº
        program_parts = []
        
        # æ·»åŠ setupä»£ç 
        if setup_code:
            program_parts.append(setup_code)
        
        # æ·»åŠ ç”Ÿæˆçš„ä»£ç 
        program_parts.append(code)
        
        # æ·»åŠ æµ‹è¯•ç”¨ä¾‹
        program_parts.extend(tests)
        
        # æ·»åŠ æˆåŠŸæ ‡è®°
        program_parts.append("print('__MBPP_ALL_TESTS_PASSED__')")
        
        program = "\n".join(program_parts)
        
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶æ‰§è¡Œ
        try:
            with tempfile.TemporaryDirectory() as tmpdir:
                filepath = os.path.join(tmpdir, f"{uuid.uuid4().hex}.py")
                with open(filepath, "w", encoding="utf-8") as f:
                    f.write(program)
                
                # æ‰§è¡Œä»£ç 
                result = subprocess.run(
                    ["python3", filepath],
                    capture_output=True,
                    text=True,
                    timeout=timeout,
                    env={"PYTHONDONTWRITEBYTECODE": "1"}  # ä¸ç”Ÿæˆ.pycæ–‡ä»¶
                )
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
                success = (
                    "__MBPP_ALL_TESTS_PASSED__" in (result.stdout or "") 
                    and result.returncode == 0
                )
                
                return success
                
        except subprocess.TimeoutExpired:
            return False  # è¶…æ—¶è§†ä¸ºå¤±è´¥
        except Exception:
            return False  # ä»»ä½•å¼‚å¸¸éƒ½è§†ä¸ºå¤±è´¥
    
    import re

    def _parse_first_def_name(text: str) -> str:
        m = re.search(r"^\s*def\s+([a-zA-Z_][\w]*)\s*\(", text, flags=re.M)
        return m.group(1) if m else ""

    def _clean_code_block(generated: str) -> str:
        s = generated.strip()
        fence = re.search(r"```(?:python)?\n([\s\S]*?)```", s, flags=re.I)
        if fence:
            return fence.group(1).strip()
        # fallback: from first def to end
        m = re.search(r"^\s*def\s+", s, flags=re.M)
        return s[m.start():].strip() if m else s

    def evaluation_fn(flat_params: jnp.ndarray) -> jnp.ndarray:
        """è¯„ä¼°MBPPä»»åŠ¡"""
        iteration_counter['count'] += 1
        
        # é‡‡æ ·å­é›†
        if eval_subset_size is not None and eval_subset_size < len(mbpp_dataset):
            indices = random.sample(range(len(mbpp_dataset)), eval_subset_size)
            eval_dataset = Subset(mbpp_dataset, indices)
            if rank == 0:
                print(f"  [MBPP] é‡‡æ · {eval_subset_size}/{len(mbpp_dataset)} æ ·æœ¬")
        else:
            eval_dataset = mbpp_dataset
        
        # DataLoader (ä½¿ç”¨MBPPä¸“ç”¨çš„collate_fn)
        dataloader = DataLoader(
            eval_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=0,
            collate_fn=lambda batch: mbpp_collate_fn(batch, tokenizer),
        )
        
        # é‡å»ºæ¨¡å‹å‚æ•°ï¼ˆä½¿ç”¨å’ŒGSM8K/BFCLç›¸åŒçš„æ–¹å¼ï¼‰
        base_model = (
            model_skeleton.module if hasattr(model_skeleton, "module") else model_skeleton
        )
        restored_model = jax_flattened_to_pytorch_model(
            flat_params, base_model, param_shapes
        )
        restored_model.eval()
        
        # è¯„ä¼°
        all_scores = []
        with torch.no_grad():
            for batch in dataloader:
                use_chat = bool(mbpp_qwen_chat and hasattr(tokenizer, "apply_chat_template"))
                test_lists = batch['test_list']
                setup_codes = batch['test_setup_code']
                test_imports = batch.get('test_imports', [""] * len(test_lists))
                ref_codes = batch.get('reference_code', [""] * len(test_lists))
                prompts = batch.get('prompts', None)

                if use_chat and prompts is not None:
                    # few-shot exemplarsï¼ˆä»æä¾›çš„æ•°æ®é›†é‡‡æ ·ï¼Œè‹¥æ— åˆ™ä¸ºç©ºï¼‰
                    exemplars = []
                    if mbpp_few_shot_dataset is not None and len(mbpp_few_shot_dataset) > 0:
                        import random as _rnd
                        k = max(0, int(mbpp_few_shot_k))
                        idxs = list(range(min(len(mbpp_few_shot_dataset), 64)))
                        _rnd.Random(42).shuffle(idxs)
                        exemplars = idxs[:k]

                    texts = []
                    for q, ref in zip(prompts, ref_codes):
                        expected = _parse_first_def_name(ref or "")
                        msgs = [{"role": "system", "content": "You are a helpful Python coder. Output only a valid Python function that solves the task."}]
                        # attach few-shot
                        for ex_i in exemplars:
                            ex_item = mbpp_few_shot_dataset[ex_i]
                            ex_q = ex_item.get("prompt") or ex_item.get("text") or ex_item.get("description") or ex_item.get("task_description") or ""
                            ex_a = ex_item.get("code", "")
                            if ex_q and ex_a:
                                msgs.append({"role": "user", "content": ex_q})
                                msgs.append({"role": "assistant", "content": f"```python\n{ex_a}\n```"})
                        # current question
                        # è‹¥å·²çŸ¥æœŸæœ›å‡½æ•°åï¼Œç»™å‡ºè½»æç¤º
                        if expected:
                            q = q + f"\nReturn a function named {expected}."
                        msgs.append({"role": "user", "content": q})
                        text = tokenizer.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True)
                        texts.append(text)

                    enc = tokenizer(texts, padding=True, truncation=True, max_length=1024, return_tensors="pt")
                    input_ids = enc["input_ids"].to(device)
                    attention_mask = enc["attention_mask"].to(device)
                else:
                    input_ids = batch['input_ids'].to(device)
                    attention_mask = batch['attention_mask'].to(device)
                
                # Generateä»£ç 
                generated_ids = restored_model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=512,  # MBPPå¯èƒ½éœ€è¦æ›´é•¿çš„ä»£ç 
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
                
                # Decodeç”Ÿæˆçš„ä»£ç 
                generated_codes = tokenizer.batch_decode(
                    generated_ids[:, input_ids.shape[1]:],
                    skip_special_tokens=True
                )
                
                # æ‰§è¡Œæµ‹è¯•è¯„ä¼°æ¯ä¸ªæ ·æœ¬
                for sample_idx, (gen_code, tests, setup, timport, ref) in enumerate(zip(generated_codes, test_lists, setup_codes, test_imports, ref_codes)):
                    try:
                        # æ¸…ç†ç”Ÿæˆçš„ä»£ç ï¼ˆç§»é™¤markdownä»£ç å—æ ‡è®°ç­‰ï¼‰
                        clean_code = _clean_code_block(gen_code)

                        # å‡½æ•°å aliasï¼šæœŸæœ›å â† ç”Ÿæˆå
                        expected = _parse_first_def_name(ref or "")
                        generated = _parse_first_def_name(clean_code)
                        alias_line = ""
                        if expected and generated and expected != generated:
                            alias_line = f"\n{expected} = {generated}"
                            clean_code = clean_code + alias_line
                        
                        # æ‰§è¡Œæµ‹è¯•
                        merged_setup = ((timport or "").strip() + "\n" + (setup or "").strip()).strip()
                        is_correct = safe_execute_code(clean_code, tests, merged_setup)
                        all_scores.append(1.0 if is_correct else 0.0)
                        
                        # è°ƒè¯•æ‰“å°ï¼ˆå— VERBOSE_EVAL æ§åˆ¶ï¼‰
                        if rank == 0 and os.environ.get("VERBOSE_EVAL", "0") == "1" and len(all_scores) <= 2:
                            print(f"  [MBPP] Sample {len(all_scores)}:")
                            print(f"    Generated: {gen_code[:150]}...")
                            print(f"    Tests: {tests[:3]}")
                            print(f"    Correct: {is_correct}")
                    except Exception as e:
                        all_scores.append(0.0)  # ä»»ä½•å¼‚å¸¸éƒ½è§†ä¸ºå¤±è´¥
                        if rank == 0 and os.environ.get("VERBOSE_EVAL", "0") == "1" and len(all_scores) <= 2:
                            print(f"  [MBPP] Sample {len(all_scores)}: Exception - {str(e)[:100]}")
        
        # åˆ†å¸ƒå¼èšåˆï¼ˆä¸GSM8K/BFCLè¯„ä¼°å‡½æ•°ä¿æŒä¸€è‡´ï¼‰
        if distributed and world_size > 1 and not return_subset_only:
            scores_tensor = torch.tensor(all_scores, dtype=torch.float32, device=device)
            gathered = [torch.zeros_like(scores_tensor) for _ in range(world_size)]
            torch.distributed.all_gather(gathered, scores_tensor)
            # æˆªæ–­åˆ°eval_datasetçš„é•¿åº¦
            all_scores = torch.cat(gathered)[:len(eval_dataset)].cpu().numpy().tolist()
        
        return jnp.array(all_scores, dtype=jnp.float32)
    
    return evaluation_fn


# ========== å¤šä»»åŠ¡è¯„ä¼°å‡½æ•° ==========
def create_multi_task_evaluation_fn(
    model_skeleton,
    param_shapes,
    gsm8k_dataset,
    bfcl_dataset,
    tokenizer,
    task_weights=None,
    batch_size=4,
    distributed=False,
    world_size=1,
    rank=0,
    eval_subset_size=None,
    mbpp_dataset=None,  # ğŸ†• æ–°å¢MBPPæ•°æ®é›†å‚æ•°
    use_mult4_eval: bool = False,
    use_mult5_eval: bool = False,
    use_bool_eval: bool = False,
    gsm8k_qwen_chat: bool = False,
    gsm8k_few_shot_k: int = 3,
    gsm8k_few_shot_dataset=None,
):
    """
    åˆ›å»ºå¤šä»»åŠ¡è¯„ä¼°å‡½æ•°ï¼šåŒæ—¶è¯„ä¼°GSM8Kå’ŒBFCL

    Args:
        task_weights: ä»»åŠ¡æƒé‡å­—å…¸ï¼Œä¾‹å¦‚ {"gsm8k": 0.4, "bfcl": 0.3, "mbpp": 0.3}
                     å¦‚æœä¸ºNoneï¼Œåˆ™æ‹¼æ¥æ‰€æœ‰ä»»åŠ¡çš„åˆ†æ•°
        eval_subset_size: æ¯ä¸ªä»»åŠ¡é‡‡æ ·çš„æ ·æœ¬æ•°
        mbpp_dataset: MBPPæ•°æ®é›†ï¼ˆå¦‚æœä¸ºNoneåˆ™ä¸è¯„ä¼°MBPPï¼‰
        
    Returns:
        evaluation_fn: è¿”å›æ‰€æœ‰ä»»åŠ¡çš„åˆ†æ•°æ‹¼æ¥ç»“æœ
    """
    # é»˜è®¤æƒé‡
    if task_weights is None:
        if mbpp_dataset is not None:
            task_weights = {"gsm8k": 0.4, "bfcl": 0.3, "mbpp": 0.3}
        else:
            task_weights = {"gsm8k": 0.5, "bfcl": 0.5}
    
    # åˆ›å»ºGSM8Kè¯„ä¼°å‡½æ•°
    gsm8k_eval_fn = create_evaluation_fn_for_llm(
        model_skeleton,
        param_shapes,
        gsm8k_dataset,
        tokenizer,
        batch_size=batch_size,
        distributed=distributed,
        world_size=world_size,
        rank=rank,
        eval_subset_size=eval_subset_size,
        return_subset_only=True,  # å¤šä»»åŠ¡ï¼šä¸æ‰©å±•ï¼Œç›´æ¥è¿”å›å­é›†åˆ†æ•°
        gsm8k_qwen_chat=gsm8k_qwen_chat,
        gsm8k_few_shot_k=gsm8k_few_shot_k,
        gsm8k_few_shot_dataset=gsm8k_few_shot_dataset,
    )
    
    # åˆ›å»ºBFCLè¯„ä¼°å‡½æ•°ï¼ˆä»…å½“æä¾›äº†æ•°æ®é›†ï¼‰
    bfcl_eval_fn = None
    if bfcl_dataset is not None:
        bfcl_eval_fn = create_bfcl_evaluation_fn(
            model_skeleton,
            param_shapes,
            bfcl_dataset,
            tokenizer,
            batch_size=batch_size,
            distributed=distributed,
            world_size=world_size,
            rank=rank,
            eval_subset_size=eval_subset_size,
            return_subset_only=True,  # å¤šä»»åŠ¡è¯„ä¼°ï¼šä¸è¿›è¡Œåˆ†å¸ƒå¼èšåˆ
        )
    
    # åˆ›å»ºMBPPè¯„ä¼°å‡½æ•°ï¼ˆå¦‚æœæä¾›äº†æ•°æ®é›†ï¼‰
    mbpp_eval_fn = None
    if mbpp_dataset is not None:
        mbpp_eval_fn = create_mbpp_evaluation_fn(
            model_skeleton, param_shapes, mbpp_dataset, tokenizer,
            batch_size=batch_size,
            distributed=distributed,
            world_size=world_size,
            rank=rank,
            eval_subset_size=eval_subset_size,
            return_subset_only=True,  # å¤šä»»åŠ¡è¯„ä¼°ï¼šä¸è¿›è¡Œåˆ†å¸ƒå¼èšåˆ
            mbpp_qwen_chat=False,  # å¯æŒ‰éœ€æ¥ä¸»å¼€å…³
            mbpp_few_shot_k=3,
            mbpp_few_shot_dataset=None,
        )
    
    # åˆ›å»ºDoTè¯„ä¼°å‡½æ•°ï¼ˆåœ¨çº¿ç”Ÿæˆï¼‰
    mult4_eval_fn = None
    mult5_eval_fn = None
    bool_eval_fn = None
    if use_mult4_eval:
        mult4_eval_fn = create_dot_eval_fn(
            model_skeleton, param_shapes, tokenizer,
            task='mult4', num_samples=(eval_subset_size or 20),
            batch_size=max(1, batch_size), distributed=distributed,
            world_size=world_size, rank=rank,
        )
    if use_mult5_eval:
        mult5_eval_fn = create_dot_eval_fn(
            model_skeleton, param_shapes, tokenizer,
            task='mult5', num_samples=(eval_subset_size or 20),
            batch_size=max(1, batch_size), distributed=distributed,
            world_size=world_size, rank=rank,
        )
    if use_bool_eval:
        bool_eval_fn = create_dot_eval_fn(
            model_skeleton, param_shapes, tokenizer,
            task='bool', num_samples=(eval_subset_size or 20),
            batch_size=max(1, batch_size), distributed=distributed,
            world_size=world_size, rank=rank,
        )

    def evaluation_fn(flat_params):
        import time
        scores_list = []
        task_times = {}
        task_scores = {}
        
        # GSM8K (è·³è¿‡æƒé‡ä¸º0çš„ä»»åŠ¡)
        if task_weights.get('gsm8k', 0.0) > 0:
            start_time = time.time()
            gsm8k_scores = gsm8k_eval_fn(flat_params)
            task_times['GSM8K'] = time.time() - start_time
            task_scores['GSM8K'] = float(jnp.mean(gsm8k_scores))
            scores_list.append(gsm8k_scores)
        
        # BFCLï¼ˆå¯é€‰ï¼Œè·³è¿‡æƒé‡ä¸º0çš„ä»»åŠ¡ï¼‰
        if bfcl_eval_fn is not None and task_weights.get('bfcl', 0.0) > 0:
            start_time = time.time()
            bfcl_scores = bfcl_eval_fn(flat_params)
            task_times['BFCL'] = time.time() - start_time
            task_scores['BFCL'] = float(jnp.mean(bfcl_scores))
            scores_list.append(bfcl_scores)
        
        # MBPPï¼ˆå¯é€‰ï¼Œè·³è¿‡æƒé‡ä¸º0çš„ä»»åŠ¡ï¼‰
        if mbpp_eval_fn is not None and task_weights.get('mbpp', 0.0) > 0:
            start_time = time.time()
            mbpp_scores = mbpp_eval_fn(flat_params)
            task_times['MBPP'] = time.time() - start_time
            task_scores['MBPP'] = float(jnp.mean(mbpp_scores))
            scores_list.append(mbpp_scores)
        
        # DoTï¼ˆå¯é€‰ï¼Œè·³è¿‡æƒé‡ä¸º0çš„ä»»åŠ¡ï¼‰
        if mult4_eval_fn is not None and task_weights.get('mult4', 0.0) > 0:
            start_time = time.time()
            mult4_scores = mult4_eval_fn(flat_params)
            task_times['Mult4'] = time.time() - start_time
            task_scores['Mult4'] = float(jnp.mean(mult4_scores))
            scores_list.append(mult4_scores)
        
        if mult5_eval_fn is not None and task_weights.get('mult5', 0.0) > 0:
            start_time = time.time()
            mult5_scores = mult5_eval_fn(flat_params)
            task_times['Mult5'] = time.time() - start_time
            task_scores['Mult5'] = float(jnp.mean(mult5_scores))
            scores_list.append(mult5_scores)
        
        if bool_eval_fn is not None and task_weights.get('bool', 0.0) > 0:
            start_time = time.time()
            bool_scores = bool_eval_fn(flat_params)
            task_times['Boolean'] = time.time() - start_time
            task_scores['Boolean'] = float(jnp.mean(bool_scores))
            scores_list.append(bool_scores)
        
        # æ‰“å°ä»»åŠ¡æ€§èƒ½ç»Ÿè®¡ï¼ˆå— VERBOSE_EVAL æ§åˆ¶ï¼‰
        if rank == 0 and os.environ.get("VERBOSE_EVAL", "0") == "1":
            print(f"\n{'='*70}")
            print(f"ğŸ“Š ä»»åŠ¡æ€§èƒ½ç»Ÿè®¡")
            print(f"{'='*70}")
            print(f"{'ä»»åŠ¡':<15} {'è€—æ—¶(s)':>12} {'å¹³å‡å¾—åˆ†':>15} {'å¾—åˆ†/ç§’':>15}")
            print(f"{'-'*70}")
            for task_name in task_times.keys():
                t = task_times[task_name]
                s = task_scores[task_name]
                efficiency = s / t if t > 0 else 0
                print(f"{task_name:<15} {t:>12.2f} {s:>15.4f} {efficiency:>15.6f}")
            print(f"{'='*70}\n")
        
        return jnp.concatenate(scores_list)

    return evaluation_fn

def create_dot_eval_fn(
    model_skeleton,
    param_shapes,
    tokenizer,
    task: str,  # 'mult4' | 'mult5' | 'bool'
    num_samples: int,
    batch_size: int = 8,
    distributed: bool = False,
    world_size: int = 1,
    rank: int = 0,
):
    """åœ¨çº¿ç”ŸæˆDoTé£æ ¼ä»»åŠ¡å¹¶è¯„ä¼°ï¼ˆpass@1ï¼‰ã€‚"""
    import torch
    device = next(model_skeleton.parameters()).device

    # ç”Ÿæˆæ•°æ®
    if task == 'mult4':
        dataset = generate_mult_dataset(num_samples=num_samples, digits=4, seed=2025)
        parse_fn = parse_int_from_text
    elif task == 'mult5':
        dataset = generate_mult_dataset(num_samples=num_samples, digits=5, seed=2025)
        parse_fn = parse_int_from_text
    elif task == 'bool':
        dataset = generate_bool_dataset(num_samples=num_samples, seed=2025)
        parse_fn = parse_bool_from_text
    else:
        raise ValueError(f"Unknown DoT task: {task}")

    prompts = [item['prompt'] for item in dataset]
    golds = [item['gold'] for item in dataset]

    def evaluation_fn(flat_params: jnp.ndarray) -> jnp.ndarray:
        base_model = (
            model_skeleton.module if hasattr(model_skeleton, "module") else model_skeleton
        )
        restored_model = jax_flattened_to_pytorch_model(
            flat_params, base_model, param_shapes
        )
        restored_model.eval()

        all_scores: list[float] = []
        # åˆ†æ‰¹tokenize+ç”Ÿæˆ
        for start in range(0, len(prompts), batch_size):
            batch_prompts = prompts[start:start+batch_size]
            batch_golds = golds[start:start+batch_size]

            enc = dot_collate(batch_prompts, tokenizer, max_length=256)
            input_ids = enc['input_ids'].to(device)
            attention_mask = enc['attention_mask'].to(device)

            with torch.no_grad():
                gen_ids = restored_model.generate(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    max_new_tokens=64,
                    do_sample=False,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )
                gen_txts = tokenizer.batch_decode(
                    gen_ids[:, input_ids.shape[1]:],
                    skip_special_tokens=True
                )

            for i, (txt, gold) in enumerate(zip(gen_txts, batch_golds)):
                pred = parse_fn(txt)
                is_correct = (pred is not None and pred == gold)
                all_scores.append(1.0 if is_correct else 0.0)
                
                # è°ƒè¯•ï¼šæ‰“å°å‰3ä¸ªæ ·æœ¬çš„è¾“å‡º
                if rank == 0 and start == 0 and i < 3:
                    print(f"  [{task.upper()}] Sample {i+1}:")
                    print(f"    Prompt: {batch_prompts[i][:80]}...")
                    print(f"    Gold: {gold}, Pred: {pred}, Output: {txt[:100]}")
                    print(f"    Correct: {is_correct}")

        return jnp.array(all_scores, dtype=jnp.float32)

    return evaluation_fn
