diff --git a/DYNAMIC_SPARSITY_README.md b/DYNAMIC_SPARSITY_README.md
index f7a295a..82725b1 100644
--- a/DYNAMIC_SPARSITY_README.md
+++ b/DYNAMIC_SPARSITY_README.md
@@ -309,3 +309,4 @@ if enable_pruning:
 
 å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—è¾“å‡ºæˆ–è”ç³»å¼€å‘è€…ã€‚
 
+
diff --git a/MBPP_IMPLEMENTATION_SUMMARY.md b/MBPP_IMPLEMENTATION_SUMMARY.md
new file mode 100644
index 0000000..697062b
--- /dev/null
+++ b/MBPP_IMPLEMENTATION_SUMMARY.md
@@ -0,0 +1,336 @@
+# MBPPé›†æˆå®ç°æ€»ç»“
+
+## ğŸ“ æ¦‚è¿°
+
+æˆåŠŸå°†MBPPï¼ˆMostly Basic Python Problemsï¼‰ä»£ç ç”Ÿæˆè¯„ä¼°é›†æˆåˆ°SparseFusionå¤šä»»åŠ¡è®­ç»ƒæ¡†æ¶ä¸­ã€‚
+
+**é›†æˆæ—¥æœŸï¼š** 2025-10-21  
+**ç‰ˆæœ¬ï¼š** v1.0  
+**æ”¯æŒçš„ä»»åŠ¡ï¼š** GSM8K + BFCL + MBPPï¼ˆä¸‰ä»»åŠ¡ï¼‰
+
+---
+
+## âœ… å®Œæˆçš„ä»»åŠ¡
+
+### 1. æ•°æ®æºä¸åŠ è½½æ¥å£ âœ“
+
+**æ–°å¢æ–‡ä»¶ï¼š**
+- `mbpp_data_utils.py` - MBPPæ•°æ®é›†ç±»å’Œæ‰¹å¤„ç†å‡½æ•°
+
+**åŠŸèƒ½ï¼š**
+- æ”¯æŒJSON/JSONLæ ¼å¼
+- è‡ªåŠ¨æ„å»ºpromptæ¨¡æ¿
+- é›†æˆHuggingFace tokenizer
+- æ‰¹å¤„ç†å’Œcollate_fn
+
+### 2. MBPPè¯„ä¼°å‡½æ•° âœ“
+
+**ä¿®æ”¹æ–‡ä»¶ï¼š**
+- `natural_niches_sparsity_aware_fn.py`
+
+**æ–°å¢å‡½æ•°ï¼š**
+```python
+def create_mbpp_evaluation_fn(
+    model_skeleton,
+    param_shapes,
+    mbpp_dataset,
+    tokenizer,
+    ...
+) -> Callable
+```
+
+**è¯„ä¼°æµç¨‹ï¼š**
+1. æ¨¡å‹ç”ŸæˆPythonä»£ç ï¼ˆmax_tokens=512, temp=0.2ï¼‰
+2. æ¸…ç†ç”Ÿæˆç»“æœï¼ˆå»é™¤markdownæ ‡è®°ï¼‰
+3. åœ¨éš”ç¦»ç¯å¢ƒä¸­æ‰§è¡Œæµ‹è¯•ï¼ˆsubprocess+ä¸´æ—¶æ–‡ä»¶ï¼‰
+4. è®¡ç®—pass@1å‡†ç¡®ç‡ï¼ˆå…¨éƒ¨æµ‹è¯•é€šè¿‡=1ï¼Œå¦åˆ™=0ï¼‰
+
+**å®‰å…¨æœºåˆ¶ï¼š**
+- è¿›ç¨‹éš”ç¦»
+- è¶…æ—¶æ§åˆ¶ï¼ˆ10ç§’/æ ·æœ¬ï¼‰
+- ä¸´æ—¶æ–‡ä»¶è‡ªåŠ¨æ¸…ç†
+- å¼‚å¸¸å¤„ç†
+
+### 3. å¤šä»»åŠ¡æƒé‡èšåˆ âœ“
+
+**ä¿®æ”¹æ–‡ä»¶ï¼š**
+- `natural_niches_sparsity_aware_fn.py`
+
+**æ›´æ–°é€»è¾‘ï¼š**
+- `create_multi_task_evaluation_fn` æ”¯æŒå¯é€‰çš„ `mbpp_dataset` å‚æ•°
+- è‡ªåŠ¨è®¡ç®— `num_tasks` åŒ…å«MBPPæ ·æœ¬æ•°
+- åˆ†æ•°æ‹¼æ¥ï¼š`[gsm8k_scores, bfcl_scores, mbpp_scores]`
+- ç«äº‰å½’ä¸€åŒ–æ”¯æŒä¸‰ä»»åŠ¡
+
+**num_tasksè®¡ç®—ï¼š**
+```python
+if eval_subset_size:
+    num_tasks = eval_subset_size * num_active_tasks
+else:
+    num_tasks = len(gsm8k) + len(bfcl) + len(mbpp)
+```
+
+### 4. CLIå‚æ•°æ‰©å±• âœ“
+
+**ä¿®æ”¹æ–‡ä»¶ï¼š**
+- `main_sparsity_aware.py`
+
+**æ–°å¢å‚æ•°ï¼š**
+```python
+--use_mbpp_eval          # å¯ç”¨MBPPè¯„ä¼°ï¼ˆflagï¼‰
+--mbpp_data_path PATH    # MBPPæ•°æ®è·¯å¾„
+--mbpp_weight FLOAT      # MBPPä»»åŠ¡æƒé‡ï¼ˆé»˜è®¤0.33ï¼‰
+```
+
+**é…ç½®æ‰“å°ï¼š**
+```
+ğŸ¯ MBPP Evaluation ENABLED
+  MBPP weight: 0.33
+  MBPP data: mbpp/data/mbpp_test.json
+```
+
+### 5. å¯è§†åŒ–ä¸åˆ†æ âœ“
+
+**ç°æœ‰è„šæœ¬å·²å…¼å®¹ï¼š**
+- `tools/analyze_results.py` - è‡ªåŠ¨å¤„ç†ä¸‰ä»»åŠ¡ç»“æœ
+- `tools/plot_checkpoint_comparison.py` - å¯¹æ¯”ä¸åŒé…ç½®
+- `tools/plot_detailed_comparison.py` - è¯¦ç»†è½¨è¿¹åˆ†æ
+
+**è¯´æ˜ï¼š**
+ç”±äºé‡‡ç”¨åˆ†æ•°æ‹¼æ¥æ–¹å¼ï¼Œç°æœ‰å¯è§†åŒ–è„šæœ¬æ— éœ€ä¿®æ”¹å³å¯å·¥ä½œã€‚å¦‚éœ€ä»»åŠ¡çº§åˆ«çš„ç»†ç²’åº¦å±•ç¤ºï¼Œå¯åœ¨futureç‰ˆæœ¬ä¸­æ‰©å±•`test_evaluations`è®°å½•ä»»åŠ¡æ ‡ç­¾ã€‚
+
+### 6. å®éªŒè„šæœ¬ä¸æ–‡æ¡£ âœ“
+
+**æ–°å¢è„šæœ¬ï¼š**
+- `scripts/experiments/run_mbpp_quick_test.sh` - å¿«é€ŸéªŒè¯è„šæœ¬
+
+**æ–°å¢æ–‡æ¡£ï¼š**
+- `docs/MBPP_INTEGRATION.md` - å®Œæ•´é›†æˆæ–‡æ¡£
+- `MBPP_QUICKSTART.md` - 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹
+- `MBPP_IMPLEMENTATION_SUMMARY.md` - æœ¬æ–‡ä»¶
+
+**æ–°å¢å·¥å…·ï¼š**
+- `tools/convert_mbpp_to_simple.py` - æ•°æ®æ ¼å¼è½¬æ¢å·¥å…·
+
+**ç¤ºä¾‹æ•°æ®ï¼š**
+- `mbpp/data/mbpp_test_sample.json` - 5ä¸ªæ ·æœ¬ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
+
+---
+
+## ğŸ“‚ æ–‡ä»¶æ”¹åŠ¨æ¸…å•
+
+### æ–°å¢æ–‡ä»¶ï¼ˆ8ä¸ªï¼‰
+
+```
+SparseFusion/
+â”œâ”€â”€ mbpp_data_utils.py                          # MBPPæ•°æ®åŠ è½½
+â”œâ”€â”€ MBPP_QUICKSTART.md                          # å¿«é€Ÿå¼€å§‹
+â”œâ”€â”€ MBPP_IMPLEMENTATION_SUMMARY.md              # æœ¬æ–‡ä»¶
+â”œâ”€â”€ docs/MBPP_INTEGRATION.md                    # é›†æˆæ–‡æ¡£
+â”œâ”€â”€ tools/convert_mbpp_to_simple.py             # æ•°æ®è½¬æ¢å·¥å…·
+â”œâ”€â”€ scripts/experiments/run_mbpp_quick_test.sh  # å¿«é€Ÿæµ‹è¯•è„šæœ¬
+â””â”€â”€ mbpp/data/
+    â””â”€â”€ mbpp_test_sample.json                   # ç¤ºä¾‹æ•°æ®ï¼ˆ5æ ·æœ¬ï¼‰
+```
+
+### ä¿®æ”¹æ–‡ä»¶ï¼ˆ2ä¸ªï¼‰
+
+```
+SparseFusion/
+â”œâ”€â”€ natural_niches_sparsity_aware_fn.py
+â”‚   â”œâ”€â”€ [æ–°å¢] create_mbpp_evaluation_fn()
+â”‚   â”œâ”€â”€ [ä¿®æ”¹] create_multi_task_evaluation_fn()
+â”‚   â”œâ”€â”€ [ä¿®æ”¹] natural_niches_sparsity_aware() å‚æ•°
+â”‚   â””â”€â”€ [ä¿®æ”¹] num_tasks è®¡ç®—é€»è¾‘
+â””â”€â”€ main_sparsity_aware.py
+    â”œâ”€â”€ [æ–°å¢] --use_mbpp_eval å‚æ•°
+    â”œâ”€â”€ [æ–°å¢] --mbpp_data_path å‚æ•°
+    â”œâ”€â”€ [æ–°å¢] --mbpp_weight å‚æ•°
+    â””â”€â”€ [ä¿®æ”¹] é…ç½®æ‰“å°å’Œå‚æ•°ä¼ é€’
+```
+
+---
+
+## ğŸš€ ä½¿ç”¨ç¤ºä¾‹
+
+### æœ€ç®€å•çš„å¯ç”¨æ–¹å¼
+
+```bash
+python3 main_sparsity_aware.py \
+  --runs 1 \
+  --model1_path models/Qwen2.5-0.5B-Instruct \
+  --model2_path models/Qwen2.5-0.5B-Instruct \
+  --pop_size 4 \
+  --total_forward_passes 100 \
+  --use_bfcl_eval \
+  --use_mbpp_eval \
+  --eval_subset_size 10
+```
+
+### å®Œæ•´ä¸‰ä»»åŠ¡é…ç½®
+
+```bash
+python3 main_sparsity_aware.py \
+  --runs 1 \
+  --model1_path models/Qwen2.5-0.5B-Instruct \
+  --model2_path models/Qwen2.5-0.5B-Instruct \
+  --pop_size 8 \
+  --total_forward_passes 3000 \
+  --omega 0.7 --beta 0.3 \
+  --pruning_sparsity 0.2 \
+  --eval_subset_size 20 \
+  --use_bfcl_eval \
+  --bfcl_data_path bfcl/data/bfcl_test_200.json \
+  --gsm8k_weight 0.4 \
+  --bfcl_weight 0.3 \
+  --use_mbpp_eval \
+  --mbpp_data_path mbpp/data/mbpp_test_sample.json \
+  --mbpp_weight 0.3 \
+  --output_dir results/ä¸‰ä»»åŠ¡å®Œæ•´å®éªŒ
+```
+
+---
+
+## ğŸ”§ æŠ€æœ¯ç»†èŠ‚
+
+### è¯„ä¼°å‡½æ•°æ¶æ„
+
+```
+create_multi_task_evaluation_fn
+â”œâ”€â”€ create_evaluation_fn_for_llm (GSM8K)
+â”œâ”€â”€ create_bfcl_evaluation_fn (BFCL)
+â””â”€â”€ create_mbpp_evaluation_fn (MBPP) â† æ–°å¢
+
+evaluation_fn(params)
+â”œâ”€â”€ gsm8k_scores: [n1]
+â”œâ”€â”€ bfcl_scores:  [n2]
+â””â”€â”€ mbpp_scores:  [n3] â† æ–°å¢
+    â””â”€â”€ concat â†’ all_scores: [n1+n2+n3]
+```
+
+### ä»£ç æ‰§è¡Œæµç¨‹
+
+```
+ç”Ÿæˆä»£ç  (æ¸©åº¦0.2, æœ€å¤§512 tokens)
+    â†“
+æ¸…ç†æ ¼å¼ (ç§»é™¤```pythonæ ‡è®°ç­‰)
+    â†“
+æ„å»ºæµ‹è¯•ç¨‹åº
+    â”œâ”€â”€ setup_code (å‰ç½®ä»£ç )
+    â”œâ”€â”€ generated_code (ç”Ÿæˆçš„å‡½æ•°)
+    â”œâ”€â”€ test_list (æ–­è¨€åˆ—è¡¨)
+    â””â”€â”€ print('__MBPP_ALL_TESTS_PASSED__')
+    â†“
+å†™å…¥ä¸´æ—¶æ–‡ä»¶ â†’ subprocessæ‰§è¡Œ (è¶…æ—¶10s)
+    â†“
+æ£€æŸ¥è¾“å‡º â†’ è¿”å›1.0æˆ–0.0
+```
+
+### æ•°æ®æ ¼å¼
+
+**è¾“å…¥æ ¼å¼ï¼š**
+```json
+{
+  "task_id": 1,
+  "text": "ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œæ£€æŸ¥ç»™å®šçš„æ•°å­—æ˜¯å¦æ˜¯å¶æ•°ã€‚",
+  "code": "def is_even(n):\n    return n % 2 == 0",
+  "test_list": [
+    "assert is_even(2) == True",
+    "assert is_even(3) == False"
+  ],
+  "test_setup_code": "",
+  "challenge_test_list": []
+}
+```
+
+**Promptæ¨¡æ¿ï¼š**
+```
+è¯·å®ç°ä»¥ä¸‹Pythonå‡½æ•°ï¼š
+
+{text}
+
+è¦æ±‚ï¼š
+- åªè¾“å‡ºå®Œæ•´çš„Pythonå‡½æ•°å®ç°ä»£ç 
+- ä¸è¦åŒ…å«è§£é‡Šæ–‡å­—å’Œé¢å¤–çš„importè¯­å¥ï¼ˆé™¤éå¿…éœ€ï¼‰
+- ä¸è¦åŒ…å«if __name__ == '__main__'å—
+- ç¡®ä¿ä»£ç å¯ä»¥ç›´æ¥æ‰§è¡Œå¹¶é€šè¿‡æµ‹è¯•
+
+å‡½æ•°å®ç°ï¼š
+```
+
+---
+
+## ğŸ“Š æ€§èƒ½å½±å“
+
+### è¯„ä¼°æ—¶é—´ï¼ˆåŸºäºQwen2.5-0.5Bï¼‰
+
+| é…ç½® | å•ä»»åŠ¡ | åŒä»»åŠ¡ | ä¸‰ä»»åŠ¡ |
+|------|--------|--------|--------|
+| eval_subset_size=10 | ~10min | ~15min | ~20min |
+| eval_subset_size=20 | ~20min | ~30min | ~40min |
+| å®Œæ•´è¯„ä¼° | ~30min | ~60min | ~90min |
+
+*æ³¨ï¼šä»¥ä¸ŠåŸºäºtotal_forward_passes=100çš„ä¼°ç®—*
+
+### å†…å­˜å ç”¨
+
+- MBPPè¯„ä¼°é¢å¤–å ç”¨ï¼šçº¦200-500MBï¼ˆå–å†³äºæ•°æ®é›†å¤§å°ï¼‰
+- subprocesséš”ç¦»ï¼šæ¯ä¸ªæµ‹è¯•ä¸´æ—¶å ç”¨çº¦50-100MB
+
+---
+
+## âš ï¸ å·²çŸ¥é™åˆ¶
+
+1. **ä»£ç æ‰§è¡Œå®‰å…¨æ€§**
+   - å½“å‰ä½¿ç”¨subprocesséš”ç¦»ï¼Œæœªä½¿ç”¨Dockerç­‰å¼ºéš”ç¦»
+   - å»ºè®®ä»…åœ¨å—ä¿¡ä»»ç¯å¢ƒä¸­è¿è¡Œ
+
+2. **è¶…æ—¶å¤„ç†**
+   - å›ºå®š10ç§’è¶…æ—¶å¯èƒ½ä¸é€‚åˆæ‰€æœ‰ä»»åŠ¡
+   - å¯åœ¨ä»£ç ä¸­ä¿®æ”¹`timeout`å‚æ•°
+
+3. **pass@kæ”¯æŒ**
+   - å½“å‰ä»…æ”¯æŒpass@1
+   - å¤šæ ·æœ¬è¯„ä¼°ï¼ˆk>1ï¼‰éœ€é¢å¤–å®ç°
+
+4. **ä»»åŠ¡çº§åˆ«å¯è§†åŒ–**
+   - å½“å‰åˆ†æ•°æ‹¼æ¥æ–¹å¼ä¸åŒºåˆ†ä»»åŠ¡æ¥æº
+   - è¯¦ç»†ä»»åŠ¡çº§åˆ«å¯è§†åŒ–éœ€futureæ‰©å±•
+
+---
+
+## ğŸ”® æœªæ¥æ”¹è¿›æ–¹å‘
+
+### çŸ­æœŸï¼ˆv1.1ï¼‰
+- [ ] æ”¯æŒpass@kè¯„ä¼°ï¼ˆk>1ï¼‰
+- [ ] å¯é…ç½®çš„è¶…æ—¶å‚æ•°ï¼ˆCLIå‚æ•°ï¼‰
+- [ ] Dockeræ²™ç®±æ‰§è¡Œé€‰é¡¹
+
+### ä¸­æœŸï¼ˆv2.0ï¼‰
+- [ ] HumanEvalé›†æˆ
+- [ ] ä»£ç è´¨é‡è¯„ä¼°ï¼ˆéä»…é€šè¿‡ç‡ï¼‰
+- [ ] ä»»åŠ¡çº§åˆ«çš„è¯¦ç»†æ—¥å¿—
+
+### é•¿æœŸï¼ˆv3.0ï¼‰
+- [ ] è‡ªå®šä¹‰ä»£ç è¯„ä¼°å™¨æ¥å£
+- [ ] å®æ—¶ä»£ç æ‰§è¡Œç›‘æ§
+- [ ] åˆ†å¸ƒå¼ä»£ç æ‰§è¡Œï¼ˆå¤šGPUåŠ é€Ÿï¼‰
+
+---
+
+## ğŸ“š å‚è€ƒèµ„æ–™
+
+- [MBPPè®ºæ–‡](https://arxiv.org/abs/2108.07732)
+- [HuggingFace MBPP](https://huggingface.co/datasets/mbpp)
+- [SparseFusionåŸå§‹å®ç°](README.md)
+
+---
+
+## ğŸ™ è‡´è°¢
+
+æ„Ÿè°¢æ‚¨ä½¿ç”¨MBPPé›†æˆï¼å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·æissueã€‚
+
+**å®ç°è€…ï¼š** AIåŠ©æ‰‹  
+**æµ‹è¯•çŠ¶æ€ï¼š** âœ… å•å…ƒæµ‹è¯•é€šè¿‡ï¼Œé›†æˆæµ‹è¯•å¾…è¿è¡Œ  
+**ä¸‹ä¸€æ­¥ï¼š** è¿è¡Œ `bash scripts/experiments/run_mbpp_quick_test.sh` éªŒè¯é›†æˆ
+
diff --git a/MBPP_QUICKSTART.md b/MBPP_QUICKSTART.md
new file mode 100644
index 0000000..8c57139
--- /dev/null
+++ b/MBPP_QUICKSTART.md
@@ -0,0 +1,188 @@
+# MBPPå¿«é€Ÿå¼€å§‹æŒ‡å—
+
+## ğŸ¯ ä»€ä¹ˆæ˜¯MBPPé›†æˆï¼Ÿ
+
+MBPP (Mostly Basic Python Problems) ç°å·²é›†æˆåˆ°SparseFusionå¤šä»»åŠ¡è¯„ä¼°ä¸­ï¼Œæ”¯æŒï¼š
+- âœ… GSM8Kï¼ˆæ•°å­¦æ¨ç†ï¼‰
+- âœ… BFCLï¼ˆå‡½æ•°è°ƒç”¨ï¼‰
+- âœ… MBPPï¼ˆä»£ç ç”Ÿæˆï¼‰ â† **æ–°å¢**
+
+## ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿä½“éªŒ
+
+### 1. å‡†å¤‡æ•°æ®ï¼ˆä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼‰
+
+```bash
+# ç¤ºä¾‹æ•°æ®å·²åŒ…å«åœ¨ä»“åº“ä¸­
+ls mbpp/data/mbpp_test_sample.json
+# åŒ…å«5ä¸ªç®€å•çš„Pythonç¼–ç¨‹é—®é¢˜
+```
+
+### 2. è¿è¡Œå¿«é€Ÿæµ‹è¯•
+
+```bash
+chmod +x scripts/experiments/run_mbpp_quick_test.sh
+bash scripts/experiments/run_mbpp_quick_test.sh
+```
+
+**é¢„è®¡è€—æ—¶ï¼š** 10-15åˆ†é’Ÿ  
+**é…ç½®ï¼š** 4ä¸ªä½“ï¼Œ50æ­¥è¿­ä»£ï¼Œæ¯ä»»åŠ¡5æ ·æœ¬
+
+### 3. æŸ¥çœ‹ç»“æœ
+
+```bash
+python tools/analyze_results.py \
+  results/mbpp_quick_test/*.pkl \
+  --no-plot
+```
+
+## ğŸ“Š å®Œæ•´ä¸‰ä»»åŠ¡å®éªŒ
+
+### å‘½ä»¤ç¤ºä¾‹
+
+```bash
+python3 main_sparsity_aware.py \
+  --runs 1 \
+  --model1_path models/Qwen2.5-0.5B-Instruct \
+  --model2_path models/Qwen2.5-0.5B-Instruct \
+  --pop_size 8 \
+  --total_forward_passes 3000 \
+  --omega 0.7 --beta 0.3 \
+  --pruning_sparsity 0.2 \
+  --eval_subset_size 20 \
+  --use_bfcl_eval \
+  --bfcl_data_path bfcl/data/bfcl_test_200.json \
+  --gsm8k_weight 0.4 \
+  --bfcl_weight 0.3 \
+  --use_mbpp_eval \
+  --mbpp_data_path mbpp/data/mbpp_test_sample.json \
+  --mbpp_weight 0.3 \
+  --output_dir results/ä¸‰ä»»åŠ¡å®éªŒ
+```
+
+### ç¯å¢ƒå˜é‡æ–¹å¼ï¼ˆæ¨èç”¨äºè„šæœ¬ï¼‰
+
+```bash
+export USE_MBPP_EVAL=true
+export MBPP_DATA_PATH=mbpp/data/mbpp_test_sample.json
+export MBPP_WEIGHT=0.3
+export GSM8K_WEIGHT=0.4
+export BFCL_WEIGHT=0.3
+
+bash scripts/experiments/run_bfcl_single_node.sh
+```
+
+## ğŸ”§ å‚æ•°è¯´æ˜
+
+| å‚æ•° | è¯´æ˜ | é»˜è®¤å€¼ |
+|------|------|--------|
+| `--use_mbpp_eval` | å¯ç”¨MBPPè¯„ä¼° | False |
+| `--mbpp_data_path` | MBPPæ•°æ®è·¯å¾„ | `mbpp/data/mbpp_test.json` |
+| `--mbpp_weight` | MBPPä»»åŠ¡æƒé‡ï¼ˆ0-1ï¼‰ | 0.33 |
+| `--gsm8k_weight` | GSM8Kä»»åŠ¡æƒé‡ | 0.5 |
+| `--bfcl_weight` | BFCLä»»åŠ¡æƒé‡ | 0.5 |
+
+**æ³¨æ„ï¼š** æƒé‡ä¼šè‡ªåŠ¨å½’ä¸€åŒ–ï¼Œæ— éœ€æ‰‹åŠ¨ä¿è¯å’Œä¸º1ã€‚
+
+## ğŸ“ˆ æƒé‡é…ç½®å»ºè®®
+
+### åœºæ™¯1ï¼šå‡è¡¡ä¸‰ä»»åŠ¡
+```bash
+--gsm8k_weight 0.4 --bfcl_weight 0.3 --mbpp_weight 0.3
+```
+é€‚ç”¨ï¼šå…¨é¢è¯„ä¼°æ¨¡å‹èƒ½åŠ›
+
+### åœºæ™¯2ï¼šå¼ºè°ƒä»£ç èƒ½åŠ›
+```bash
+--gsm8k_weight 0.3 --bfcl_weight 0.3 --mbpp_weight 0.4
+```
+é€‚ç”¨ï¼šä»£ç ç”Ÿæˆä»»åŠ¡ä¼˜å…ˆ
+
+### åœºæ™¯3ï¼šæ•°å­¦ä¸ºä¸»
+```bash
+--gsm8k_weight 0.6 --bfcl_weight 0.2 --mbpp_weight 0.2
+```
+é€‚ç”¨ï¼šæ•°å­¦æ¨ç†ä»»åŠ¡ä¸ºä¸»
+
+## ğŸ“ æ•°æ®å‡†å¤‡é€‰é¡¹
+
+### é€‰é¡¹1ï¼šä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼ˆæœ€å¿«ï¼‰
+```bash
+# å·²åŒ…å«ï¼Œç›´æ¥ä½¿ç”¨
+--mbpp_data_path mbpp/data/mbpp_test_sample.json
+```
+
+### é€‰é¡¹2ï¼šä¸‹è½½å®Œæ•´MBPP
+```bash
+# ä»HuggingFaceä¸‹è½½
+python -c "
+from datasets import load_dataset
+ds = load_dataset('mbpp', 'sanitized')
+ds['test'].to_json('mbpp/data/mbpp_test_full.json')
+"
+
+# ä½¿ç”¨å®Œæ•´æ•°æ®
+--mbpp_data_path mbpp/data/mbpp_test_full.json
+```
+
+### é€‰é¡¹3ï¼šè½¬æ¢è‡ªå®šä¹‰æ•°æ®
+```bash
+python tools/convert_mbpp_to_simple.py \
+  --input your_data.jsonl \
+  --output mbpp/data/custom.json \
+  --limit 100
+```
+
+## ğŸ› å¸¸è§é—®é¢˜
+
+### Q: MBPPè¯„ä¼°å¾ˆæ…¢ï¼Ÿ
+```bash
+# å‡å°‘æ ·æœ¬æ•°åŠ é€Ÿ
+--eval_subset_size 10  # æ¯ä»»åŠ¡10æ ·æœ¬
+```
+
+### Q: æµ‹è¯•ç»å¸¸è¶…æ—¶ï¼Ÿ
+- åŸå› ï¼šä»£ç ç”Ÿæˆè´¨é‡è¾ƒä½æˆ–åŒ…å«æ­»å¾ªç¯
+- è§£å†³ï¼šä½¿ç”¨æ›´å¤§çš„æ¨¡å‹æˆ–è°ƒæ•´è¶…æ—¶å‚æ•°
+
+### Q: å‡†ç¡®ç‡ä¸º0ï¼Ÿ
+- æ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®
+- æŸ¥çœ‹ç”Ÿæˆçš„ä»£ç ï¼šæ·»åŠ  `--log_sparsity_stats` æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
+- å°è¯•é™ä½æ¨¡å‹å¤æ‚åº¦
+
+## ğŸ“– è¯¦ç»†æ–‡æ¡£
+
+- **å®Œæ•´é›†æˆæ–‡æ¡£ï¼š** [docs/MBPP_INTEGRATION.md](docs/MBPP_INTEGRATION.md)
+- **BFCLæ–‡æ¡£ï¼š** [docs/BFCL_QUICK_START.md](docs/BFCL_QUICK_START.md)
+- **åŸºç¡€ä½¿ç”¨ï¼š** [README.md](README.md)
+
+## ğŸ¯ ä¸‹ä¸€æ­¥
+
+1. **è¿è¡ŒåŸºå‡†æµ‹è¯•**
+   ```bash
+   bash scripts/experiments/run_mbpp_quick_test.sh
+   ```
+
+2. **å¯¹æ¯”ä¸åŒé…ç½®**
+   ```bash
+   python tools/plot_checkpoint_comparison.py \
+     --baseline results/baseline/*.pkl \
+     --sparsity results/mbpp_test/*.pkl \
+     --output mbpp_comparison.png
+   ```
+
+3. **å®Œæ•´å®éªŒ**
+   - ä½¿ç”¨å®Œæ•´MBPPæ•°æ®é›†
+   - å¢åŠ è¿­ä»£æ¬¡æ•°åˆ°3000+
+   - å°è¯•ä¸åŒçš„æƒé‡é…ç½®
+
+## ğŸ’¡ æç¤º
+
+- é¦–æ¬¡ä½¿ç”¨å»ºè®®å…ˆè·‘å¿«é€Ÿæµ‹è¯•éªŒè¯ç¯å¢ƒ
+- MBPPè¯„ä¼°éœ€è¦æ‰§è¡Œä»£ç ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„CPU/å†…å­˜
+- æƒé‡é…ç½®å¯¹æœ€ç»ˆæ€§èƒ½å½±å“å¾ˆå¤§ï¼Œå»ºè®®å¤šè¯•å‡ ç»„
+- ä½¿ç”¨`--eval_subset_size`å¯æ˜¾è‘—åŠ é€Ÿè¯„ä¼°ï¼ˆæ¨è20-30ï¼‰
+
+---
+
+**ğŸ‰ ç°åœ¨å¼€å§‹ä½“éªŒä¸‰ä»»åŠ¡è”åˆè®­ç»ƒï¼**
+
diff --git a/docs/MBPP_INTEGRATION.md b/docs/MBPP_INTEGRATION.md
new file mode 100644
index 0000000..c9b7272
--- /dev/null
+++ b/docs/MBPP_INTEGRATION.md
@@ -0,0 +1,252 @@
+# MBPPé›†æˆæ–‡æ¡£
+
+## æ¦‚è¿°
+
+MBPP (Mostly Basic Python Problems) æ˜¯ä¸€ä¸ªåŒ…å«åŸºç¡€Pythonç¼–ç¨‹é—®é¢˜çš„ä»£ç ç”ŸæˆåŸºå‡†æµ‹è¯•é›†ã€‚æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•åœ¨SparseFusionä¸­ä½¿ç”¨MBPPè¿›è¡Œå¤šä»»åŠ¡è¯„ä¼°ã€‚
+
+## æ•°æ®å‡†å¤‡
+
+### 1. æ•°æ®æ ¼å¼
+
+MBPPæ•°æ®ä½¿ç”¨JSONæ ¼å¼ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«ï¼š
+
+```json
+{
+  "task_id": 1,
+  "text": "ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œæ£€æŸ¥ç»™å®šçš„æ•°å­—æ˜¯å¦æ˜¯å¶æ•°ã€‚",
+  "code": "def is_even(n):\n    return n % 2 == 0",
+  "test_list": [
+    "assert is_even(2) == True",
+    "assert is_even(3) == False"
+  ],
+  "test_setup_code": "",
+  "challenge_test_list": []
+}
+```
+
+### 2. è·å–MBPPæ•°æ®
+
+**é€‰é¡¹1ï¼šä½¿ç”¨ç¤ºä¾‹æ•°æ®ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰**
+```bash
+# å·²åŒ…å«åœ¨ä»“åº“ä¸­
+mbpp/data/mbpp_test_sample.json  # 5ä¸ªç®€å•æ ·æœ¬
+```
+
+**é€‰é¡¹2ï¼šä»HuggingFaceä¸‹è½½å®Œæ•´æ•°æ®**
+```bash
+# ä¸‹è½½MBPPæ•°æ®é›†
+python -c "
+from datasets import load_dataset
+ds = load_dataset('mbpp', 'sanitized')
+ds['test'].to_json('mbpp/data/mbpp_test_full.json')
+"
+```
+
+**é€‰é¡¹3ï¼šè½¬æ¢è‡ªå®šä¹‰æ•°æ®**
+```bash
+python tools/convert_mbpp_to_simple.py \
+  --input your_mbpp.jsonl \
+  --output mbpp/data/mbpp_test.json \
+  --limit 100
+```
+
+## å¿«é€Ÿå¼€å§‹
+
+### 1. å¿«é€ŸéªŒè¯ï¼ˆæ¨èé¦–æ¬¡ä½¿ç”¨ï¼‰
+
+```bash
+# ä½¿ç”¨å°‘é‡æ ·æœ¬å¿«é€Ÿæµ‹è¯•MBPPé›†æˆ
+bash scripts/experiments/run_mbpp_quick_test.sh
+```
+
+é…ç½®ï¼š
+- ç§ç¾¤å¤§å°ï¼š4
+- è¿­ä»£æ¬¡æ•°ï¼š50
+- è¯„ä¼°æ ·æœ¬ï¼š5ä¸ª/ä»»åŠ¡
+- ä¸‰ä»»åŠ¡ï¼šGSM8K (40%) + BFCL (30%) + MBPP (30%)
+
+é¢„è®¡è€—æ—¶ï¼š**10-15åˆ†é’Ÿ**
+
+### 2. å®Œæ•´å®éªŒ
+
+```bash
+# ä¸‰ä»»åŠ¡å®Œæ•´è¯„ä¼°
+python3 main_sparsity_aware.py \
+  --runs 1 \
+  --model1_path models/Qwen2.5-0.5B-Instruct \
+  --model2_path models/Qwen2.5-0.5B-Instruct \
+  --pop_size 8 \
+  --total_forward_passes 3000 \
+  --omega 0.7 --beta 0.3 \
+  --eval_subset_size 20 \
+  --use_bfcl_eval \
+  --bfcl_data_path bfcl/data/bfcl_test_200.json \
+  --gsm8k_weight 0.4 \
+  --bfcl_weight 0.3 \
+  --use_mbpp_eval \
+  --mbpp_data_path mbpp/data/mbpp_test.json \
+  --mbpp_weight 0.3 \
+  --output_dir results/mbpp_full_test
+```
+
+## å‚æ•°è¯´æ˜
+
+### MBPPç›¸å…³å‚æ•°
+
+| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
+|------|------|--------|------|
+| `--use_mbpp_eval` | flag | False | å¯ç”¨MBPPè¯„ä¼° |
+| `--mbpp_data_path` | str | `mbpp/data/mbpp_test.json` | MBPPæ•°æ®è·¯å¾„ |
+| `--mbpp_weight` | float | 0.33 | MBPPä»»åŠ¡æƒé‡ |
+
+### æƒé‡é…ç½®å»ºè®®
+
+**ä¸‰ä»»åŠ¡å‡è¡¡ï¼ˆæ¨èï¼‰ï¼š**
+```bash
+--gsm8k_weight 0.4 --bfcl_weight 0.3 --mbpp_weight 0.3
+```
+
+**å¼ºè°ƒæ•°å­¦æ¨ç†ï¼š**
+```bash
+--gsm8k_weight 0.6 --bfcl_weight 0.2 --mbpp_weight 0.2
+```
+
+**å¼ºè°ƒä»£ç ç”Ÿæˆï¼š**
+```bash
+--gsm8k_weight 0.3 --bfcl_weight 0.3 --mbpp_weight 0.4
+```
+
+## è¯„ä¼°æœºåˆ¶
+
+### ä»£ç ç”Ÿæˆä¸æ‰§è¡Œ
+
+1. **ç”Ÿæˆé˜¶æ®µ**
+   - æ¨¡å‹æ ¹æ®é—®é¢˜æè¿°ç”ŸæˆPythonå‡½æ•°
+   - ä½¿ç”¨ä½æ¸©åº¦ï¼ˆ0.2ï¼‰ç¡®ä¿ç¨³å®šæ€§
+   - æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼š512 tokens
+
+2. **æµ‹è¯•æ‰§è¡Œ**
+   - åœ¨éš”ç¦»çš„ä¸´æ—¶ç›®å½•ä¸­è¿è¡Œä»£ç 
+   - è¶…æ—¶é™åˆ¶ï¼š10ç§’/æ ·æœ¬
+   - æ‰§è¡Œæ ·æœ¬çš„`test_list`ä¸­çš„æ‰€æœ‰æ–­è¨€
+
+3. **åˆ¤åˆ†**
+   - æ‰€æœ‰æµ‹è¯•é€šè¿‡ â†’ 1.0åˆ†
+   - ä»»æ„æµ‹è¯•å¤±è´¥/è¶…æ—¶/å¼‚å¸¸ â†’ 0.0åˆ†
+   - æœ€ç»ˆå‡†ç¡®ç‡ = é€šè¿‡æ ·æœ¬æ•° / æ€»æ ·æœ¬æ•°
+
+### å®‰å…¨æœºåˆ¶
+
+- **è¿›ç¨‹éš”ç¦»**ï¼šæ¯ä¸ªæµ‹è¯•åœ¨ç‹¬ç«‹çš„subprocessä¸­è¿è¡Œ
+- **è¶…æ—¶æ§åˆ¶**ï¼šé˜²æ­¢æ— é™å¾ªç¯
+- **ä¸´æ—¶æ–‡ä»¶**ï¼šè‡ªåŠ¨æ¸…ç†ï¼Œé¿å…æ±¡æŸ“
+- **ç¦ç”¨å­—èŠ‚ç **ï¼šè®¾ç½®`PYTHONDONTWRITEBYTECODE=1`
+
+## ç»“æœåˆ†æ
+
+### æŸ¥çœ‹å®éªŒæ‘˜è¦
+
+```bash
+python tools/analyze_results.py \
+  results/mbpp_full_test/*.pkl \
+  --no-plot
+```
+
+è¾“å‡ºç¤ºä¾‹ï¼š
+```
+[Run 1] è¿­ä»£æ­¥æ•°: 300
+  archive_fitness_mean: 0.4521
+  archive_sparsity_mean: 0.2134
+  archive_total_score_mean: 0.3856
+```
+
+### å¯è§†åŒ–å¯¹æ¯”
+
+```bash
+# å¯¹æ¯”ä¸åŒæƒé‡é…ç½®
+python tools/plot_checkpoint_comparison.py \
+  --baseline results/gsm8k_only/result.pkl \
+  --sparsity results/mbpp_multi_task/result.pkl \
+  --output comparison_mbpp.png
+```
+
+## å¸¸è§é—®é¢˜
+
+### Q1: MBPPè¯„ä¼°å¾ˆæ…¢ï¼Ÿ
+
+**åŸå› **ï¼šä»£ç æ‰§è¡Œéœ€è¦å¯åŠ¨subprocessã€‚
+
+**è§£å†³**ï¼š
+```bash
+# å‡å°‘è¯„ä¼°æ ·æœ¬æ•°
+--eval_subset_size 10  # æ¯ä»»åŠ¡åªè¯„ä¼°10ä¸ªæ ·æœ¬
+
+# å‡å°‘è¿­ä»£æ¬¡æ•°ï¼ˆå¿«é€Ÿæµ‹è¯•ï¼‰
+--total_forward_passes 100
+```
+
+### Q2: æµ‹è¯•ç»å¸¸è¶…æ—¶ï¼Ÿ
+
+**åŸå› **ï¼šç”Ÿæˆçš„ä»£ç å¯èƒ½åŒ…å«æ­»å¾ªç¯æˆ–ä½æ•ˆç®—æ³•ã€‚
+
+**è§£å†³**ï¼š
+- è°ƒæ•´è¶…æ—¶å‚æ•°ï¼ˆåœ¨`natural_niches_sparsity_aware_fn.py`ä¸­ä¿®æ”¹`timeout`å‚æ•°ï¼‰
+- ä½¿ç”¨æ›´å¼ºçš„åŸºç¡€æ¨¡å‹
+- å¢åŠ æ¸©åº¦å¤šæ ·æ€§ï¼ˆä¿®æ”¹`temperature`å‚æ•°ï¼‰
+
+### Q3: å‡†ç¡®ç‡å¾ˆä½ï¼Ÿ
+
+**å¯èƒ½åŸå› **ï¼š
+1. æ¨¡å‹å¤ªå°ï¼ˆ0.5Bå¯èƒ½ä¸è¶³ä»¥ç”Ÿæˆå¤æ‚ä»£ç ï¼‰
+2. æƒé‡é…ç½®ä¸å½“
+3. promptæ¨¡æ¿éœ€è¦ä¼˜åŒ–
+
+**å»ºè®®**ï¼š
+- ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚7Bï¼‰
+- è°ƒæ•´`mbpp_data_utils.py`ä¸­çš„promptæ¨¡æ¿
+- å¢åŠ `eval_subset_size`è§‚å¯Ÿæ›´å¤šæ ·æœ¬
+
+### Q4: å¦‚ä½•è‡ªå®šä¹‰promptï¼Ÿ
+
+ä¿®æ”¹ `mbpp_data_utils.py` ä¸­çš„ `_build_prompt` æ–¹æ³•ï¼š
+
+```python
+def _build_prompt(self, item: Dict) -> str:
+    text = item.get("text", "")
+    # è‡ªå®šä¹‰ä½ çš„promptæ ¼å¼
+    prompt = f"è¯·ç”¨Pythonå®ç°ï¼š{text}\n\nä»£ç ï¼š"
+    return prompt
+```
+
+## æ€§èƒ½åŸºå‡†
+
+åŸºäºQwen2.5-0.5B-Instructçš„å‚è€ƒæ€§èƒ½ï¼š
+
+| é…ç½® | GSM8K Pass@1 | BFCL Acc | MBPP Pass@1 | æ€»è¯„ä¼°æ—¶é—´ |
+|------|-------------|----------|-------------|-----------|
+| å•ä»»åŠ¡ï¼ˆä»…GSM8Kï¼‰ | ~35% | - | - | ~30min |
+| åŒä»»åŠ¡ï¼ˆGSM8K+BFCLï¼‰ | ~32% | ~28% | - | ~45min |
+| ä¸‰ä»»åŠ¡ï¼ˆ+MBPPï¼‰ | ~30% | ~25% | ~15% | ~60min |
+
+*æ³¨ï¼šä»¥ä¸Šæ•°æ®åŸºäºeval_subset_size=20ï¼Œtotal_forward_passes=3000*
+
+## ä¸‹ä¸€æ­¥
+
+1. **æ‰©å±•æ•°æ®é›†**
+   - MBPP-Plusï¼ˆæ›´ä¸¥æ ¼çš„æµ‹è¯•ï¼‰
+   - HumanEvalï¼ˆæ›´å¤æ‚çš„ç®—æ³•é¢˜ï¼‰
+
+2. **ä¼˜åŒ–æ‰§è¡Œ**
+   - æ‰¹é‡æ‰§è¡Œå¤šä¸ªæµ‹è¯•
+   - ä½¿ç”¨Dockeræ²™ç®±å¢å¼ºå®‰å…¨æ€§
+
+3. **æ”¹è¿›è¯„ä¼°**
+   - æ”¯æŒpass@k (k>1)
+   - ä»£ç ç›¸ä¼¼åº¦è¯„ä¼°ï¼ˆéä»…é€šè¿‡ç‡ï¼‰
+
+## å‚è€ƒèµ„æ–™
+
+- [MBPPè®ºæ–‡](https://arxiv.org/abs/2108.07732)
+- [HuggingFace MBPPæ•°æ®é›†](https://huggingface.co/datasets/mbpp)
+- [EvalPlusé¡¹ç›®](https://github.com/evalplus/evalplus)
+
diff --git a/dot_eval_utils.py b/dot_eval_utils.py
new file mode 100644
index 0000000..63781af
--- /dev/null
+++ b/dot_eval_utils.py
@@ -0,0 +1,133 @@
+#!/usr/bin/env python3
+"""
+DoTä»»åŠ¡ï¼ˆ4x4/5x5ä¹˜æ³•ã€å¸ƒå°”é€»è¾‘ï¼‰çš„åœ¨çº¿æ•°æ®ç”Ÿæˆä¸è¯„æµ‹è¾…åŠ©å·¥å…·ã€‚
+æä¾›ï¼š
+- éšæœºç”Ÿæˆå¤šä½æ•°ä¹˜æ³•ä¸å¸ƒå°”é€»è¾‘æ ·æœ¬ï¼ˆå¯æ§æ•°é‡ä¸éšæœºç§å­ï¼‰
+- Promptæ„å»ºï¼ˆè¦æ±‚ä»…è¾“å‡ºæœ€ç»ˆç­”æ¡ˆï¼‰
+- é¢„æµ‹è¾“å‡ºè§£æï¼ˆæ•°å€¼/å¸ƒå°”æ ‡å‡†åŒ–ï¼‰
+- ç®€å•collateä»¥ä¾¿tokenizeræ‰¹å¤„ç†
+"""
+
+import random
+import re
+from typing import List, Dict, Tuple
+
+
+# ========== æ•°æ®ç”Ÿæˆ ==========
+
+def generate_mult_dataset(num_samples: int, digits: int = 4, seed: int = 42) -> List[Dict]:
+    assert digits in (4, 5), "digits must be 4 or 5"
+    rng = random.Random(seed)
+    low = 10 ** (digits - 1)
+    high = 10 ** digits - 1
+    data: List[Dict] = []
+    for _ in range(num_samples):
+        a = rng.randint(low, high)
+        b = rng.randint(low, high)
+        gold = a * b
+        prompt = (
+            f"Compute the product: {a} Ã— {b}. Only output the final integer, no text."
+        )
+        data.append({"prompt": prompt, "gold": gold})
+    return data
+
+
+def _rand_bool_expr(rng: random.Random, vars_vals: Dict[str, bool], max_depth: int = 2) -> str:
+    # ç®€å•é€’å½’ç”Ÿæˆï¼šåŸå­(Var æˆ– not Var) æˆ– äºŒå…ƒ (expr op expr)
+    if max_depth <= 0:
+        var = rng.choice(list(vars_vals.keys()))
+        if rng.random() < 0.3:
+            return f"not {var}"
+        return var
+    # äºŒå…ƒæˆ–ä¸€å…ƒ
+    if rng.random() < 0.4:
+        var = rng.choice(list(vars_vals.keys()))
+        if rng.random() < 0.5:
+            return f"not {var}"
+        return var
+    left = _rand_bool_expr(rng, vars_vals, max_depth - 1)
+    right = _rand_bool_expr(rng, vars_vals, max_depth - 1)
+    op = rng.choice(["and", "or", "xor"])  # æ”¯æŒxor
+    return f"({left} {op} {right})"
+
+
+def generate_bool_dataset(num_samples: int, seed: int = 42) -> List[Dict]:
+    rng = random.Random(seed)
+    data: List[Dict] = []
+    for _ in range(num_samples):
+        vals = {"A": bool(rng.getrandbits(1)), "B": bool(rng.getrandbits(1)), "C": bool(rng.getrandbits(1))}
+        expr = _rand_bool_expr(rng, vals, max_depth=2)
+        # è®¡ç®—gold
+        def _val(name: str) -> bool:
+            return vals[name]
+        # å®‰å…¨æ±‚å€¼ï¼šä»…å…è®¸ and/or/not/xor/A/B/C/æ‹¬å·/ç©ºæ ¼
+        safe_expr = expr.replace("xor", "^")
+        local = {
+            "A": vals["A"],
+            "B": vals["B"],
+            "C": vals["C"],
+            "and": lambda x, y: x and y,  # æœªè¢«evalç›´æ¥è°ƒç”¨ï¼Œè¿™é‡Œåªæ˜¯å ä½
+            "or": lambda x, y: x or y,
+            "not": lambda x: (not x),
+        }
+        # å°† ^ ä½œä¸ºå¼‚æˆ–ï¼šè½¬ä¸ºPythonæŒ‰ä½å¼‚æˆ–ï¼Œå†æ˜ å°„åˆ°bool
+        # æˆ‘ä»¬ç”¨æ›¿æ¢æ–¹æ¡ˆï¼šA ^ B -> (A) ^ (B)
+        # æœ€ç»ˆç”¨ eval ä»…åœ¨åŒ…å« A/B/C/()/^/not/and/or çš„ä¸Šä¸‹æ–‡ä¸­ã€‚
+        expr_eval = safe_expr
+        # è¯„ä¼°ï¼šå°† A/B/C æ›¿æ¢ä¸º True/False å­—é¢å€¼
+        expr_eval = (expr_eval
+                     .replace("A", str(vals["A"]))
+                     .replace("B", str(vals["B"]))
+                     .replace("C", str(vals["C"]))
+                     )
+        # å°† xor(^) æ˜ å°„ä¸ºä¸ç­‰ï¼šx ^ y ç­‰ä»·äº (x != y)
+        expr_eval = re.sub(r"\^", " != ", expr_eval)
+        gold = bool(eval(expr_eval))  # å®‰å…¨å‰æï¼šå—æ§ç”Ÿæˆ
+        prompt = (
+            f"Given A={vals['A']}, B={vals['B']}, C={vals['C']}, evaluate: {expr}. "
+            f"Answer 'True' or 'False' only."
+        )
+        data.append({"prompt": prompt, "gold": gold})
+    return data
+
+
+# ========== è§£æä¸æ ‡å‡†åŒ– ==========
+
+def parse_int_from_text(text: str) -> int | None:
+    # æå–é¦–ä¸ªæœ‰ç¬¦å·æ•´æ•°
+    m = re.search(r"[-+]?\d+", text)
+    if not m:
+        return None
+    try:
+        return int(m.group(0))
+    except Exception:
+        return None
+
+
+def parse_bool_from_text(text: str) -> bool | None:
+    t = text.strip().lower()
+    if t in ("true", "1", "yes"):  # å®½æ¾ä¸€äº›
+        return True
+    if t in ("false", "0", "no"):
+        return False
+    # å…œåº•ï¼šæå–ç¬¬ä¸€ä¸ªå•è¯
+    m = re.search(r"\b(true|false|1|0)\b", t)
+    if m:
+        return True if m.group(1) in ("true", "1") else False
+    return None
+
+
+# ========== æ‰¹å¤„ç†è¾…åŠ© ==========
+
+def dot_collate(prompts: List[str], tokenizer, max_length: int = 256) -> Dict:
+    encoded = tokenizer(
+        prompts,
+        padding=True,
+        truncation=True,
+        max_length=max_length,
+        return_tensors="pt"
+    )
+    return {
+        "input_ids": encoded["input_ids"],
+        "attention_mask": encoded["attention_mask"],
+    }
diff --git a/main_sparsity_aware.py b/main_sparsity_aware.py
index c6b75f7..83dbd54 100644
--- a/main_sparsity_aware.py
+++ b/main_sparsity_aware.py
@@ -1,4 +1,4 @@
-import os
+g1import os
 
 os.environ["JAX_PLATFORM_NAME"] = "cpu"
 import pickle
@@ -77,6 +77,29 @@ def parse_arguments():
     parser.add_argument("--bfcl_weight", type=float, default=0.5,
                         help="Weight for BFCL task in multi-task learning")
     
+    # ğŸ¯ MBPPä»£ç ç”Ÿæˆè¯„ä¼°å‚æ•°
+    parser.add_argument("--use_mbpp_eval", action="store_true",
+                        help="Enable MBPP code generation evaluation")
+    parser.add_argument("--mbpp_data_path", type=str,
+                        default="mbpp",
+                        help="Path to MBPP test dataset or HF identifier")
+    parser.add_argument("--mbpp_weight", type=float, default=0.33,
+                        help="Weight for MBPP task in multi-task learning")
+    
+    # DoT: 4x4 / 5x5 Multiplication & Boolean Logic (optional)
+    parser.add_argument("--use_mult4_eval", action="store_true",
+                        help="Enable 4x4 multiplication evaluation (DoT-style)")
+    parser.add_argument("--use_mult5_eval", action="store_true",
+                        help="Enable 5x5 multiplication evaluation (DoT-style)")
+    parser.add_argument("--use_bool_eval", action="store_true",
+                        help="Enable Boolean logic evaluation (DoT-style)")
+    parser.add_argument("--mult4_weight", type=float, default=0.0,
+                        help="Weight for 4x4 multiplication in multi-task")
+    parser.add_argument("--mult5_weight", type=float, default=0.0,
+                        help="Weight for 5x5 multiplication in multi-task")
+    parser.add_argument("--bool_weight", type=float, default=0.0,
+                        help="Weight for Boolean logic in multi-task")
+    
     # ğŸ”„ åŠ¨æ€ç¨€ç–åº¦è°ƒåº¦å‚æ•°ï¼ˆCosine Annealing with Warm Restartsï¼‰
     parser.add_argument("--use_dynamic_sparsity", action="store_true",
                         help="Enable dynamic sparsity scheduling (overrides --pruning_sparsity)")
@@ -109,6 +132,18 @@ def main():
         print(f"  GSM8K weight: {args.gsm8k_weight}")
         print(f"  BFCL weight: {args.bfcl_weight}")
         print(f"  BFCL data: {args.bfcl_data_path}")
+    if args.use_mbpp_eval:
+        print(f"ğŸ¯ MBPP Evaluation ENABLED")
+        print(f"  MBPP weight: {args.mbpp_weight}")
+        print(f"  MBPP data: {args.mbpp_data_path}")
+    if args.use_mult4_eval or args.use_mult5_eval or args.use_bool_eval:
+        print(f"ğŸ¯ DoT-style tasks:")
+        if args.use_mult4_eval:
+            print(f"  4x4 Mult. weight: {args.mult4_weight}")
+        if args.use_mult5_eval:
+            print(f"  5x5 Mult. weight: {args.mult5_weight}")
+        if args.use_bool_eval:
+            print(f"  Boolean Logic weight: {args.bool_weight}")
     print(f"\nSparsity-Aware Parameters:")
     print(f"  Ï‰ (omega): {args.omega} - Fitness weight")
     print(f"  Î² (beta): {args.beta} - Sparsity weight")
@@ -169,6 +204,16 @@ def main():
         bfcl_data_path=args.bfcl_data_path,
         gsm8k_weight=args.gsm8k_weight,
         bfcl_weight=args.bfcl_weight,
+        use_mbpp_eval=args.use_mbpp_eval,  # ğŸ¯ MBPPè¯„ä¼°
+        mbpp_data_path=args.mbpp_data_path,
+        mbpp_weight=args.mbpp_weight,
+        # DoT tasks
+        use_mult4_eval=args.use_mult4_eval,
+        use_mult5_eval=args.use_mult5_eval,
+        use_bool_eval=args.use_bool_eval,
+        mult4_weight=args.mult4_weight,
+        mult5_weight=args.mult5_weight,
+        bool_weight=args.bool_weight,
         # ğŸ”„ åŠ¨æ€ç¨€ç–åº¦å‚æ•°
         use_dynamic_sparsity=args.use_dynamic_sparsity,
         sparsity_min=args.sparsity_min,
diff --git a/mbpp/data/mbpp_test_sample.json b/mbpp/data/mbpp_test_sample.json
new file mode 100644
index 0000000..0baad90
--- /dev/null
+++ b/mbpp/data/mbpp_test_sample.json
@@ -0,0 +1,63 @@
+[
+  {
+    "task_id": 1,
+    "text": "ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œæ£€æŸ¥ç»™å®šçš„æ•°å­—æ˜¯å¦æ˜¯å¶æ•°ã€‚",
+    "code": "def is_even(n):\n    return n % 2 == 0",
+    "test_list": [
+      "assert is_even(2) == True",
+      "assert is_even(3) == False",
+      "assert is_even(0) == True"
+    ],
+    "test_setup_code": "",
+    "challenge_test_list": []
+  },
+  {
+    "task_id": 2,
+    "text": "ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œè¿”å›åˆ—è¡¨ä¸­çš„æœ€å¤§å€¼ã€‚",
+    "code": "def find_max(numbers):\n    return max(numbers)",
+    "test_list": [
+      "assert find_max([1, 2, 3, 4, 5]) == 5",
+      "assert find_max([10, 20, 5]) == 20",
+      "assert find_max([-1, -5, -3]) == -1"
+    ],
+    "test_setup_code": "",
+    "challenge_test_list": []
+  },
+  {
+    "task_id": 3,
+    "text": "ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œè®¡ç®—ä¸¤ä¸ªæ•°çš„å’Œã€‚",
+    "code": "def add(a, b):\n    return a + b",
+    "test_list": [
+      "assert add(2, 3) == 5",
+      "assert add(-1, 1) == 0",
+      "assert add(0, 0) == 0"
+    ],
+    "test_setup_code": "",
+    "challenge_test_list": []
+  },
+  {
+    "task_id": 4,
+    "text": "ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œåè½¬å­—ç¬¦ä¸²ã€‚",
+    "code": "def reverse_string(s):\n    return s[::-1]",
+    "test_list": [
+      "assert reverse_string('hello') == 'olleh'",
+      "assert reverse_string('python') == 'nohtyp'",
+      "assert reverse_string('') == ''"
+    ],
+    "test_setup_code": "",
+    "challenge_test_list": []
+  },
+  {
+    "task_id": 5,
+    "text": "ç¼–å†™ä¸€ä¸ªå‡½æ•°ï¼Œè®¡ç®—åˆ—è¡¨ä¸­æ‰€æœ‰æ•°å­—çš„å’Œã€‚",
+    "code": "def sum_list(numbers):\n    return sum(numbers)",
+    "test_list": [
+      "assert sum_list([1, 2, 3, 4, 5]) == 15",
+      "assert sum_list([10, 20, 30]) == 60",
+      "assert sum_list([]) == 0"
+    ],
+    "test_setup_code": "",
+    "challenge_test_list": []
+  }
+]
+
diff --git a/mbpp_data_utils.py b/mbpp_data_utils.py
new file mode 100644
index 0000000..4d48be4
--- /dev/null
+++ b/mbpp_data_utils.py
@@ -0,0 +1,147 @@
+#!/usr/bin/env python3
+"""
+MBPPæ•°æ®åŠ è½½ä¸å¤„ç†å·¥å…·
+æ”¯æŒMBPP (Mostly Basic Python Problems)æ•°æ®é›†çš„åŠ è½½ã€é¢„å¤„ç†å’Œæ‰¹å¤„ç†
+"""
+
+import os
+import json
+import torch
+from typing import Dict, List, Any
+from torch.utils.data import Dataset
+from datasets import load_dataset
+
+
+class MBPPDataset(Dataset):
+    """
+    MBPPæ•°æ®é›†ç±»ï¼Œæ”¯æŒä»HuggingFace datasetsåŠ è½½æˆ–æœ¬åœ°æ–‡ä»¶åŠ è½½
+    """
+    
+    def __init__(self, data_path: str, tokenizer=None, split="test"):
+        """
+        Args:
+            data_path: MBPPæ•°æ®é›†æ ‡è¯†ç¬¦ (å¦‚ 'mbpp') æˆ–æœ¬åœ°æ–‡ä»¶è·¯å¾„
+            tokenizer: HuggingFace tokenizer
+            split: æ•°æ®é›†åˆ’åˆ† ('test', 'train', 'validation')
+        """
+        self.data = self._load_data(data_path, split)
+        self.tokenizer = tokenizer
+    
+    def _load_data(self, data_path: str, split: str) -> List[Dict]:
+        """ä»HuggingFace datasetsæˆ–æœ¬åœ°æ–‡ä»¶åŠ è½½MBPPæ•°æ®"""
+        if os.path.exists(data_path):
+            # ä»æœ¬åœ°æ–‡ä»¶åŠ è½½
+            print(f"Loading MBPP data from local path: {data_path}")
+            with open(data_path, 'r', encoding='utf-8') as f:
+                if data_path.endswith('.jsonl'):
+                    data = [json.loads(line) for line in f if line.strip()]
+                else:
+                    data = json.load(f)
+        else:
+            # ä»HuggingFace datasetsåŠ è½½
+            print(f"Loading MBPP data from HuggingFace Hub: '{data_path}' (split: {split})")
+            try:
+                dataset = load_dataset(data_path, 'sanitized', split=split)
+                data = list(dataset)
+            except Exception as e:
+                print(f"Failed to load from HuggingFace Hub. Error: {e}")
+                # Fallback to default mbpp if 'sanitized' fails
+                try:
+                    dataset = load_dataset(data_path, split=split)
+                    data = list(dataset)
+                except Exception as e_fallback:
+                    print(f"Fallback to default MBPP also failed. Error: {e_fallback}")
+                    raise ValueError("Could not load MBPP dataset from Hub or local path.")
+
+        if isinstance(data, dict):
+            data = [data]
+        
+        return data
+    
+    def __len__(self):
+        return len(self.data)
+    
+    def __getitem__(self, idx):
+        item = self.data[idx]
+        
+        # æ„å»ºpromptï¼ˆæŒ‡å¯¼æ¨¡å‹ç”Ÿæˆä»£ç ï¼‰
+        prompt = self._build_prompt(item)
+        
+        return {
+            "task_id": item.get("task_id", idx),
+            "prompt": prompt,
+            "text": item.get("text", ""),
+            "test_list": item.get("test_list", []),
+            "test_setup_code": item.get("test_setup_code", ""),
+            "challenge_test_list": item.get("challenge_test_list", []),
+            "reference_code": item.get("code", ""),  # å‚è€ƒå®ç°
+        }
+    
+    def _build_prompt(self, item: Dict) -> str:
+        """
+        æ„å»ºç”Ÿæˆä»£ç çš„prompt
+        
+        æ ¼å¼ï¼š
+        è¯·å®ç°ä»¥ä¸‹Pythonå‡½æ•°ï¼š
+        
+        {text}
+        
+        è¦æ±‚ï¼š
+        - åªè¾“å‡ºå®Œæ•´çš„Pythonå‡½æ•°å®ç°ä»£ç 
+        - ä¸è¦åŒ…å«è§£é‡Šæ–‡å­—
+        - ä¸è¦åŒ…å«if __name__ == '__main__'å—
+        - ç¡®ä¿ä»£ç å¯ä»¥ç›´æ¥æ‰§è¡Œ
+        """
+        text = item.get("text", "")
+        
+        prompt = f"""è¯·å®ç°ä»¥ä¸‹Pythonå‡½æ•°ï¼š
+
+{text}
+
+è¦æ±‚ï¼š
+- åªè¾“å‡ºå®Œæ•´çš„Pythonå‡½æ•°å®ç°ä»£ç 
+- ä¸è¦åŒ…å«è§£é‡Šæ–‡å­—å’Œé¢å¤–çš„importè¯­å¥ï¼ˆé™¤éå¿…éœ€ï¼‰
+- ä¸è¦åŒ…å«if __name__ == '__main__'å—
+- ç¡®ä¿ä»£ç å¯ä»¥ç›´æ¥æ‰§è¡Œå¹¶é€šè¿‡æµ‹è¯•
+
+å‡½æ•°å®ç°ï¼š
+"""
+        return prompt
+
+
+def mbpp_collate_fn(batch, tokenizer, max_length=512):
+    """
+    MBPPæ‰¹å¤„ç†å‡½æ•°
+    
+    Args:
+        batch: ä¸€æ‰¹æ ·æœ¬
+        tokenizer: HuggingFace tokenizer
+        max_length: æœ€å¤§åºåˆ—é•¿åº¦
+    
+    Returns:
+        æ‰¹å¤„ç†åçš„å­—å…¸ï¼ŒåŒ…å«ï¼š
+        - input_ids: tensor (batch_size, seq_len)
+        - attention_mask: tensor (batch_size, seq_len)
+        - test_list: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨
+        - task_ids: ä»»åŠ¡IDåˆ—è¡¨
+    """
+    prompts = [item["prompt"] for item in batch]
+    
+    # Tokenize
+    encoded = tokenizer(
+        prompts,
+        padding=True,
+        truncation=True,
+        max_length=max_length,
+        return_tensors="pt"
+    )
+    
+    return {
+        "input_ids": encoded["input_ids"],
+        "attention_mask": encoded["attention_mask"],
+        "test_list": [item["test_list"] for item in batch],
+        "test_setup_code": [item.get("test_setup_code", "") for item in batch],
+        "task_ids": [item["task_id"] for item in batch],
+        "prompts": prompts,  # ä¿ç•™åŸå§‹promptç”¨äºè°ƒè¯•
+    }
+
diff --git a/natural_niches_sparsity_aware_fn.py b/natural_niches_sparsity_aware_fn.py
index 6a24f40..27357b8 100644
--- a/natural_niches_sparsity_aware_fn.py
+++ b/natural_niches_sparsity_aware_fn.py
@@ -72,6 +72,13 @@ from helper_fn import (
 )
 from config import GSM8K_DIR, RESULTS_DIR
 from lib.async_shard import AsyncShardCoordinator
+from dot_eval_utils import (
+    generate_mult_dataset,
+    generate_bool_dataset,
+    parse_int_from_text,
+    parse_bool_from_text,
+    dot_collate,
+)
 
 
 def _init_distributed_if_needed() -> tuple[int, int]:
@@ -867,6 +874,10 @@ def run_natural_niches_sparsity_aware(
     bfcl_data_path: str = "bfcl/data/bfcl_test_200.json",  # BFCLæ•°æ®è·¯å¾„
     gsm8k_weight: float = 0.5,  # GSM8Kä»»åŠ¡æƒé‡
     bfcl_weight: float = 0.5,  # BFCLä»»åŠ¡æƒé‡
+    # ğŸ¯ MBPP: MBPPä»£ç ç”Ÿæˆè¯„ä¼°
+    use_mbpp_eval: bool = False,  # æ˜¯å¦å¯ç”¨MBPPè¯„ä¼°
+    mbpp_data_path: str = "mbpp/data/mbpp_test.json",  # MBPPæ•°æ®è·¯å¾„
+    mbpp_weight: float = 0.33,  # MBPPä»»åŠ¡æƒé‡
     # ğŸ”„ NEW: Dynamic Sparsity with Warm Restarts
     use_dynamic_sparsity: bool = False,  # æ˜¯å¦å¯ç”¨åŠ¨æ€ç¨€ç–åº¦è°ƒåº¦
     sparsity_min: float = 0.1,  # æœ€å°ç¨€ç–åº¦ (eta_min)
@@ -875,6 +886,13 @@ def run_natural_niches_sparsity_aware(
     sparsity_t_mult: int = 2,  # å‘¨æœŸé•¿åº¦ä¹˜æ•°ï¼ˆ1=å›ºå®šå‘¨æœŸï¼Œ2=æ¯æ¬¡ç¿»å€ï¼‰
     async_num_nodes: Optional[int] = None,
     async_sync_interval: int = 10,
+    # DoT tasks optional
+    use_mult4_eval: bool = False,
+    use_mult5_eval: bool = False,
+    use_bool_eval: bool = False,
+    mult4_weight: float = 0.0,
+    mult5_weight: float = 0.0,
+    bool_weight: float = 0.0,
 ) -> list:
     """
     Run Natural Niches with Sparsity-Aware Selection and Wanda Pruning
@@ -1036,7 +1054,26 @@ def run_natural_niches_sparsity_aware(
             bfcl_dataset = None
             use_bfcl_eval = False
 
-    # åˆå§‹num_tasksè®¾ç½®ï¼ˆåç»­ä¼šæ ¹æ®æ˜¯å¦ä½¿ç”¨BFCLå’Œåˆ†å¸ƒå¼è°ƒæ•´ï¼‰
+    # ============================================================================
+    # ğŸ¯ MBPP Data Loading (if enabled)
+    # ============================================================================
+    mbpp_dataset = None
+    if use_mbpp_eval:
+        if is_main_process:
+            print(f"\nğŸ¯ Loading MBPP dataset: {mbpp_data_path}")
+        try:
+            from mbpp_data_utils import MBPPDataset
+            mbpp_dataset = MBPPDataset(mbpp_data_path, tokenizer)
+            if is_main_process:
+                print(f"âœ… MBPP dataset loaded: {len(mbpp_dataset)} samples")
+        except Exception as e:
+            if is_main_process:
+                print(f"âŒ Failed to load MBPP dataset: {e}")
+                print("Continuing without MBPP...")
+            mbpp_dataset = None
+            use_mbpp_eval = False
+    
+    # åˆå§‹num_tasksè®¾ç½®ï¼ˆåç»­ä¼šæ ¹æ®æ˜¯å¦ä½¿ç”¨BFCL/MBPPå’Œåˆ†å¸ƒå¼è°ƒæ•´ï¼‰
     num_tasks = len(tokenized_train_dataset)
     if dist_enabled and world_size > 1:
         num_tasks = num_tasks * world_size  # åˆ†å¸ƒå¼èšåˆåçš„æ€»ä»»åŠ¡æ•°
@@ -1061,12 +1098,45 @@ def run_natural_niches_sparsity_aware(
     # ============================================================================
     # ğŸ¯ Create Evaluation Functions (GSM8K or Multi-Task)
     # ============================================================================
-    if use_bfcl_eval and bfcl_dataset is not None:
-        # Multi-task evaluation: GSM8K + BFCL
+    if (use_bfcl_eval and bfcl_dataset is not None) or (use_mbpp_eval and mbpp_dataset is not None) or (use_mult4_eval or use_mult5_eval or use_bool_eval):
+        # Multi-task evaluation: GSM8K + (BFCL) + (MBPP) + (DoT)
         if is_main_process:
-            print("\nğŸ¯ Creating Multi-Task Evaluation (GSM8K + BFCL)")
+            task_names = ["GSM8K"]
+            if bfcl_dataset is not None and use_bfcl_eval:
+                task_names.append("BFCL")
+            if mbpp_dataset is not None and use_mbpp_eval:
+                task_names.append("MBPP")
+            if use_mult4_eval:
+                task_names.append("4x4 Mult.")
+            if use_mult5_eval:
+                task_names.append("5x5 Mult.")
+            if use_bool_eval:
+                task_names.append("Boolean")
+            print(f"\nğŸ¯ Creating Multi-Task Evaluation ({' + '.join(task_names)})")
             print(f"  GSM8K weight: {gsm8k_weight}")
-            print(f"  BFCL weight: {bfcl_weight}")
+            if use_bfcl_eval and bfcl_dataset is not None:
+                print(f"  BFCL weight: {bfcl_weight}")
+            if use_mbpp_eval and mbpp_dataset is not None:
+                print(f"  MBPP weight: {mbpp_weight}")
+            if use_mult4_eval:
+                print(f"  4x4 Mult. weight: {mult4_weight}")
+            if use_mult5_eval:
+                print(f"  5x5 Mult. weight: {mult5_weight}")
+            if use_bool_eval:
+                print(f"  Boolean weight: {bool_weight}")
+        
+        # ä»»åŠ¡æƒé‡å­—å…¸ï¼ˆå¦‚éœ€ç”¨åˆ°ï¼‰
+        task_weights_dict = {"gsm8k": gsm8k_weight}
+        if use_bfcl_eval and bfcl_dataset is not None:
+            task_weights_dict["bfcl"] = bfcl_weight
+        if use_mbpp_eval and mbpp_dataset is not None:
+            task_weights_dict["mbpp"] = mbpp_weight
+        if use_mult4_eval:
+            task_weights_dict["mult4"] = mult4_weight
+        if use_mult5_eval:
+            task_weights_dict["mult5"] = mult5_weight
+        if use_bool_eval:
+            task_weights_dict["bool"] = bool_weight
 
         train_eval_fn = create_multi_task_evaluation_fn(
             model_skeleton,
@@ -1074,11 +1144,15 @@ def run_natural_niches_sparsity_aware(
             tokenized_train_dataset,
             bfcl_dataset,
             tokenizer,
-            task_weights={"gsm8k": gsm8k_weight, "bfcl": bfcl_weight},
+            task_weights=task_weights_dict,
             distributed=dist_enabled,
             world_size=world_size,
+            mbpp_dataset=mbpp_dataset,
             rank=rank,
             eval_subset_size=eval_subset_size,
+            use_mult4_eval=use_mult4_eval,
+            use_mult5_eval=use_mult5_eval,
+            use_bool_eval=use_bool_eval,
         )
 
         # Test evaluation: GSM8K only (for compatibility)
@@ -1094,12 +1168,34 @@ def run_natural_niches_sparsity_aware(
         )
 
         # Update num_tasks for competitive normalization
-        # Multi-task: eval_subset_size * 2 (GSM8K + BFCL)
+        # Multi-task: eval_subset_size * num_active_tasks
+        num_active_tasks = 1  # GSM8K
+        if use_bfcl_eval and bfcl_dataset is not None:
+            num_active_tasks += 1
+        if use_mbpp_eval and mbpp_dataset is not None:
+            num_active_tasks += 1
+        if use_mult4_eval:
+            num_active_tasks += 1
+        if use_mult5_eval:
+            num_active_tasks += 1
+        if use_bool_eval:
+            num_active_tasks += 1
         if eval_subset_size is not None:
-            num_tasks = eval_subset_size * 2
+            num_tasks = eval_subset_size * num_active_tasks
         else:
-            num_tasks = len(tokenized_train_dataset) + len(bfcl_dataset)
-
+            num_tasks = len(tokenized_train_dataset)
+            if use_bfcl_eval and bfcl_dataset is not None:
+                num_tasks += len(bfcl_dataset)
+            if use_mbpp_eval and mbpp_dataset is not None:
+                num_tasks += len(mbpp_dataset)
+            # DoTåœ¨çº¿ä»»åŠ¡ï¼šè‹¥æœªé‡‡æ ·å­é›†ï¼ŒæŒ‰è¯„ä¼°é»˜è®¤æ•°é‡ï¼ˆä¸eval_subset_sizeç­‰åŒæˆ–20ï¼‰
+            default_dot = 20
+            if use_mult4_eval:
+                num_tasks += default_dot
+            if use_mult5_eval:
+                num_tasks += default_dot
+            if use_bool_eval:
+                num_tasks += default_dot
     else:
         # Single-task evaluation: GSM8K only
         if is_main_process:
@@ -1800,6 +1896,188 @@ def create_bfcl_evaluation_fn(
     return evaluation_fn
 
 
+# ========== MBPPè¯„ä¼°å‡½æ•° ==========
+def create_mbpp_evaluation_fn(
+    model_skeleton,
+    param_shapes,
+    mbpp_dataset,
+    tokenizer: AutoTokenizer,
+    batch_size: int = 4,
+    distributed: bool = False,
+    world_size: int = 1,
+    rank: int = 0,
+    eval_subset_size: int = None,
+    return_subset_only: bool = False,  # å¤šä»»åŠ¡è¯„ä¼°æ—¶è®¾ä¸ºTrueï¼Œä¸è¿›è¡Œåˆ†å¸ƒå¼èšåˆ
+):
+    """
+    åˆ›å»ºMBPP (Mostly Basic Python Problems) è¯„ä¼°å‡½æ•°
+    
+    è¯„ä¼°ä»£ç ç”Ÿæˆèƒ½åŠ›ï¼š
+    1. ç»™å®šé—®é¢˜æè¿°
+    2. æ¨¡å‹ç”ŸæˆPythonä»£ç 
+    3. æ‰§è¡Œå•å…ƒæµ‹è¯•éªŒè¯æ­£ç¡®æ€§
+    
+    Returns:
+        evaluation_fn: è¿”å›æ¯ä¸ªæ ·æœ¬çš„å¾—åˆ† (1.0=æ‰€æœ‰æµ‹è¯•é€šè¿‡, 0.0=å¤±è´¥)
+    """
+    from mbpp_data_utils import mbpp_collate_fn
+    from torch.utils.data import DataLoader, Subset
+    import random
+    import subprocess
+    import tempfile
+    import uuid
+    
+    device = next(model_skeleton.parameters()).device
+    iteration_counter = {'count': 0}
+    
+    def safe_execute_code(code: str, tests: list, setup_code: str = "", timeout: int = 10) -> bool:
+        """
+        å®‰å…¨æ‰§è¡Œä»£ç å¹¶è¿è¡Œæµ‹è¯•
+        
+        Args:
+            code: ç”Ÿæˆçš„ä»£ç 
+            tests: æµ‹è¯•ç”¨ä¾‹åˆ—è¡¨ï¼ˆassertè¯­å¥ï¼‰
+            setup_code: æµ‹è¯•å‰ç½®ä»£ç 
+            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
+        
+        Returns:
+            æ˜¯å¦æ‰€æœ‰æµ‹è¯•é€šè¿‡
+        """
+        # æ„å»ºå®Œæ•´çš„æµ‹è¯•ç¨‹åº
+        program_parts = []
+        
+        # æ·»åŠ setupä»£ç 
+        if setup_code:
+            program_parts.append(setup_code)
+        
+        # æ·»åŠ ç”Ÿæˆçš„ä»£ç 
+        program_parts.append(code)
+        
+        # æ·»åŠ æµ‹è¯•ç”¨ä¾‹
+        program_parts.extend(tests)
+        
+        # æ·»åŠ æˆåŠŸæ ‡è®°
+        program_parts.append("print('__MBPP_ALL_TESTS_PASSED__')")
+        
+        program = "\n".join(program_parts)
+        
+        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶æ‰§è¡Œ
+        try:
+            with tempfile.TemporaryDirectory() as tmpdir:
+                filepath = os.path.join(tmpdir, f"{uuid.uuid4().hex}.py")
+                with open(filepath, "w", encoding="utf-8") as f:
+                    f.write(program)
+                
+                # æ‰§è¡Œä»£ç 
+                result = subprocess.run(
+                    ["python3", filepath],
+                    capture_output=True,
+                    text=True,
+                    timeout=timeout,
+                    env={"PYTHONDONTWRITEBYTECODE": "1"}  # ä¸ç”Ÿæˆ.pycæ–‡ä»¶
+                )
+                
+                # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
+                success = (
+                    "__MBPP_ALL_TESTS_PASSED__" in (result.stdout or "") 
+                    and result.returncode == 0
+                )
+                
+                return success
+                
+        except subprocess.TimeoutExpired:
+            return False  # è¶…æ—¶è§†ä¸ºå¤±è´¥
+        except Exception:
+            return False  # ä»»ä½•å¼‚å¸¸éƒ½è§†ä¸ºå¤±è´¥
+    
+    def evaluation_fn(flat_params: jnp.ndarray) -> jnp.ndarray:
+        """è¯„ä¼°MBPPä»»åŠ¡"""
+        iteration_counter['count'] += 1
+        
+        # é‡‡æ ·å­é›†
+        if eval_subset_size is not None and eval_subset_size < len(mbpp_dataset):
+            indices = random.sample(range(len(mbpp_dataset)), eval_subset_size)
+            eval_dataset = Subset(mbpp_dataset, indices)
+            if rank == 0:
+                print(f"  [MBPP] é‡‡æ · {eval_subset_size}/{len(mbpp_dataset)} æ ·æœ¬")
+        else:
+            eval_dataset = mbpp_dataset
+        
+        # DataLoader (ä½¿ç”¨MBPPä¸“ç”¨çš„collate_fn)
+        dataloader = DataLoader(
+            eval_dataset,
+            batch_size=batch_size,
+            shuffle=False,
+            num_workers=0,
+            collate_fn=lambda batch: mbpp_collate_fn(batch, tokenizer),
+        )
+        
+        # é‡å»ºæ¨¡å‹å‚æ•°ï¼ˆä½¿ç”¨å’ŒGSM8K/BFCLç›¸åŒçš„æ–¹å¼ï¼‰
+        base_model = (
+            model_skeleton.module if hasattr(model_skeleton, "module") else model_skeleton
+        )
+        restored_model = jax_flattened_to_pytorch_model(
+            flat_params, base_model, param_shapes
+        )
+        restored_model.eval()
+        
+        # è¯„ä¼°
+        all_scores = []
+        with torch.no_grad():
+            for batch in dataloader:
+                input_ids = batch['input_ids'].to(device)
+                attention_mask = batch['attention_mask'].to(device)
+                test_lists = batch['test_list']
+                setup_codes = batch['test_setup_code']
+                
+                # Generateä»£ç 
+                generated_ids = restored_model.generate(
+                    input_ids=input_ids,
+                    attention_mask=attention_mask,
+                    max_new_tokens=512,  # MBPPå¯èƒ½éœ€è¦æ›´é•¿çš„ä»£ç 
+                    do_sample=False,
+                    temperature=0.2,  # ä½æ¸©åº¦ç¡®ä¿ç¨³å®šæ€§
+                    pad_token_id=tokenizer.pad_token_id,
+                    eos_token_id=tokenizer.eos_token_id,
+                )
+                
+                # Decodeç”Ÿæˆçš„ä»£ç 
+                generated_codes = tokenizer.batch_decode(
+                    generated_ids[:, input_ids.shape[1]:],
+                    skip_special_tokens=True
+                )
+                
+                # æ‰§è¡Œæµ‹è¯•è¯„ä¼°æ¯ä¸ªæ ·æœ¬
+                for gen_code, tests, setup in zip(generated_codes, test_lists, setup_codes):
+                    try:
+                        # æ¸…ç†ç”Ÿæˆçš„ä»£ç ï¼ˆç§»é™¤markdownä»£ç å—æ ‡è®°ç­‰ï¼‰
+                        clean_code = gen_code.strip()
+                        if clean_code.startswith("```python"):
+                            clean_code = clean_code[len("```python"):].strip()
+                        if clean_code.startswith("```"):
+                            clean_code = clean_code[3:].strip()
+                        if clean_code.endswith("```"):
+                            clean_code = clean_code[:-3].strip()
+                        
+                        # æ‰§è¡Œæµ‹è¯•
+                        is_correct = safe_execute_code(clean_code, tests, setup)
+                        all_scores.append(1.0 if is_correct else 0.0)
+                    except Exception:
+                        all_scores.append(0.0)  # ä»»ä½•å¼‚å¸¸éƒ½è§†ä¸ºå¤±è´¥
+        
+        # åˆ†å¸ƒå¼èšåˆï¼ˆä¸GSM8K/BFCLè¯„ä¼°å‡½æ•°ä¿æŒä¸€è‡´ï¼‰
+        if distributed and world_size > 1 and not return_subset_only:
+            scores_tensor = torch.tensor(all_scores, dtype=torch.float32, device=device)
+            gathered = [torch.zeros_like(scores_tensor) for _ in range(world_size)]
+            torch.distributed.all_gather(gathered, scores_tensor)
+            # æˆªæ–­åˆ°eval_datasetçš„é•¿åº¦
+            all_scores = torch.cat(gathered)[:len(eval_dataset)].cpu().numpy().tolist()
+        
+        return jnp.array(all_scores, dtype=jnp.float32)
+    
+    return evaluation_fn
+
+
 # ========== å¤šä»»åŠ¡è¯„ä¼°å‡½æ•° ==========
 def create_multi_task_evaluation_fn(
     model_skeleton,
@@ -1813,22 +2091,31 @@ def create_multi_task_evaluation_fn(
     world_size=1,
     rank=0,
     eval_subset_size=None,
+    mbpp_dataset=None,  # ğŸ†• æ–°å¢MBPPæ•°æ®é›†å‚æ•°
+    use_mult4_eval: bool = False,
+    use_mult5_eval: bool = False,
+    use_bool_eval: bool = False,
 ):
     """
     åˆ›å»ºå¤šä»»åŠ¡è¯„ä¼°å‡½æ•°ï¼šåŒæ—¶è¯„ä¼°GSM8Kå’ŒBFCL
 
     Args:
-        task_weights: ä»»åŠ¡æƒé‡å­—å…¸ï¼Œä¾‹å¦‚ {"gsm8k": 0.5, "bfcl": 0.5}
+        task_weights: ä»»åŠ¡æƒé‡å­—å…¸ï¼Œä¾‹å¦‚ {"gsm8k": 0.4, "bfcl": 0.3, "mbpp": 0.3}
                      å¦‚æœä¸ºNoneï¼Œåˆ™æ‹¼æ¥æ‰€æœ‰ä»»åŠ¡çš„åˆ†æ•°
         eval_subset_size: æ¯ä¸ªä»»åŠ¡é‡‡æ ·çš„æ ·æœ¬æ•°
-
+        mbpp_dataset: MBPPæ•°æ®é›†ï¼ˆå¦‚æœä¸ºNoneåˆ™ä¸è¯„ä¼°MBPPï¼‰
+        
     Returns:
         evaluation_fn: è¿”å›æ‰€æœ‰ä»»åŠ¡çš„åˆ†æ•°æ‹¼æ¥ç»“æœ
     """
+    # é»˜è®¤æƒé‡
     if task_weights is None:
-        task_weights = {"gsm8k": 0.5, "bfcl": 0.5}
-
-    # åˆ›å»ºä¸¤ä¸ªè¯„ä¼°å‡½æ•°
+        if mbpp_dataset is not None:
+            task_weights = {"gsm8k": 0.4, "bfcl": 0.3, "mbpp": 0.3}
+        else:
+            task_weights = {"gsm8k": 0.5, "bfcl": 0.5}
+    
+    # åˆ›å»ºGSM8Kè¯„ä¼°å‡½æ•°
     gsm8k_eval_fn = create_evaluation_fn_for_llm(
         model_skeleton,
         param_shapes,
@@ -1841,7 +2128,8 @@ def create_multi_task_evaluation_fn(
         eval_subset_size=eval_subset_size,
         return_subset_only=True,  # å¤šä»»åŠ¡ï¼šä¸æ‰©å±•ï¼Œç›´æ¥è¿”å›å­é›†åˆ†æ•°
     )
-
+    
+    # åˆ›å»ºBFCLè¯„ä¼°å‡½æ•°
     bfcl_eval_fn = create_bfcl_evaluation_fn(
         model_skeleton,
         param_shapes,
@@ -1854,16 +2142,139 @@ def create_multi_task_evaluation_fn(
         eval_subset_size=eval_subset_size,
         return_subset_only=True,  # å¤šä»»åŠ¡è¯„ä¼°ï¼šä¸è¿›è¡Œåˆ†å¸ƒå¼èšåˆ
     )
+    
+    # åˆ›å»ºMBPPè¯„ä¼°å‡½æ•°ï¼ˆå¦‚æœæä¾›äº†æ•°æ®é›†ï¼‰
+    mbpp_eval_fn = None
+    if mbpp_dataset is not None:
+        mbpp_eval_fn = create_mbpp_evaluation_fn(
+            model_skeleton, param_shapes, mbpp_dataset, tokenizer,
+            batch_size=batch_size,
+            distributed=distributed,
+            world_size=world_size,
+            rank=rank,
+            eval_subset_size=eval_subset_size,
+            return_subset_only=True,  # å¤šä»»åŠ¡è¯„ä¼°ï¼šä¸è¿›è¡Œåˆ†å¸ƒå¼èšåˆ
+        )
+    
+    # åˆ›å»ºDoTè¯„ä¼°å‡½æ•°ï¼ˆåœ¨çº¿ç”Ÿæˆï¼‰
+    mult4_eval_fn = None
+    mult5_eval_fn = None
+    bool_eval_fn = None
+    if use_mult4_eval:
+        mult4_eval_fn = create_dot_eval_fn(
+            model_skeleton, param_shapes, tokenizer,
+            task='mult4', num_samples=(eval_subset_size or 20),
+            batch_size=max(1, batch_size), distributed=distributed,
+            world_size=world_size, rank=rank,
+        )
+    if use_mult5_eval:
+        mult5_eval_fn = create_dot_eval_fn(
+            model_skeleton, param_shapes, tokenizer,
+            task='mult5', num_samples=(eval_subset_size or 20),
+            batch_size=max(1, batch_size), distributed=distributed,
+            world_size=world_size, rank=rank,
+        )
+    if use_bool_eval:
+        bool_eval_fn = create_dot_eval_fn(
+            model_skeleton, param_shapes, tokenizer,
+            task='bool', num_samples=(eval_subset_size or 20),
+            batch_size=max(1, batch_size), distributed=distributed,
+            world_size=world_size, rank=rank,
+        )
 
     def evaluation_fn(flat_params):
-        """è¯„ä¼°ä¸¤ä¸ªä»»åŠ¡å¹¶æ‹¼æ¥åˆ†æ•°"""
-        # è¯„ä¼°ä¸¤ä¸ªä»»åŠ¡
-        gsm8k_scores = gsm8k_eval_fn(flat_params)  # shape: (n1,)
-        bfcl_scores = bfcl_eval_fn(flat_params)  # shape: (n2,)
+        scores_list = []
+        # GSM8K
+        scores_list.append(gsm8k_eval_fn(flat_params))
+        # BFCL
+        scores_list.append(bfcl_eval_fn(flat_params))
+        # MBPPï¼ˆå¯é€‰ï¼‰
+        if mbpp_eval_fn is not None:
+            scores_list.append(mbpp_eval_fn(flat_params))
+        # DoTï¼ˆå¯é€‰ï¼‰
+        if mult4_eval_fn is not None:
+            scores_list.append(mult4_eval_fn(flat_params))
+        if mult5_eval_fn is not None:
+            scores_list.append(mult5_eval_fn(flat_params))
+        if bool_eval_fn is not None:
+            scores_list.append(bool_eval_fn(flat_params))
+        return jnp.concatenate(scores_list)
 
-        # æ‹¼æ¥æ‰€æœ‰åˆ†æ•°ï¼ˆä¿æŒper-sampleç²’åº¦ç”¨äºcompetitive normalizationï¼‰
-        all_scores = jnp.concatenate([gsm8k_scores, bfcl_scores])
+    return evaluation_fn
 
-        return all_scores
+def create_dot_eval_fn(
+    model_skeleton,
+    param_shapes,
+    tokenizer,
+    task: str,  # 'mult4' | 'mult5' | 'bool'
+    num_samples: int,
+    batch_size: int = 8,
+    distributed: bool = False,
+    world_size: int = 1,
+    rank: int = 0,
+):
+    """åœ¨çº¿ç”ŸæˆDoTé£æ ¼ä»»åŠ¡å¹¶è¯„ä¼°ï¼ˆpass@1ï¼‰ã€‚"""
+    import torch
+    device = next(model_skeleton.parameters()).device
+
+    # ç”Ÿæˆæ•°æ®
+    if task == 'mult4':
+        dataset = generate_mult_dataset(num_samples=num_samples, digits=4, seed=2025)
+        parse_fn = parse_int_from_text
+    elif task == 'mult5':
+        dataset = generate_mult_dataset(num_samples=num_samples, digits=5, seed=2025)
+        parse_fn = parse_int_from_text
+    elif task == 'bool':
+        dataset = generate_bool_dataset(num_samples=num_samples, seed=2025)
+        parse_fn = parse_bool_from_text
+    else:
+        raise ValueError(f"Unknown DoT task: {task}")
+
+    prompts = [item['prompt'] for item in dataset]
+    golds = [item['gold'] for item in dataset]
+
+    def evaluation_fn(flat_params: jnp.ndarray) -> jnp.ndarray:
+        base_model = (
+            model_skeleton.module if hasattr(model_skeleton, "module") else model_skeleton
+        )
+        restored_model = jax_flattened_to_pytorch_model(
+            flat_params, base_model, param_shapes
+        )
+        restored_model.eval()
+
+        all_scores: list[float] = []
+        # åˆ†æ‰¹tokenize+ç”Ÿæˆ
+        for start in range(0, len(prompts), batch_size):
+            batch_prompts = prompts[start:start+batch_size]
+            batch_golds = golds[start:start+batch_size]
+
+            enc = dot_collate(batch_prompts, tokenizer, max_length=256)
+            input_ids = enc['input_ids'].to(device)
+            attention_mask = enc['attention_mask'].to(device)
+
+            with torch.no_grad():
+                gen_ids = restored_model.generate(
+                    input_ids=input_ids,
+                    attention_mask=attention_mask,
+                    max_new_tokens=64,
+                    do_sample=False,
+                    temperature=0.2,
+                    pad_token_id=tokenizer.pad_token_id,
+                    eos_token_id=tokenizer.eos_token_id,
+                )
+                gen_txts = tokenizer.batch_decode(
+                    gen_ids[:, input_ids.shape[1]:],
+                    skip_special_tokens=True
+                )
+
+            for txt, gold in zip(gen_txts, batch_golds):
+                if task in ('mult4', 'mult5'):
+                    pred = parse_fn(txt)
+                    all_scores.append(1.0 if (pred is not None and pred == gold) else 0.0)
+                else:
+                    pred = parse_fn(txt)
+                    all_scores.append(1.0 if (pred is not None and pred == gold) else 0.0)
+
+        return jnp.array(all_scores, dtype=jnp.float32)
 
     return evaluation_fn
diff --git a/run_bfcl_dynamic_sparsity.sh b/run_bfcl_dynamic_sparsity.sh
index 9a54720..e3aa1fa 100755
--- a/run_bfcl_dynamic_sparsity.sh
+++ b/run_bfcl_dynamic_sparsity.sh
@@ -100,3 +100,4 @@ echo "âœ… å®éªŒå®Œæˆï¼"
 echo "ç»“æœä¿å­˜åœ¨: ${OUTPUT_DIR}"
 echo "=========================================="
 
+
diff --git a/scripts/experiments/run_mbpp_quick_test.sh b/scripts/experiments/run_mbpp_quick_test.sh
new file mode 100644
index 0000000..269844c
--- /dev/null
+++ b/scripts/experiments/run_mbpp_quick_test.sh
@@ -0,0 +1,103 @@
+#!/usr/bin/env bash
+set -euo pipefail
+
+# ============================================================================
+# ğŸ§ª MBPP Quick Test - å¿«é€ŸéªŒè¯MBPPé›†æˆ
+# ============================================================================
+
+echo "========================================"
+echo "ğŸ§ª MBPP Quick Test - Fast Validation"
+echo "========================================"
+
+# === æ¨¡å‹é…ç½® ===
+MODEL1_PATH="${MODEL1_PATH:-models/Qwen2.5-0.5B-Instruct}"
+MODEL2_PATH="${MODEL2_PATH:-models/Qwen2.5-0.5B-Instruct}"
+
+# === å®éªŒå‚æ•° ===
+POP_SIZE=4
+TOTAL_FORWARD_PASSES=50      # å¿«é€Ÿæµ‹è¯•ï¼š50æ­¥
+RUNS=1
+EVAL_SUBSET_SIZE=5          # æ¯ä¸ªä»»åŠ¡åªè¯„ä¼°5ä¸ªæ ·æœ¬
+
+# === ç¨€ç–åº¦å‚æ•° ===
+OMEGA=0.7
+BETA=0.3
+PRUNING_SPARSITY=0.2
+PRUNING_METHOD="wanda"
+
+# === å¤šä»»åŠ¡æƒé‡ ===
+GSM8K_WEIGHT=0.4
+BFCL_WEIGHT=0.3
+MBPP_WEIGHT=0.3
+
+# === æ•°æ®è·¯å¾„ ===
+BFCL_DATA_PATH="${BFCL_DATA_PATH:-bfcl/data/bfcl_test_200.json}"
+MBPP_DATA_PATH="${MBPP_DATA_PATH:-mbpp/data/mbpp_test_sample.json}"
+
+# === è¾“å‡ºç›®å½• ===
+OUTPUT_DIR="${OUTPUT_DIR:-results/mbpp_quick_test}"
+
+echo ""
+echo "é…ç½®å‚æ•°ï¼š"
+echo "  æ¨¡å‹: $MODEL1_PATH"
+echo "  ç§ç¾¤å¤§å°: $POP_SIZE"
+echo "  è¿­ä»£æ¬¡æ•°: $TOTAL_FORWARD_PASSES"
+echo "  è¯„ä¼°å­é›†: $EVAL_SUBSET_SIZE æ ·æœ¬/ä»»åŠ¡"
+echo ""
+echo "å¤šä»»åŠ¡æƒé‡ï¼š"
+echo "  GSM8K: $GSM8K_WEIGHT"
+echo "  BFCL: $BFCL_WEIGHT"
+echo "  MBPP: $MBPP_WEIGHT"
+echo ""
+
+# æ£€æŸ¥æ•°æ®æ–‡ä»¶
+if [[ ! -f "$BFCL_DATA_PATH" ]]; then
+  echo "âŒ BFCLæ•°æ®ä¸å­˜åœ¨: $BFCL_DATA_PATH" >&2
+  exit 1
+fi
+
+if [[ ! -f "$MBPP_DATA_PATH" ]]; then
+  echo "âŒ MBPPæ•°æ®ä¸å­˜åœ¨: $MBPP_DATA_PATH" >&2
+  echo "æç¤º: ä½¿ç”¨ç¤ºä¾‹æ•°æ® mbpp/data/mbpp_test_sample.json" >&2
+  exit 1
+fi
+
+echo "âœ… æ•°æ®æ£€æŸ¥é€šè¿‡"
+echo ""
+echo "ğŸš€ å¼€å§‹MBPPä¸‰ä»»åŠ¡å¿«é€Ÿæµ‹è¯•..."
+echo ""
+
+# è¿è¡Œå®éªŒ
+python3 main_sparsity_aware.py \
+  --runs $RUNS \
+  --model1_path "$MODEL1_PATH" \
+  --model2_path "$MODEL2_PATH" \
+  --pop_size $POP_SIZE \
+  --total_forward_passes $TOTAL_FORWARD_PASSES \
+  --omega $OMEGA \
+  --beta $BETA \
+  --pruning_sparsity $PRUNING_SPARSITY \
+  --pruning_method $PRUNING_METHOD \
+  --eval_subset_size $EVAL_SUBSET_SIZE \
+  --use_bfcl_eval \
+  --bfcl_data_path "$BFCL_DATA_PATH" \
+  --gsm8k_weight $GSM8K_WEIGHT \
+  --bfcl_weight $BFCL_WEIGHT \
+  --use_mbpp_eval \
+  --mbpp_data_path "$MBPP_DATA_PATH" \
+  --mbpp_weight $MBPP_WEIGHT \
+  --output_dir "$OUTPUT_DIR" \
+  --log_sparsity_stats
+
+echo ""
+echo "========================================"
+echo "âœ… MBPPå¿«é€Ÿæµ‹è¯•å®Œæˆï¼"
+echo "========================================"
+echo ""
+echo "ç»“æœä¿å­˜åœ¨: $OUTPUT_DIR"
+echo ""
+echo "ä¸‹ä¸€æ­¥ï¼š"
+echo "  1. æŸ¥çœ‹ç»“æœ: python tools/analyze_results.py $OUTPUT_DIR/*.pkl --no-plot"
+echo "  2. å®Œæ•´å®éªŒ: bash scripts/experiments/run_mbpp_full_exp.sh"
+echo ""
+
diff --git a/tools/convert_mbpp_to_simple.py b/tools/convert_mbpp_to_simple.py
new file mode 100644
index 0000000..9b05dfd
--- /dev/null
+++ b/tools/convert_mbpp_to_simple.py
@@ -0,0 +1,86 @@
+#!/usr/bin/env python3
+"""
+è½¬æ¢MBPPå®˜æ–¹æ•°æ®åˆ°ç®€åŒ–æ ¼å¼
+
+å®˜æ–¹MBPPæ•°æ®é€šå¸¸æ ¼å¼å¦‚ä¸‹ï¼š
+{
+    "task_id": int,
+    "text": str,  # é—®é¢˜æè¿°
+    "code": str,  # å‚è€ƒå®ç°
+    "test_list": [str],  # æµ‹è¯•ç”¨ä¾‹
+    "test_setup_code": str,  # å¯é€‰çš„å‰ç½®ä»£ç 
+    "challenge_test_list": [str]  # å¯é€‰çš„é¢å¤–æµ‹è¯•
+}
+
+ä½¿ç”¨æ–¹æ³•:
+    python tools/convert_mbpp_to_simple.py \
+        --input mbpp_original.jsonl \
+        --output mbpp/data/mbpp_test.json \
+        --limit 100
+"""
+
+import json
+import argparse
+from pathlib import Path
+
+
+def convert_mbpp_data(input_file: str, output_file: str, limit: int = None):
+    """è½¬æ¢MBPPæ•°æ®åˆ°ç®€åŒ–æ ¼å¼"""
+    
+    print(f"è¯»å–MBPPæ•°æ®: {input_file}")
+    
+    # è¯»å–æ•°æ®
+    with open(input_file, 'r', encoding='utf-8') as f:
+        if input_file.endswith('.jsonl'):
+            data = [json.loads(line) for line in f if line.strip()]
+        else:
+            data = json.load(f)
+    
+    print(f"åŸå§‹æ•°æ®: {len(data)} æ¡")
+    
+    # é™åˆ¶æ•°é‡
+    if limit and limit < len(data):
+        data = data[:limit]
+        print(f"é™åˆ¶åˆ°: {limit} æ¡")
+    
+    # è½¬æ¢æ ¼å¼ï¼ˆä¿æŒå­—æ®µä¸å˜ï¼Œç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨ï¼‰
+    converted = []
+    for item in data:
+        converted_item = {
+            "task_id": item.get("task_id", item.get("id", len(converted))),
+            "text": item.get("text", item.get("prompt", "")),
+            "code": item.get("code", item.get("solution", "")),
+            "test_list": item.get("test_list", item.get("tests", [])),
+            "test_setup_code": item.get("test_setup_code", ""),
+            "challenge_test_list": item.get("challenge_test_list", [])
+        }
+        converted.append(converted_item)
+    
+    # ä¿å­˜
+    output_path = Path(output_file)
+    output_path.parent.mkdir(parents=True, exist_ok=True)
+    
+    with open(output_file, 'w', encoding='utf-8') as f:
+        json.dump(converted, f, ensure_ascii=False, indent=2)
+    
+    print(f"âœ… å·²ä¿å­˜åˆ°: {output_file}")
+    print(f"   åŒ…å« {len(converted)} æ¡æ•°æ®")
+
+
+def main():
+    parser = argparse.ArgumentParser(description='è½¬æ¢MBPPæ•°æ®åˆ°ç®€åŒ–æ ¼å¼')
+    parser.add_argument('--input', type=str, required=True,
+                       help='è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆJSONLæˆ–JSONï¼‰')
+    parser.add_argument('--output', type=str, default='mbpp/data/mbpp_test.json',
+                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
+    parser.add_argument('--limit', type=int, default=None,
+                       help='é™åˆ¶æ•°æ®æ¡æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰')
+    
+    args = parser.parse_args()
+    
+    convert_mbpp_data(args.input, args.output, args.limit)
+
+
+if __name__ == "__main__":
+    main()
+
diff --git a/tools/dot/evaluation_batch.py b/tools/dot/evaluation_batch.py
new file mode 100644
index 0000000..5594bc2
--- /dev/null
+++ b/tools/dot/evaluation_batch.py
@@ -0,0 +1,458 @@
+from collections import defaultdict
+import fire
+import mup
+import lib.datasets
+from lib.datasets import get_dataloaders
+import lib.models
+import lib.utils
+import os
+import torch
+import logging, sys
+import time
+import random
+import numpy as np
+from lib.dpm_solver_pytorch import NoiseSchedulePlaid, model_wrapper, DPM_Solver, ModelWrapper
+
+
+def extract_gsm8k_answer(text):
+    text = text.split(lib.datasets.EOS_TOKEN)[0]
+    split_pattern = '####'
+    if split_pattern not in text: # answer only
+        return text.split(lib.datasets.SEP_TOKEN)[-1].strip().replace(',', '')
+    else:
+        _, ans = text.strip().split(split_pattern, 1)
+        ans = ans.replace(lib.datasets.SEP_TOKEN, '').strip().replace(',', '')
+        return ans
+
+def extract_5by5_answer(text):
+    text = text.split(lib.datasets.EOS_TOKEN)[0].split(lib.datasets.SEP_TOKEN)[-1]
+    if '####' in text: # gold 
+        return text.strip().split('####')[-1].strip()
+    else: # predicted 
+        return text.strip().split('=')[-1].strip(" +")
+
+def extract_4by4_answer(text):
+    text = text.split(lib.datasets.EOS_TOKEN)[0].split(lib.datasets.SEP_TOKEN)[-1]
+    if '####' in text: # gold 
+        return text.strip().split('####')[-1].strip()
+    else: # predicted 
+        return text.strip().split('=')[-1].strip(" +")
+
+def shift_sep_to_pad(tensor, sep_idx, pad_idx):
+    '''Used in MP-dot, shift the sep token to the rightside of the newly generated thought'''
+    new_tensor = []
+    seq_len = tensor.shape[1]
+    new_mask = tensor.new_zeros(tensor.shape, dtype=bool)
+    for i, b in enumerate(tensor.tolist()):
+        try:  # sometimes a thought is long thus no pad is predicted
+            pad_token_idx = b.index(pad_idx)
+            b = b[:pad_token_idx]
+        except:
+            pass 
+        b.remove(sep_idx)  # sep should always exists if the model learns to copy it
+        b.append(sep_idx)
+        new_tensor.append(torch.tensor(b, dtype=torch.int64))
+        new_mask[i][:len(b)] = True
+    dummy_seq = torch.tensor([0]*seq_len, dtype=torch.int64)  # add a dummy seq with length=seq_len
+    new_tensor = torch.nn.utils.rnn.pad_sequence([dummy_seq]+new_tensor, batch_first=True, padding_value=pad_idx)
+    new_tensor = new_tensor[1:]  # drop the dummy sequence
+    new_tensor = new_tensor.type_as(tensor)
+    return new_tensor, new_mask
+
+
+def ids_to_txts(tokenizer, x_samples):
+    return [tokenizer.decode(x.tolist() if isinstance(x, torch.Tensor) else x, skip_special_tokens=False) 
+            for x in x_samples]
+
+
+def generate_samples(x, src_mask, modules, args, timesteps_togo=None):
+    '''We go args.sampling_timesteps steps for all inputs if timesteps_togo is None'''
+    with torch.no_grad():
+        embedding_matrix = modules['embedding_matrix']()
+        x_embed = embedding_matrix[x] # batch,seq_len, dim
+
+        if args.dpm_solver:
+            noise = torch.randn(x_embed.shape, device=x_embed.device)
+            x_noised = torch.where(src_mask[...,None], x_embed, noise)
+            
+            ## init a model_fn such that self_cond is reinitialized
+            ## Convert your discrete-time `model` to the continuous-time
+            ## noise prediction model. Here is an example for a diffusion model
+            ## `model` with the noise prediction type ("noise") 
+            model_kwargs = {'x_selfcond':  torch.zeros_like(x_embed).float(),
+                            'x_embed': x_embed,
+                            'src_mask': src_mask,
+                            'logits': None,
+                            'score_temp': 1,
+                            'cur_t_count': 0,
+                            'total_t_count': args.sampling_timesteps
+                            }
+            model_fn = model_wrapper(
+                ModelWrapper(modules),
+                args.noise_schedule,
+                model_type="x_start",  # or "x_start" or "v" or "score"
+                model_kwargs=model_kwargs,
+                guidance_type="uncond",
+            )
+
+            ## Define dpm-solver and sample by multistep DPM-Solver.
+            ## (We recommend multistep DPM-Solver for conditional sampling)
+            ## You can adjust the `steps` to balance the computation
+            ## costs and the sample quality.
+            dpm_solver = DPM_Solver(model_fn, args.noise_schedule, algorithm_type="dpmsolver++")
+
+            x_sample = dpm_solver.sample(
+                x_noised,
+                steps=args.sampling_timesteps,
+                order=1,  # or 2
+                skip_type="time_uniform",
+                method="multistep",
+                input_ids_mask=~src_mask[...,None],
+                x_start=x_embed,
+            )
+            logits = model_kwargs['logits']
+
+        else:
+            gamma_0, gamma_1 = modules['gamma_bounds']()
+
+            z = torch.randn(x_embed.shape, device='cuda') * args.initial_noise_scale
+            x_selfcond = torch.zeros_like(z).float()
+
+            unfinished = x.new_ones(x_embed.shape[0], dtype=bool)
+            end = False
+            logits = None
+            for i, t in enumerate(torch.linspace(1., 0., args.sampling_timesteps)):
+                t = t[None].cuda()
+                s = t - 1. / args.sampling_timesteps
+                gamma_s = modules['noise_schedule'](s).double()
+                gamma_t = modules['noise_schedule'](t).double()
+                gamma_s = gamma_0 + (gamma_1 - gamma_0) * gamma_s
+                gamma_t = gamma_0 + (gamma_1 - gamma_0) * gamma_t
+                alpha_squared_s = torch.sigmoid(-gamma_s)
+                alpha_squared_t = torch.sigmoid(-gamma_t)
+                alpha_s = alpha_squared_s.sqrt()
+                alpha_t = alpha_squared_t.sqrt()
+                sigma_squared_s = torch.sigmoid(gamma_s)
+                sigma_squared_t = torch.sigmoid(gamma_t)
+                sigma_s = sigma_squared_s.sqrt()
+                sigma_t = sigma_squared_t.sqrt()
+
+                logits_partial, x_reconst = modules['model'](
+                    z=z[unfinished].to(torch.float32, copy=True),
+                    gamma=gamma_t.float(),
+                    embedding_matrix=embedding_matrix,
+                    bias_scale=1.,
+                    x_selfcond=x_selfcond[unfinished],
+                    x_embed=x_embed[unfinished] if args.fix_src else None,
+                    src_mask=src_mask[unfinished] if args.fix_src else None
+                )
+                if logits is None:
+                    logits = logits_partial
+                else:
+                    logits[unfinished] = logits_partial
+
+                x_selfcond[unfinished] = x_reconst.clone().detach()
+                x_reconst = x_reconst.double()
+                epsilon_pred = (z[unfinished] - (alpha_t * x_reconst)) / sigma_t
+                epsilon_pred /= args.score_temp
+                x_reconst = (z[unfinished] - (sigma_t * epsilon_pred)) / alpha_t
+                    
+                if t > 0:
+                    # App A.4, p(z_s|z_t), NN gives x_reconst based on z_t, then reparam. x_reconst to get z_s
+                    c = -torch.expm1(gamma_s - gamma_t)
+                    z[unfinished] *= (1 - c) * alpha_squared_s.sqrt() / alpha_squared_t.sqrt()
+                    z[unfinished] += c * (alpha_squared_s.sqrt() * x_reconst.double())
+                    z[unfinished] += (c * (1 - alpha_squared_s)).sqrt() * torch.randn_like(z[unfinished])
+
+                if timesteps_togo is not None:
+                    for j, _ in enumerate(x):
+                        if unfinished[j] and i+1 == timesteps_togo[j]: # i -> i+1
+                            unfinished[j] = False
+                            if all(~unfinished):
+                                end = True
+                    if end: 
+                        break
+
+            logits, _ = modules['model'](
+                z=z.float(),
+                gamma=gamma_t.float(),
+                embedding_matrix=embedding_matrix,
+                bias_scale=1.,
+                x_selfcond=x_selfcond,
+                x_embed=x_embed if args.fix_src else None,
+                src_mask=src_mask if args.fix_src else None
+            )
+
+        if args.logit_sample and args.logit_temp > 0:
+            logits = logits / args.logit_temp
+            _reshaped_logits = logits.reshape(-1, logits.shape[-1])
+            _reshapedx_samples = torch.multinomial(_reshaped_logits.softmax(dim=-1), num_samples=1).squeeze(-1)
+            x_samples = _reshapedx_samples.reshape(logits.shape[:-1])
+        else:
+            x_samples = logits.argmax(dim=-1)
+        
+        if args.fix_src:
+            x_samples = torch.where(src_mask, x, x_samples)
+
+        return x_samples
+
+def generate_cot_samples(x, src_mask, modules, args):
+    batch_size = x.shape[0]
+    unfinished = x.new_ones(batch_size, dtype=bool)
+    end = False
+    for _ in range(args.cot_steps):
+        x[unfinished] = generate_samples(x[unfinished], src_mask[unfinished], modules, args)
+
+        # res_txts = ids_to_txts(x[unfinished])
+        # for res_txt in res_txts:
+        #     res_txt = res_txt.replace(lib.datasets.SEP_TOKEN, "").replace(lib.datasets.PAD_TOKEN, "")
+        #     logging.info(res_txt)
+
+        for i, item in enumerate(x):
+            if unfinished[i] and lib.datasets.EOS_TOKEN_ID in item: 
+                unfinished[i] = False
+                if all(~unfinished):
+                    end = True
+        if end: 
+            break
+        
+        # for unfinished x, remove sep, add sep at the first pad position   
+        x[unfinished], src_mask[unfinished] = shift_sep_to_pad(x[unfinished], sep_idx=lib.datasets.SEP_TOKEN_ID, pad_idx=lib.datasets.PAD_TOKEN_ID)
+
+    return x
+
+
+def set_seed(seed: int):
+    random.seed(seed)
+    np.random.seed(seed)
+    torch.manual_seed(seed)
+    torch.cuda.manual_seed_all(seed)
+
+
+def vote(pred_list):
+    counts = {}
+    for pred in pred_list:
+        counts[pred] = counts.get(pred, 0) + 1
+    count_sorted = sorted(counts.items(), key=lambda x: x[1])
+    return count_sorted[-1][0]
+
+
+def evaluate(
+        args, 
+        test_loader, 
+        tokenizer, 
+        modules, 
+        log_interval=False,
+        runs=1,
+        apply_sc=False,
+    ):
+    results = []
+    print(f"total instances: {len(test_loader.dataset)}")
+    for run in range(runs):
+        logging.info(f"evaluating {args.dataset} at Run {run}...")
+        set_seed(2024+run)
+        start_time = time.time()
+        local_corr = 0
+        local_total = 0
+        local_result = []
+        for i, batch in enumerate(test_loader):
+            x, src_mask, tgt_texts, task_ids = batch
+            x = x.cuda()
+            src_mask = src_mask.cuda()
+            task_ids = task_ids.tolist()
+
+            if args.cot:
+                res_ids = generate_cot_samples(x, src_mask, modules, args)
+            else:
+                res_ids = generate_samples(x, src_mask, modules, args)
+
+            res_txts = ids_to_txts(tokenizer, res_ids)
+
+            for res_txt, tgt_text, task_id in zip(res_txts, tgt_texts, task_ids):
+                if log_interval:
+                    # ori_item = test_loader.dataset.dataset[i*args.batch_size*lib.ddp.world_size()+j*lib.ddp.world_size()+lib.ddp.rank()]
+                    log_txt = res_txt.replace(lib.datasets.SEP_TOKEN, "").replace(lib.datasets.PAD_TOKEN, "")
+                    logging.info(log_txt)
+            
+                if args.dataset in ['gsm8k', '5by5', '4by4']:
+                    pred = eval(f"extract_{args.dataset}_answer")(res_txt)
+                    gold = eval(f"extract_{args.dataset}_answer")(tgt_text)
+                    local_result.append(
+                        {
+                            "task_id": int(task_id),
+                            "pred": pred,
+                            "gold": gold,
+                        }
+                    )
+                    local_corr += pred == gold
+                    local_total += 1
+                    if log_interval:
+                        logging.info(f"pred:{pred}; gold:{gold}; local idx/corr/acc: {local_total}/{local_corr}/{local_corr/local_total}")
+                
+            if args.limit and i == 5:
+                break
+
+        if apply_sc:
+            # a list of list of dicts
+            global_result = lib.ddp.gather_list(local_result)
+            # convert to a list of dicts
+            global_result = [item for sublist in global_result for item in sublist]
+            results.append(global_result)
+        else:
+            corr = lib.ddp.reduce_sum(local_corr).item()
+            total = lib.ddp.reduce_sum(local_total).item()
+
+            acc = corr/total
+            logging.info(f"total: {total}, corr: {corr}, acc: {acc}")
+            logging.info(f"time: {time.time()-start_time}s")
+            results.append(acc)
+
+    if apply_sc:
+        # results is a list of list of dicts
+        # convert to a dict of dicts grouped by task_id
+        # the outer dict has keys: task_id
+        results_dict = defaultdict(dict)
+        for res in results:
+            for item in res:
+                results_dict[item["task_id"]]["preds"] = results_dict[item["task_id"]].get("preds", []) + [item["pred"]]
+                if "gold" not in results_dict[item["task_id"]]:
+                    results_dict[item["task_id"]]["gold"] = item["gold"]
+                else:
+                    assert results_dict[item["task_id"]]["gold"] == item["gold"]
+        # convert to a list of dicts with keys: preds, gold
+        results_list = []
+        for task_id in results_dict:
+            results_list.append(results_dict[task_id])
+        
+        total = len(results_list)
+        for vote_at_k in range(1, args.runs+1):
+            corr = 0
+            for res in results_list:
+                pred = vote(res["preds"][:vote_at_k])
+                gold = res["gold"]
+                if pred == gold:
+                    corr += 1
+                acc = corr/total
+            logging.info(f"[[Self-consistency @ {vote_at_k}]]: {total}, corr: {corr}, acc: {acc}")
+        return acc
+    else:
+        # Calculate mean and std
+        mean = np.mean(results)
+        std = np.std(results)
+        logging.info(f"Mean: {mean}, Std: {std}")
+        return mean
+
+
+def main(**args):
+    torch.backends.cuda.matmul.allow_tf32 = True
+    torch.backends.cudnn.allow_tf32 = True
+
+    args = lib.utils.AttributeDict(args)
+    args.setdefault('dataset', 'openwebtext')
+    args.setdefault('seq_len', 256)
+    args.setdefault('vocab_size', 32768)
+    args.setdefault('weights_path', "plaid1b_weights")
+    args.setdefault('dim', 2048)
+    args.setdefault('n_blocks', 24)
+    args.setdefault('n_heads', 32)
+    args.setdefault('gamma_0', -3.)
+    args.setdefault('gamma_1', 6.)
+    args.setdefault('embed_dim', 16)
+    args.setdefault('initial_noise_scale', 1.0)
+    args.setdefault('batch_size', 168)
+    args.setdefault('sampling_timesteps', 64)
+    args.setdefault('dpm_solver', False)
+    args.setdefault('score_temp', 0.5)
+    # add logit sampling procedures
+    args.setdefault('logit_sample', False)
+    args.setdefault('logit_temp', 0.5)
+    args.setdefault('runs', 1)
+    args.setdefault('apply_sc', False)
+    args.setdefault('fix_src', False)
+    args.setdefault('cot', False) # thought-level diffusion, q+previous cot -> next thought
+    args.setdefault('cot_steps', 12) # 
+    args.setdefault('digit', False) # 
+    args.setdefault('limit', False) # limit 5 instances
+    
+    eval_log_name = f"eval-{args.sampling_timesteps}-score_{args.score_temp}"
+    if args.apply_sc:
+        eval_log_name += f'-sc'
+    if args.dpm_solver:
+        eval_log_name += '-dpmsolver'
+    if args.logit_sample:
+        eval_log_name += f'-logit-{args.logit_temp}'
+
+    args.eval_log = os.path.join(args.weights_path, f"{eval_log_name}.log")
+    if lib.ddp.rank() == 0:
+        if os.path.exists(args.eval_log): 
+            os.remove(args.eval_log)
+
+    targets = logging.StreamHandler(sys.stdout), logging.FileHandler(args.eval_log, mode='w')
+    logging.basicConfig(format='[%(asctime)s] %(message)s', level=logging.INFO, handlers=targets)
+
+    lib.utils.print_args(args)
+
+    torch.backends.cuda.matmul.allow_tf32 = True
+    torch.backends.cudnn.allow_tf32 = True
+    # torch.set_default_device('cuda')
+
+    # Lots of annoying big/small numbers throughout this code, so we'll do
+    # everything in fp64 by default and explicitly switch to fp32/bf16 where
+    # appropriate.
+    torch.set_default_dtype(torch.float64)
+
+    def log1mexp(x):
+        # Computes log(1-exp(-|x|))
+        x = -x.abs()
+        return torch.where(
+            x > -0.693,
+            torch.log(-torch.expm1(x)),
+            torch.log1p(-torch.exp(x))
+        )
+
+    def create_modules(dim, n_heads):
+        return {
+            'noise_schedule': lib.models.NoiseSchedule().float(),
+            'gamma_bounds': lib.models.GammaBounds(args.gamma_0, args.gamma_1).float(),
+            'embedding_matrix': lib.models.EmbeddingMatrix(args.vocab_size, args.embed_dim).float(),
+            'model': lib.models.DiffusionModel(dim, args.embed_dim, args.n_blocks, n_heads, args.vocab_size).float()
+        }
+    modules = create_modules(args.dim, args.n_heads)
+    base_modules = create_modules(256, 4)
+    delta_modules = create_modules(128, 2)
+    for key in modules:
+        main, base, delta = modules[key], base_modules[key], delta_modules[key]
+        mup.set_base_shapes(main, base, delta=delta)
+        main.cuda()
+
+    logging.info(f'Loading weights from {args.weights_path}')
+    for name, module in modules.items():
+        module.load_state_dict(torch.load(
+            os.path.join(args.weights_path, f'{name}.pt'),
+            map_location=torch.device('cuda')
+        ))
+
+    for key in modules:
+        logging.info(key+':')
+        lib.utils.print_model(modules[key])
+
+    (test_loader,), (word2idx, idx2word), tokenizer = get_dataloaders(
+        args.dataset, args.batch_size, args.seq_len, args.cot, args.digit, only_test=True
+    )
+
+    if args.dpm_solver:
+        args.noise_schedule = NoiseSchedulePlaid(modules['noise_schedule'])
+
+    evaluate(
+        args, 
+        test_loader, 
+        tokenizer, 
+        modules, 
+        log_interval=True, 
+        runs=args.runs,
+        apply_sc=args.apply_sc
+    )
+
+
+if __name__ == '__main__':
+    fire.Fire(lib.ddp.wrap_main(main))
\ No newline at end of file
diff --git a/tools/dot/lib/datasets.py b/tools/dot/lib/datasets.py
new file mode 100644
index 0000000..6212043
--- /dev/null
+++ b/tools/dot/lib/datasets.py
@@ -0,0 +1,292 @@
+import os
+from typing import Dict
+import torch
+import torch.nn.utils.rnn
+from torch.utils.data import DataLoader, Dataset, SequentialSampler
+from torch.utils.data.distributed import DistributedSampler
+from torch.nn.utils.rnn import pad_sequence
+import datasets
+import random
+from functools import partial
+from tokenizers.implementations import ByteLevelBPETokenizer
+from tokenizers.pre_tokenizers import Digits
+
+EOS_TOKEN = "<|endoftext_R9VQqF0Ag7|>"
+PAD_TOKEN = "ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚ÃƒÃ‚"
+SEP_TOKEN = '----------------------------------------------------------------'
+PRINT_PAD_TOKEN = '[PAD]'
+PRINT_SEP_TOKEN = '[SEP]'
+EOS_TOKEN_ID = 0
+SEP_TOKEN_ID = 32021
+PAD_TOKEN_ID = 25670
+
+class DummyEncoding():
+    def __init__(self, ids):
+        self.ids = ids
+
+class DigitWrapper(ByteLevelBPETokenizer):
+    def __init__(self, tokenizer):
+        self.tokenizer = tokenizer
+        self.digit_tokenizer = Digits(individual_digits=True)
+        self.__dict__.update(self.tokenizer.__dict__.items())
+
+    def encode(self, text, digit=True):
+        if digit:
+            chunks = self.digit_tokenizer.pre_tokenize_str(text)
+            res = self.encode_batch([i[0] for i in chunks], digit=False)
+            ids = []
+            for r in res:
+                ids.extend(r.ids)
+            enc = DummyEncoding(ids)
+            return enc
+        return self.tokenizer(text)
+
+
+    def encode_batch(self, texts, digit=True):
+        if digit:
+            return [self.encode(text, digit=True) for text in texts]
+        return self.tokenizer.encode_batch(texts)
+    
+    def get_vocab(self, with_added_tokens: bool = True) -> Dict[str, int]:
+        return self.tokenizer.get_vocab(with_added_tokens)
+
+    def decode(self, *args, **kwargs):
+        return self.tokenizer.decode(*args, **kwargs)
+    
+
+# str: <<45-40=5>> <<10*1.2=12>> <<12*5=60>> <<10*40=400>> <<400+60=460>> #### 460
+# list: ['<<45-40=5>>', ..., '<<400+60=460>> #### 460']
+def split_gsm8k_target(target):
+    splits = target.split(' ')
+    splits = splits[:-3] + [' '.join(splits[-3:])] 
+    return [' '+i for i in splits]
+
+def get_gsm8k_dataset(split):
+    with open(f'data/gsm8k/{split}.txt', encoding="utf-8") as f:
+        lines = [line.split('||') for line in f.read().splitlines() 
+                if (len(line) > 0 and not line.isspace()
+                                    and len(line.split('||')) ==2 )]
+        src_lines, tgt_lines = list(zip(*lines))
+        src_lines = list(src_lines)
+        tgt_lines = list(tgt_lines)
+        res = datasets.Dataset.from_dict(
+            {
+                'src': src_lines, 
+                'tgt': [split_gsm8k_target(i) for i in tgt_lines],
+                'task_id': [i for i in range(len(src_lines))]
+            }
+        )
+        return res
+
+
+def split_5by5_target(text):
+    rationales, target = text.split('####')
+    rationales = rationales.strip().split('+')
+    target = target.strip()
+    # 25 * 10 = 50 + 200 = 250  => ["50 + ", "200 = ", "250"]
+    cot_sequences = [r.strip()+' + ' for r in rationales[:-1]] + [rationales[-1].strip() + ' = ', target]
+    return cot_sequences
+
+def get_5by5_dataset(split):
+    with open(f'data/5by5/{split}.txt', encoding="utf-8") as f:
+        lines = [line.split('||') for line in f.read().splitlines() 
+                if (len(line) > 0 and not line.isspace()
+                                    and len(line.split('||')) ==2 )]
+        src_lines, tgt_lines = list(zip(*lines))
+        src_lines = list(src_lines)
+        tgt_lines = list(tgt_lines)
+        res = datasets.Dataset.from_dict({'src': [s+' = ' for s in src_lines], 
+                                          'tgt': [split_5by5_target(i) for i in tgt_lines],
+                                          'task_id': [i for i in range(len(src_lines))]})
+        return res
+
+
+def split_4by4_target(text):
+    rationales, target = text.split('####')
+    rationales = rationales.strip().split('+')
+    target = target.strip()
+    # 25 * 10 = 50 + 200 = 250  => ["50 + ", "200 = ", "250"]
+    cot_sequences = [r.strip()+' + ' for r in rationales[:-1]] + [rationales[-1].strip() + ' = ', target]
+    return cot_sequences
+
+def get_4by4_dataset(split):
+    with open(f'data/4by4/{split}.txt', encoding="utf-8") as f:
+        lines = [line.split('||') for line in f.read().splitlines() 
+                if (len(line) > 0 and not line.isspace()
+                                    and len(line.split('||')) ==2 )]
+        src_lines, tgt_lines = list(zip(*lines))
+        src_lines = list(src_lines)
+        tgt_lines = list(tgt_lines)
+        res = datasets.Dataset.from_dict({'src': [s+' = ' for s in src_lines], 
+                                          'tgt': [split_4by4_target(i) for i in tgt_lines],
+                                          'task_id': [i for i in range(len(src_lines))]})
+        return res
+
+
+def _tokenize(items, tokenizer):
+    src_encoding = tokenizer.encode_batch(items['src'])
+    tgt_encoding_list= [tokenizer.encode_batch(i) for i in items['tgt']]
+    return {'src_ids': [i.ids for i in src_encoding], 'tgt_ids_list': [[i.ids for i in tgt_encoding]
+                                                                       for tgt_encoding in tgt_encoding_list]}
+
+class TextDataset(Dataset):
+    def __init__(self, dataset, split, tokenizer):
+        self.dataset = eval(f'get_{dataset}_dataset')(split)
+        self.dataset = self.dataset.map(partial(_tokenize, tokenizer=tokenizer),
+                                        num_proc=4,
+                                        batched=True,
+                                        load_from_cache_file=False,
+                                        desc="Running tokenizer on dataset",)
+        self.tokenizer = tokenizer
+
+    def __len__(self):
+        return len(self.dataset)
+
+    def __getitem__(self, idx):
+        return self.dataset[idx]
+
+
+def length_to_mask(length_list, max_length=None):
+    if max_length is None:
+        max_length = max(length_list)
+    batch_size = len(length_list)  # Assuming length_tensor has shape (batch_size,)
+    mask = torch.arange(max_length).expand(batch_size, -1) < torch.tensor(length_list).unsqueeze(1)
+    return mask
+
+def collate_fn(batch, cot=False, seq_len=None, glance=False):
+    # Sort the batch in descending order of text length
+    batch = sorted(batch, key=lambda x: len(x), reverse=True)
+    texts = []
+    src_lens = []
+    for i_in_batch, b in enumerate(batch):
+        b['tgt_ids_list'][-1].append(EOS_TOKEN_ID)  # add eos after the last thought
+        if cot:
+            i = 0
+            pre_cot = random.randint(0, len(b['tgt_ids_list'])-1)
+            src_ids = b['src_ids']
+            tgt_next = []
+            if pre_cot == 0:
+                tgt_ids = b['tgt_ids_list'][0]
+            else:
+                tgt_ids = []
+                for i, tgt in enumerate(b['tgt_ids_list']):
+                    if i < pre_cot:
+                        src_ids.extend(tgt)
+                    else:
+                        tgt_ids.extend(tgt)
+                        break
+            if i < len(b['tgt_ids_list'])-1:
+                tgt_next.extend(b['tgt_ids_list'][i+1])
+            
+        else:   
+            src_ids = b['src_ids']
+            tgt_ids = []
+            for tgt in b['tgt_ids_list']:
+                tgt_ids.extend(tgt)
+        
+        if seq_len is not None:
+            # keep all tgt, rest for src
+            tgt_ids = tgt_ids[:seq_len]
+            src_ids = src_ids[-(seq_len-len(tgt_ids)):]
+
+        if glance and len(tgt_next)>0 and i_in_batch < 2 and random.random() < 0.1:
+            src_lens.append(len(src_ids))
+            texts.append(torch.tensor(src_ids + tgt_ids + [SEP_TOKEN_ID] + tgt_next, dtype=torch.int64))
+        else:
+            src_lens.append(len(src_ids) + 1)
+            texts.append(torch.tensor(src_ids + [SEP_TOKEN_ID] + tgt_ids, dtype=torch.int64))
+
+    texts_padded = pad_sequence(texts, batch_first=True, padding_value=PAD_TOKEN_ID)
+    attn_mask = length_to_mask([len(text) for text in texts])
+    src_mask = length_to_mask(src_lens, max_length=attn_mask.shape[1])
+    return texts_padded, attn_mask, src_mask
+
+
+def collate_fn_test(batch, seq_len):
+    # Sort the batch in descending order of text length
+    src_list = []
+    src_lens = []
+    tgt_texts = []
+    task_ids = []
+
+    for b in batch:
+        src_ids = b['src_ids']
+        
+        # src_ids = src_ids[:seq_len-2] # if src is too long, cut it; one for sep, one for tgt prediction
+        if len(src_ids) >= seq_len:
+            raise ValueError(f'seq_len={seq_len} is too short, one src has length {len(src_ids)}')
+
+        src_lens.append(len(src_ids) + 1)
+        src_list.append(torch.tensor(src_ids + [SEP_TOKEN_ID], dtype=torch.int64))
+        tgt_texts.append(b['tgt'][-1])
+
+        task_ids.append(b['task_id'])
+
+    dummy_seq = torch.tensor([0]*seq_len, dtype=torch.int64)  # add a dummy seq with length=seq_len
+    texts_padded = pad_sequence([dummy_seq]+src_list, batch_first=True, padding_value=PAD_TOKEN_ID)
+    texts_padded = texts_padded[1:]  # drop the dummy sequence
+    src_mask = length_to_mask(src_lens, max_length=seq_len)
+    task_ids = torch.tensor(task_ids, dtype=torch.int64)
+
+    return texts_padded, src_mask, tgt_texts, task_ids
+
+
+def infinite_loader(data_loader):
+    while True:
+        yield from data_loader
+    
+def get_tokenizer(digit=False):
+    tokenizer_path = os.path.join('misc/owt2_tokenizer.json')
+    from tokenizers import Tokenizer
+    tokenizer = Tokenizer.from_file(tokenizer_path)
+    return DigitWrapper(tokenizer) if digit else tokenizer
+
+def get_dataloader(dataset, split, batch_size, tokenizer, seq_len, cot, glance=False):
+    dataset = TextDataset(dataset, split, tokenizer)
+    if split != 'test':
+        sampler = DistributedSampler(dataset)  
+        data_loader = DataLoader(
+            dataset,
+            batch_size=batch_size,
+            sampler=sampler,
+            collate_fn=partial(collate_fn, cot=cot, seq_len=seq_len, glance=glance)
+        )
+    else:
+        sampler = DistributedSampler(dataset, shuffle=False)
+        data_loader = DataLoader(
+            dataset,
+            batch_size=batch_size,
+            sampler=sampler,
+            collate_fn=partial(collate_fn_test, seq_len=seq_len)
+        )
+    return data_loader
+
+
+def get_dataloaders(dataset, batch_size, seq_len, cot, digit, glance, only_test=False):
+    if seq_len is None:
+        seq_len = 1024
+
+    tokenizer = get_tokenizer(digit)
+    word2idx = {k.encode('utf-8'):v for k,v in tokenizer.get_vocab().items()}
+    idx2word = {v:k for k,v in word2idx.items()}
+    if only_test:
+        test_loader = get_dataloader(dataset, 'test', batch_size, tokenizer, seq_len, cot)
+        return (test_loader,), (word2idx, idx2word), tokenizer
+    else:
+        train_loader = get_dataloader(dataset, 'train', batch_size, tokenizer, seq_len, cot, glance)
+        valid_loader = get_dataloader(dataset, 'valid', batch_size, tokenizer, seq_len, cot)
+        return (train_loader, valid_loader), (word2idx, idx2word), tokenizer
+
+
+if __name__ == '__main__':
+    digit_tokenizer = get_tokenizer(digit=True)
+    texts = ["This is a text", "This is 1+11=12"]
+    # [1116, 321, 258, 3241]
+    # [1116, 321, 221, 17, 11, 17, 17, 29, 17, 18]
+    [print(r.ids) for r in digit_tokenizer.encode_batch(texts)]
+    
+    # [1116, 321, 258, 3241]
+    # [1116, 321, 406, 11, 1970, 29, 2100]
+    [print(r.ids) for r in digit_tokenizer.tokenizer.encode_batch(texts)]
+    print(digit_tokenizer.tokenizer.get_vocab_size())
+    print(PAD_TOKEN_ID, SEP_TOKEN_ID)
\ No newline at end of file
diff --git a/tools/dot/lib/ddp.py b/tools/dot/lib/ddp.py
new file mode 100644
index 0000000..5541c0b
--- /dev/null
+++ b/tools/dot/lib/ddp.py
@@ -0,0 +1,118 @@
+import sys
+import os
+import numpy as np
+import torch
+import torch.distributed as dist
+import torch.multiprocessing as mp
+import random
+from collections import defaultdict
+
+def _worker_fn(rank, world_size, main_fn, args_dict):
+    # Setup
+    torch.cuda.set_device(rank)
+    dist.init_process_group(backend='nccl', rank=rank,
+        world_size=world_size)
+    if rank != 0:
+        sys.stdout = open('/dev/null', 'w')
+
+    # Main function
+    main_fn(**args_dict)
+
+    # Cleanup
+    dist.destroy_process_group()
+
+
+def _torchrun_worker_fn(main_fn, args_dict):
+    # Setup
+    local_rank = int(os.environ['LOCAL_RANK'])
+    torch.cuda.set_device(local_rank)
+    dist.init_process_group(backend='nccl')
+    print(f'Rank: {rank()}/{world_size()} (local rank {local_rank})')
+    if local_rank != 0:
+        sys.stdout = open('/dev/null', 'w')
+
+    # Main function
+    main_fn(**args_dict)
+
+    # Cleanup
+    dist.destroy_process_group()
+
+
+def wrap_main(main_fn):
+    """
+    Usage: instead of calling main() directly, call wrap_main(main)().
+    main should take only kwargs.
+    """
+    world_size = torch.cuda.device_count()
+    def main(**args):
+        if 'RANK' in os.environ:
+            mp.set_start_method('spawn')
+            _torchrun_worker_fn(main_fn, args)
+        else:
+            os.environ['PYTHONUNBUFFERED'] = '1'
+            os.environ['MASTER_ADDR'] = 'localhost'
+            os.environ['MASTER_PORT'] = str(random.randint(1024, 65536))
+            mp.set_start_method('spawn')
+            if world_size == 1:
+                _worker_fn(0, world_size, main_fn, args)
+            else:
+                mp.spawn(
+                    _worker_fn,
+                    (world_size, main_fn, args),
+                    nprocs=world_size,
+                    join=True
+                )
+
+    return main
+
+def wrap_main_torchrun(main_fn):
+    def main(**args):
+        local_rank = int(os.environ['LOCAL_RANK'])
+        global_rank = int(os.environ['RANK'])
+        mp.set_start_method('spawn')
+        torch.cuda.set_device(local_rank)
+        dist.init_process_group(backend='nccl')
+        main_fn(args)
+        dist.destroy_process_group()
+    return main
+
+def is_init():
+    return dist.is_initialized()
+
+def gather_list(local_list):
+    """
+    Gather a list from all processes.
+    """
+    with torch.no_grad():
+        if not is_init():
+            return local_list
+        else:
+            gathered_buf = [None for _ in range(world_size())]
+            torch.distributed.all_gather_object(gathered_buf, local_list)
+            return gathered_buf
+
+def rank():
+    if dist.is_initialized():
+        return dist.get_rank()
+    else:
+        return 0
+
+def world_size():
+    if dist.is_initialized():
+        return dist.get_world_size()
+    else:
+        return 1
+
+def reduce_sum(x):
+    with torch.no_grad():
+        if isinstance(x, torch.Tensor):
+            x_copy = x.clone()
+        else:
+            x_copy = torch.tensor(x, device='cuda')
+        if dist.is_initialized():
+            torch.distributed.all_reduce(
+                x_copy, op=torch.distributed.ReduceOp.SUM, async_op=False)
+        return x_copy
+
+def reduce_mean(x):
+    return reduce_sum(x) / world_size()
\ No newline at end of file
diff --git a/tools/dot/lib/decay_to_init.py b/tools/dot/lib/decay_to_init.py
new file mode 100644
index 0000000..06cd0b7
--- /dev/null
+++ b/tools/dot/lib/decay_to_init.py
@@ -0,0 +1,24 @@
+import copy
+import torch
+from contextlib import contextmanager
+
+
+class DecayToInit:
+    def __init__(self, module, decay):
+        super().__init__()
+        self.decay = decay
+        if self.decay > 0:
+            self.module = module
+            self.init = copy.deepcopy(module)
+
+    def _param_pairs(self):
+        module_params = sorted(list(self.module.named_parameters()))
+        init_params = sorted(list(self.init.named_parameters()))
+        return [(p1,p2) for (_,p1), (_,p2) in zip(module_params, init_params)]
+
+    def step(self, step, total_steps):
+        if self.decay > 0:
+            for p_module, p_init in self._param_pairs():
+                decay = self.decay * (1 - (step / total_steps))
+                p_module.data.mul_(1 - decay)
+                p_module.data.add_(decay * p_init.data)
\ No newline at end of file
diff --git a/tools/dot/lib/dpm_solver_pytorch.py b/tools/dot/lib/dpm_solver_pytorch.py
new file mode 100644
index 0000000..fabb41a
--- /dev/null
+++ b/tools/dot/lib/dpm_solver_pytorch.py
@@ -0,0 +1,1406 @@
+'''
+source code from https://github.com/LuChengTHU/dpm-solver
+'''
+
+import torch
+import torch.nn.functional as F
+import math
+
+
+class ModelWrapper(torch.nn.Module):
+    def __init__(self, modules):
+        super().__init__()
+        self.modules = modules
+    
+    def forward(self, x, t, x_selfcond, x_embed, src_mask, **kwargs): 
+        # here x is x_embed,  t is scalar
+        gamma_0, gamma_1 = self.modules['gamma_bounds']()
+        gamma_t = self.modules['noise_schedule'](t).double()
+        gamma_t = gamma_0 + (gamma_1 - gamma_0) * gamma_t
+        # print(gamma_t.shape)
+        embedding_matrix = self.modules['embedding_matrix']()
+        logits, x_reconst = self.modules['model'](
+            z=x.to(torch.float32, copy=True),
+            gamma=gamma_t.float(),
+            embedding_matrix=embedding_matrix,
+            bias_scale=1.,
+            x_selfcond=x_selfcond,
+            x_embed=x_embed,
+            src_mask=src_mask
+        )
+        return logits, x_reconst
+    
+
+class NoiseScheduleVP:
+    def __init__(
+            self,
+            schedule='discrete',
+            betas=None,
+            alphas_cumprod=None,
+            continuous_beta_0=0.1,
+            continuous_beta_1=20.,
+            dtype=torch.float32,
+        ):
+        """Create a wrapper class for the forward SDE (VP type).
+
+        ***
+        Update: We support discrete-time diffusion models by implementing a picewise linear interpolation for log_alpha_t.
+                We recommend to use schedule='discrete' for the discrete-time diffusion models, especially for high-resolution images.
+        ***
+
+        The forward SDE ensures that the condition distribution q_{t|0}(x_t | x_0) = N ( alpha_t * x_0, sigma_t^2 * I ).
+        We further define lambda_t = log(alpha_t) - log(sigma_t), which is the half-logSNR (described in the DPM-Solver paper).
+        Therefore, we implement the functions for computing alpha_t, sigma_t and lambda_t. For t in [0, T], we have:
+
+            log_alpha_t = self.marginal_log_mean_coeff(t)
+            sigma_t = self.marginal_std(t)
+            lambda_t = self.marginal_lambda(t)
+
+        Moreover, as lambda(t) is an invertible function, we also support its inverse function:
+
+            t = self.inverse_lambda(lambda_t)
+
+        ===============================================================
+
+        We support both discrete-time DPMs (trained on n = 0, 1, ..., N-1) and continuous-time DPMs (trained on t in [t_0, T]).
+
+        1. For discrete-time DPMs:
+
+            For discrete-time DPMs trained on n = 0, 1, ..., N-1, we convert the discrete steps to continuous time steps by:
+                t_i = (i + 1) / N
+            e.g. for N = 1000, we have t_0 = 1e-3 and T = t_{N-1} = 1.
+            We solve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.
+
+            Args:
+                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)
+                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)
+
+            Note that we always have alphas_cumprod = cumprod(1 - betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.
+
+            **Important**:  Please pay special attention for the args for `alphas_cumprod`:
+                The `alphas_cumprod` is the \hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that
+                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \sqrt{\hat{alpha_n}} * x_0, (1 - \hat{alpha_n}) * I ).
+                Therefore, the notation \hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have
+                    alpha_{t_n} = \sqrt{\hat{alpha_n}},
+                and
+                    log(alpha_{t_n}) = 0.5 * log(\hat{alpha_n}).
+
+
+        2. For continuous-time DPMs:
+
+            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise
+            schedule are the default settings in DDPM and improved-DDPM:
+
+            Args:
+                beta_min: A `float` number. The smallest beta for the linear schedule.
+                beta_max: A `float` number. The largest beta for the linear schedule.
+                cosine_s: A `float` number. The hyperparameter in the cosine schedule.
+                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.
+                T: A `float` number. The ending time of the forward process.
+
+        ===============================================================
+
+        Args:
+            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,
+                    'linear' or 'cosine' for continuous-time DPMs.
+        Returns:
+            A wrapper object of the forward SDE (VP type).
+        
+        ===============================================================
+
+        Example:
+
+        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):
+        >>> ns = NoiseScheduleVP('discrete', betas=betas)
+
+        # For discrete-time DPMs, given alphas_cumprod (the \hat{alpha_n} array for n = 0, 1, ..., N - 1):
+        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)
+
+        # For continuous-time DPMs (VPSDE), linear schedule:
+        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)
+
+        """
+
+        if schedule not in ['discrete', 'linear', 'cosine']:
+            raise ValueError("Unsupported noise schedule {}. The schedule needs to be 'discrete' or 'linear' or 'cosine'".format(schedule))
+
+        self.schedule = schedule
+        if schedule == 'discrete':
+            if betas is not None:
+                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)
+            else:
+                assert alphas_cumprod is not None
+                log_alphas = 0.5 * torch.log(alphas_cumprod)
+            self.total_N = len(log_alphas)
+            self.T = 1.
+            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1)).to(dtype=dtype)
+            self.log_alpha_array = log_alphas.reshape((1, -1,)).to(dtype=dtype)
+        else:
+            self.total_N = 1000
+            self.beta_0 = continuous_beta_0
+            self.beta_1 = continuous_beta_1
+            self.cosine_s = 0.008
+            self.cosine_beta_max = 999.
+            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s
+            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))
+            self.schedule = schedule
+            if schedule == 'cosine':
+                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.
+                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.
+                self.T = 0.9946
+            else:
+                self.T = 1.
+
+    def marginal_log_mean_coeff(self, t):
+        """
+        Compute log(alpha_t) of a given continuous-time label t in [0, T].
+        """
+        if self.schedule == 'discrete':
+            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))
+        elif self.schedule == 'linear':
+            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0
+        elif self.schedule == 'cosine':
+            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))
+            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0
+            return log_alpha_t
+
+    def marginal_alpha(self, t):
+        """
+        Compute alpha_t of a given continuous-time label t in [0, T].
+        """
+        return torch.exp(self.marginal_log_mean_coeff(t))
+
+    def marginal_std(self, t):
+        """
+        Compute sigma_t of a given continuous-time label t in [0, T].
+        """
+        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))
+
+    def marginal_lambda(self, t):
+        """
+        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].
+        """
+        log_mean_coeff = self.marginal_log_mean_coeff(t)
+        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))
+        return log_mean_coeff - log_std
+
+    def inverse_lambda(self, lamb):
+        """
+        Compute the continuous-time label t in [0, T] of a given half-logSNR lambda_t.
+        """
+        if self.schedule == 'linear':
+            tmp = 2. * (self.beta_1 - self.beta_0) * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))
+            Delta = self.beta_0**2 + tmp
+            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)
+        elif self.schedule == 'discrete':
+            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)
+            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))
+            return t.reshape((-1,))
+        else:
+            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))
+            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s
+            t = t_fn(log_alpha)
+            return t
+
+
+class NoiseSchedulePlaid:
+    def __init__(
+            self,
+            model, # give gamma_t
+            dtype=torch.float32,
+        ):
+        self.schedule = 'learned'
+        self.model = model
+        self.T = 1.
+        self.total_N = 1000
+
+    def gamma(self, t):
+        try:
+            return self.model(t.reshape(-1))
+        except:
+            import pdb; pdb.set_trace();
+        
+    def marginal_log_mean_coeff(self, t):
+        return torch.log(self.marginal_alpha(t))
+
+    def marginal_alpha(self, t):
+        """
+        Compute alpha_t of a given continuous-time label t in [0, T].
+        """
+        return torch.sigmoid(-self.gamma(t)).sqrt()
+
+    def marginal_std(self, t):
+        """
+        Compute sigma_t of a given continuous-time label t in [0, T].
+        """
+        return torch.sqrt(1. - self.marginal_alpha(t)**2)
+
+    def marginal_lambda(self, t):
+        """
+        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].
+        """
+        alpha_square = self.marginal_alpha(t) ** 2
+        return 1/2 * torch.log(alpha_square/(1-alpha_square))
+
+
+def model_wrapper(
+    model,
+    noise_schedule,
+    model_type="noise",
+    model_kwargs={},
+    guidance_type="uncond",
+    condition=None,
+    unconditional_condition=None,
+    guidance_scale=1.,
+    classifier_fn=None,
+    classifier_kwargs={},
+):
+    """Create a wrapper function for the noise prediction model.
+
+    DPM-Solver needs to solve the continuous-time diffusion ODEs. For DPMs trained on discrete-time labels, we need to
+    firstly wrap the model function to a noise prediction model that accepts the continuous time as the input.
+
+    We support four types of the diffusion model by setting `model_type`:
+
+        1. "noise": noise prediction model. (Trained by predicting noise).
+
+        2. "x_start": data prediction model. (Trained by predicting the data x_0 at time 0).
+
+        3. "v": velocity prediction model. (Trained by predicting the velocity).
+            The "v" prediction is derivation detailed in Appendix D of [1], and is used in Imagen-Video [2].
+
+            [1] Salimans, Tim, and Jonathan Ho. "Progressive distillation for fast sampling of diffusion models."
+                arXiv preprint arXiv:2202.00512 (2022).
+            [2] Ho, Jonathan, et al. "Imagen Video: High Definition Video Generation with Diffusion Models."
+                arXiv preprint arXiv:2210.02303 (2022).
+    
+        4. "score": marginal score function. (Trained by denoising score matching).
+            Note that the score function and the noise prediction model follows a simple relationship:
+            ```
+                noise(x_t, t) = -sigma_t * score(x_t, t)
+            ```
+
+    We support three types of guided sampling by DPMs by setting `guidance_type`:
+        1. "uncond": unconditional sampling by DPMs.
+            The input `model` has the following format:
+            ``
+                model(x, t_input, **model_kwargs) -> noise | x_start | v | score
+            ``
+
+        2. "classifier": classifier guidance sampling [3] by DPMs and another classifier.
+            The input `model` has the following format:
+            ``
+                model(x, t_input, **model_kwargs) -> noise | x_start | v | score
+            `` 
+
+            The input `classifier_fn` has the following format:
+            ``
+                classifier_fn(x, t_input, cond, **classifier_kwargs) -> logits(x, t_input, cond)
+            ``
+
+            [3] P. Dhariwal and A. Q. Nichol, "Diffusion models beat GANs on image synthesis,"
+                in Advances in Neural Information Processing Systems, vol. 34, 2021, pp. 8780-8794.
+
+        3. "classifier-free": classifier-free guidance sampling by conditional DPMs.
+            The input `model` has the following format:
+            ``
+                model(x, t_input, cond, **model_kwargs) -> noise | x_start | v | score
+            `` 
+            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.
+
+            [4] Ho, Jonathan, and Tim Salimans. "Classifier-free diffusion guidance."
+                arXiv preprint arXiv:2207.12598 (2022).
+        
+
+    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)
+    or continuous-time labels (i.e. epsilon to T).
+
+    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:
+    ``
+        def model_fn(x, t_continuous) -> noise:
+            t_input = get_model_input_time(t_continuous)
+            return noise_pred(model, x, t_input, **model_kwargs)         
+    ``
+    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.
+
+    ===============================================================
+
+    Args:
+        model: A diffusion model with the corresponding format described above.
+        noise_schedule: A noise schedule object, such as NoiseScheduleVP.
+        model_type: A `str`. The parameterization type of the diffusion model.
+                    "noise" or "x_start" or "v" or "score".
+        model_kwargs: A `dict`. A dict for the other inputs of the model function.
+        guidance_type: A `str`. The type of the guidance for sampling.
+                    "uncond" or "classifier" or "classifier-free".
+        condition: A pytorch tensor. The condition for the guided sampling.
+                    Only used for "classifier" or "classifier-free" guidance type.
+        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.
+                    Only used for "classifier-free" guidance type.
+        guidance_scale: A `float`. The scale for the guided sampling.
+        classifier_fn: A classifier function. Only used for the classifier guidance.
+        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.
+    Returns:
+        A noise prediction model that accepts the noised data and the continuous time as the inputs.
+    """
+
+    def get_model_input_time(t_continuous):
+        """
+        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.
+        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].
+        For continuous-time DPMs, we just use `t_continuous`.
+        """
+        if noise_schedule.schedule == 'discrete':
+            # return (t_continuous - 1. / noise_schedule.total_N) * 1000.
+            return (t_continuous) * noise_schedule.total_N
+        else:
+            return t_continuous
+
+    def noise_pred_fn(x, t_continuous, cond=None):
+        t_input = get_model_input_time(t_continuous)
+        if cond is None:
+            output = model(x, t_input, **model_kwargs)
+        else:
+            output = model(x, t_input, cond, **model_kwargs)
+        
+        if 'x_selfcond' in model_kwargs:
+            logits, output = output
+            model_kwargs['x_selfcond'] = output.clone().detach()
+            model_kwargs['logits'] = logits.clone().detach()
+            # model_kwargs['logits2text'](model_kwargs['logits'], model_kwargs['cur_t_count'])
+
+        if model_type == "noise":
+            return output
+        elif model_type == "x_start":
+            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)
+            noise = (x - alpha_t[...,None,None] * output) / sigma_t[...,None,None]
+            model_kwargs['cur_t_count'] += 1
+            # if model_kwargs['cur_t_count'] < model_kwargs['total_t_count']:
+            #     # dont modify the last one
+            noise /= model_kwargs['score_temp']
+            return noise
+        elif model_type == "v":
+            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)
+            return alpha_t * output + sigma_t * x
+        elif model_type == "score":
+            sigma_t = noise_schedule.marginal_std(t_continuous)
+            return -sigma_t * output
+
+    def cond_grad_fn(x, t_input):
+        """
+        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).
+        """
+        with torch.enable_grad():
+            x_in = x.detach().requires_grad_(True)
+            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)
+            return torch.autograd.grad(log_prob.sum(), x_in)[0]
+
+    def model_fn(x, t_continuous):
+        """
+        The noise predicition model function that is used for DPM-Solver.
+        """
+        if guidance_type == "uncond":
+            return noise_pred_fn(x, t_continuous)
+        elif guidance_type == "classifier":
+            assert classifier_fn is not None
+            t_input = get_model_input_time(t_continuous)
+            cond_grad = cond_grad_fn(x, t_input)
+            sigma_t = noise_schedule.marginal_std(t_continuous)
+            noise = noise_pred_fn(x, t_continuous)
+            return noise - guidance_scale * sigma_t * cond_grad
+        elif guidance_type == "classifier-free":
+            if guidance_scale == 1. or unconditional_condition is None:
+                return noise_pred_fn(x, t_continuous, cond=condition)
+            else:
+                x_in = torch.cat([x] * 2)
+                t_in = torch.cat([t_continuous] * 2)
+                c_in = torch.cat([unconditional_condition, condition])
+                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)
+                return noise_uncond + guidance_scale * (noise - noise_uncond)
+
+    assert model_type in ["noise", "x_start", "v"]
+    assert guidance_type in ["uncond", "classifier", "classifier-free"]
+    return model_fn
+
+
+class DPM_Solver:
+    def __init__(
+        self,
+        model_fn,
+        noise_schedule,
+        algorithm_type="dpmsolver++",
+        correcting_x0_fn=None,
+        correcting_xt_fn=None,
+        thresholding_max_val=1.,
+        dynamic_thresholding_ratio=0.995,
+    ):
+        """Construct a DPM-Solver. 
+
+        We support both DPM-Solver (`algorithm_type="dpmsolver"`) and DPM-Solver++ (`algorithm_type="dpmsolver++"`).
+
+        We also support the "dynamic thresholding" method in Imagen[1]. For pixel-space diffusion models, you
+        can set both `algorithm_type="dpmsolver++"` and `correcting_x0_fn="dynamic_thresholding"` to use the
+        dynamic thresholding. The "dynamic thresholding" can greatly improve the sample quality for pixel-space
+        DPMs with large guidance scales. Note that the thresholding method is **unsuitable** for latent-space
+        DPMs (such as stable-diffusion).
+
+        To support advanced algorithms in image-to-image applications, we also support corrector functions for
+        both x0 and xt.
+
+        Args:
+            model_fn: A noise prediction model function which accepts the continuous-time input (t in [epsilon, T]):
+                ``
+                def model_fn(x, t_continuous):
+                    return noise
+                ``
+                The shape of `x` is `(batch_size, **shape)`, and the shape of `t_continuous` is `(batch_size,)`.
+            noise_schedule: A noise schedule object, such as NoiseScheduleVP.
+            algorithm_type: A `str`. Either "dpmsolver" or "dpmsolver++".
+            correcting_x0_fn: A `str` or a function with the following format:
+                ```
+                def correcting_x0_fn(x0, t):
+                    x0_new = ...
+                    return x0_new
+                ```
+                This function is to correct the outputs of the data prediction model at each sampling step. e.g.,
+                ```
+                x0_pred = data_pred_model(xt, t)
+                if correcting_x0_fn is not None:
+                    x0_pred = correcting_x0_fn(x0_pred, t)
+                xt_1 = update(x0_pred, xt, t)
+                ```
+                If `correcting_x0_fn="dynamic_thresholding"`, we use the dynamic thresholding proposed in Imagen[1].
+            correcting_xt_fn: A function with the following format:
+                ```
+                def correcting_xt_fn(xt, t, step):
+                    x_new = ...
+                    return x_new
+                ```
+                This function is to correct the intermediate samples xt at each sampling step. e.g.,
+                ```
+                xt = ...
+                xt = correcting_xt_fn(xt, t, step)
+                ```
+            thresholding_max_val: A `float`. The max value for thresholding.
+                Valid only when use `dpmsolver++` and `correcting_x0_fn="dynamic_thresholding"`.
+            dynamic_thresholding_ratio: A `float`. The ratio for dynamic thresholding (see Imagen[1] for details).
+                Valid only when use `dpmsolver++` and `correcting_x0_fn="dynamic_thresholding"`.
+
+        [1] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,
+            Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-image diffusion models
+            with deep language understanding. arXiv preprint arXiv:2205.11487, 2022b.
+        """
+        self.model = lambda x, t: model_fn(x, t.expand((x.shape[0])))
+        self.noise_schedule = noise_schedule
+        assert algorithm_type in ["dpmsolver", "dpmsolver++"]
+        self.algorithm_type = algorithm_type
+        if correcting_x0_fn == "dynamic_thresholding":
+            self.correcting_x0_fn = self.dynamic_thresholding_fn
+        else:
+            self.correcting_x0_fn = correcting_x0_fn
+        self.correcting_xt_fn = correcting_xt_fn
+        self.dynamic_thresholding_ratio = dynamic_thresholding_ratio
+        self.thresholding_max_val = thresholding_max_val
+
+    def dynamic_thresholding_fn(self, x0, t):
+        """
+        The dynamic thresholding method. 
+        """
+        dims = x0.dim()
+        p = self.dynamic_thresholding_ratio
+        s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)
+        s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)
+        x0 = torch.clamp(x0, -s, s) / s
+        return x0
+
+    def noise_prediction_fn(self, x, t):
+        """
+        Return the noise prediction model.
+        """
+        return self.model(x, t)
+
+    def data_prediction_fn(self, x, t):
+        """
+        Return the data prediction model (with corrector).
+        """
+        noise = self.noise_prediction_fn(x, t)
+        alpha_t, sigma_t = self.noise_schedule.marginal_alpha(t), self.noise_schedule.marginal_std(t)
+        x0 = (x - sigma_t * noise) / alpha_t
+        if self.correcting_x0_fn is not None:
+            x0 = self.correcting_x0_fn(x0)
+        return x0
+
+    def model_fn(self, x, t):
+        """
+        Convert the model to the noise prediction model or the data prediction model. 
+        """
+        if self.algorithm_type == "dpmsolver++":
+            return self.data_prediction_fn(x, t)
+        else:
+            return self.noise_prediction_fn(x, t)
+
+    def get_time_steps(self, skip_type, t_T, t_0, N, device):
+        """Compute the intermediate time steps for sampling.
+
+        Args:
+            skip_type: A `str`. The type for the spacing of the time steps. We support three types:
+                - 'logSNR': uniform logSNR for the time steps.
+                - 'time_uniform': uniform time for the time steps. (**Recommended for high-resolutional data**.)
+                - 'time_quadratic': quadratic time for the time steps. (Used in DDIM for low-resolutional data.)
+            t_T: A `float`. The starting time of the sampling (default is T).
+            t_0: A `float`. The ending time of the sampling (default is epsilon).
+            N: A `int`. The total number of the spacing of the time steps.
+            device: A torch device.
+        Returns:
+            A pytorch tensor of the time steps, with the shape (N + 1,).
+        """
+        if skip_type == 'logSNR':
+            lambda_T = self.noise_schedule.marginal_lambda(torch.tensor(t_T).to(device))
+            lambda_0 = self.noise_schedule.marginal_lambda(torch.tensor(t_0).to(device))
+            logSNR_steps = torch.linspace(lambda_T.cpu().item(), lambda_0.cpu().item(), N + 1).to(device)
+            return self.noise_schedule.inverse_lambda(logSNR_steps)
+        elif skip_type == 'time_uniform':
+            return torch.linspace(t_T, t_0, N + 1).to(device)
+        elif skip_type == 'time_quadratic':
+            t_order = 2
+            t = torch.linspace(t_T**(1. / t_order), t_0**(1. / t_order), N + 1).pow(t_order).to(device)
+            return t
+        else:
+            raise ValueError("Unsupported skip_type {}, need to be 'logSNR' or 'time_uniform' or 'time_quadratic'".format(skip_type))
+
+    def get_orders_and_timesteps_for_singlestep_solver(self, steps, order, skip_type, t_T, t_0, device):
+        """
+        Get the order of each step for sampling by the singlestep DPM-Solver.
+
+        We combine both DPM-Solver-1,2,3 to use all the function evaluations, which is named as "DPM-Solver-fast".
+        Given a fixed number of function evaluations by `steps`, the sampling procedure by DPM-Solver-fast is:
+            - If order == 1:
+                We take `steps` of DPM-Solver-1 (i.e. DDIM).
+            - If order == 2:
+                - Denote K = (steps // 2). We take K or (K + 1) intermediate time steps for sampling.
+                - If steps % 2 == 0, we use K steps of DPM-Solver-2.
+                - If steps % 2 == 1, we use K steps of DPM-Solver-2 and 1 step of DPM-Solver-1.
+            - If order == 3:
+                - Denote K = (steps // 3 + 1). We take K intermediate time steps for sampling.
+                - If steps % 3 == 0, we use (K - 2) steps of DPM-Solver-3, and 1 step of DPM-Solver-2 and 1 step of DPM-Solver-1.
+                - If steps % 3 == 1, we use (K - 1) steps of DPM-Solver-3 and 1 step of DPM-Solver-1.
+                - If steps % 3 == 2, we use (K - 1) steps of DPM-Solver-3 and 1 step of DPM-Solver-2.
+
+        ============================================
+        Args:
+            order: A `int`. The max order for the solver (2 or 3).
+            steps: A `int`. The total number of function evaluations (NFE).
+            skip_type: A `str`. The type for the spacing of the time steps. We support three types:
+                - 'logSNR': uniform logSNR for the time steps.
+                - 'time_uniform': uniform time for the time steps. (**Recommended for high-resolutional data**.)
+                - 'time_quadratic': quadratic time for the time steps. (Used in DDIM for low-resolutional data.)
+            t_T: A `float`. The starting time of the sampling (default is T).
+            t_0: A `float`. The ending time of the sampling (default is epsilon).
+            device: A torch device.
+        Returns:
+            orders: A list of the solver order of each step.
+        """
+        if order == 3:
+            K = steps // 3 + 1
+            if steps % 3 == 0:
+                orders = [3,] * (K - 2) + [2, 1]
+            elif steps % 3 == 1:
+                orders = [3,] * (K - 1) + [1]
+            else:
+                orders = [3,] * (K - 1) + [2]
+        elif order == 2:
+            if steps % 2 == 0:
+                K = steps // 2
+                orders = [2,] * K
+            else:
+                K = steps // 2 + 1
+                orders = [2,] * (K - 1) + [1]
+        elif order == 1:
+            K = 1
+            orders = [1,] * steps
+        else:
+            raise ValueError("'order' must be '1' or '2' or '3'.")
+        if skip_type == 'logSNR':
+            # To reproduce the results in DPM-Solver paper
+            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, K, device)
+        else:
+            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, steps, device)[torch.cumsum(torch.tensor([0,] + orders), 0).to(device)]
+        return timesteps_outer, orders
+
+    def denoise_to_zero_fn(self, x, s):
+        """
+        Denoise at the final step, which is equivalent to solve the ODE from lambda_s to infty by first-order discretization. 
+        """
+        return self.data_prediction_fn(x, s)
+
+    def dpm_solver_first_update(self, x, s, t, model_s=None, return_intermediate=False):
+        """
+        DPM-Solver-1 (equivalent to DDIM) from time `s` to time `t`.
+
+        Args:
+            x: A pytorch tensor. The initial value at time `s`.
+            s: A pytorch tensor. The starting time, with the shape (1,).
+            t: A pytorch tensor. The ending time, with the shape (1,).
+            model_s: A pytorch tensor. The model function evaluated at time `s`.
+                If `model_s` is None, we evaluate the model by `x` and `s`; otherwise we directly use it.
+            return_intermediate: A `bool`. If true, also return the model value at time `s`.
+        Returns:
+            x_t: A pytorch tensor. The approximated solution at time `t`.
+        """
+        ns = self.noise_schedule
+        dims = x.dim()
+        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)
+        h = lambda_t - lambda_s
+        log_alpha_s, log_alpha_t = ns.marginal_log_mean_coeff(s), ns.marginal_log_mean_coeff(t)
+        sigma_s, sigma_t = ns.marginal_std(s), ns.marginal_std(t)
+        alpha_t = torch.exp(log_alpha_t)
+
+        if self.algorithm_type == "dpmsolver++":
+            phi_1 = torch.expm1(-h)
+            if model_s is None:
+                model_s = self.model_fn(x, s)
+            x_t = (
+                sigma_t / sigma_s * x
+                - alpha_t * phi_1 * model_s
+            )
+            if return_intermediate:
+                return x_t, {'model_s': model_s}
+            else:
+                return x_t
+        else:
+            phi_1 = torch.expm1(h)
+            if model_s is None:
+                model_s = self.model_fn(x, s)
+            x_t = (
+                torch.exp(log_alpha_t - log_alpha_s) * x
+                - (sigma_t * phi_1) * model_s
+            )
+            if return_intermediate:
+                return x_t, {'model_s': model_s}
+            else:
+                return x_t
+
+    def singlestep_dpm_solver_second_update(self, x, s, t, r1=0.5, model_s=None, return_intermediate=False, solver_type='dpmsolver'):
+        """
+        Singlestep solver DPM-Solver-2 from time `s` to time `t`.
+
+        Args:
+            x: A pytorch tensor. The initial value at time `s`.
+            s: A pytorch tensor. The starting time, with the shape (1,).
+            t: A pytorch tensor. The ending time, with the shape (1,).
+            r1: A `float`. The hyperparameter of the second-order solver.
+            model_s: A pytorch tensor. The model function evaluated at time `s`.
+                If `model_s` is None, we evaluate the model by `x` and `s`; otherwise we directly use it.
+            return_intermediate: A `bool`. If true, also return the model value at time `s` and `s1` (the intermediate time).
+            solver_type: either 'dpmsolver' or 'taylor'. The type for the high-order solvers.
+                The type slightly impacts the performance. We recommend to use 'dpmsolver' type.
+        Returns:
+            x_t: A pytorch tensor. The approximated solution at time `t`.
+        """
+        if solver_type not in ['dpmsolver', 'taylor']:
+            raise ValueError("'solver_type' must be either 'dpmsolver' or 'taylor', got {}".format(solver_type))
+        if r1 is None:
+            r1 = 0.5
+        ns = self.noise_schedule
+        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)
+        h = lambda_t - lambda_s
+        lambda_s1 = lambda_s + r1 * h
+        s1 = ns.inverse_lambda(lambda_s1)
+        log_alpha_s, log_alpha_s1, log_alpha_t = ns.marginal_log_mean_coeff(s), ns.marginal_log_mean_coeff(s1), ns.marginal_log_mean_coeff(t)
+        sigma_s, sigma_s1, sigma_t = ns.marginal_std(s), ns.marginal_std(s1), ns.marginal_std(t)
+        alpha_s1, alpha_t = torch.exp(log_alpha_s1), torch.exp(log_alpha_t)
+
+        if self.algorithm_type == "dpmsolver++":
+            phi_11 = torch.expm1(-r1 * h)
+            phi_1 = torch.expm1(-h)
+
+            if model_s is None:
+                model_s = self.model_fn(x, s)
+            x_s1 = (
+                (sigma_s1 / sigma_s) * x
+                - (alpha_s1 * phi_11) * model_s
+            )
+            model_s1 = self.model_fn(x_s1, s1)
+            if solver_type == 'dpmsolver':
+                x_t = (
+                    (sigma_t / sigma_s) * x
+                    - (alpha_t * phi_1) * model_s
+                    - (0.5 / r1) * (alpha_t * phi_1) * (model_s1 - model_s)
+                )
+            elif solver_type == 'taylor':
+                x_t = (
+                    (sigma_t / sigma_s) * x
+                    - (alpha_t * phi_1) * model_s
+                    + (1. / r1) * (alpha_t * (phi_1 / h + 1.)) * (model_s1 - model_s)
+                )
+        else:
+            phi_11 = torch.expm1(r1 * h)
+            phi_1 = torch.expm1(h)
+
+            if model_s is None:
+                model_s = self.model_fn(x, s)
+            x_s1 = (
+                torch.exp(log_alpha_s1 - log_alpha_s) * x
+                - (sigma_s1 * phi_11) * model_s
+            )
+            model_s1 = self.model_fn(x_s1, s1)
+            if solver_type == 'dpmsolver':
+                x_t = (
+                    torch.exp(log_alpha_t - log_alpha_s) * x
+                    - (sigma_t * phi_1) * model_s
+                    - (0.5 / r1) * (sigma_t * phi_1) * (model_s1 - model_s)
+                )
+            elif solver_type == 'taylor':
+                x_t = (
+                    torch.exp(log_alpha_t - log_alpha_s) * x
+                    - (sigma_t * phi_1) * model_s
+                    - (1. / r1) * (sigma_t * (phi_1 / h - 1.)) * (model_s1 - model_s)
+                )
+        if return_intermediate:
+            return x_t, {'model_s': model_s, 'model_s1': model_s1}
+        else:
+            return x_t
+
+    def singlestep_dpm_solver_third_update(self, x, s, t, r1=1./3., r2=2./3., model_s=None, model_s1=None, return_intermediate=False, solver_type='dpmsolver'):
+        """
+        Singlestep solver DPM-Solver-3 from time `s` to time `t`.
+
+        Args:
+            x: A pytorch tensor. The initial value at time `s`.
+            s: A pytorch tensor. The starting time, with the shape (1,).
+            t: A pytorch tensor. The ending time, with the shape (1,).
+            r1: A `float`. The hyperparameter of the third-order solver.
+            r2: A `float`. The hyperparameter of the third-order solver.
+            model_s: A pytorch tensor. The model function evaluated at time `s`.
+                If `model_s` is None, we evaluate the model by `x` and `s`; otherwise we directly use it.
+            model_s1: A pytorch tensor. The model function evaluated at time `s1` (the intermediate time given by `r1`).
+                If `model_s1` is None, we evaluate the model at `s1`; otherwise we directly use it.
+            return_intermediate: A `bool`. If true, also return the model value at time `s`, `s1` and `s2` (the intermediate times).
+            solver_type: either 'dpmsolver' or 'taylor'. The type for the high-order solvers.
+                The type slightly impacts the performance. We recommend to use 'dpmsolver' type.
+        Returns:
+            x_t: A pytorch tensor. The approximated solution at time `t`.
+        """
+        if solver_type not in ['dpmsolver', 'taylor']:
+            raise ValueError("'solver_type' must be either 'dpmsolver' or 'taylor', got {}".format(solver_type))
+        if r1 is None:
+            r1 = 1. / 3.
+        if r2 is None:
+            r2 = 2. / 3.
+        ns = self.noise_schedule
+        lambda_s, lambda_t = ns.marginal_lambda(s), ns.marginal_lambda(t)
+        h = lambda_t - lambda_s
+        lambda_s1 = lambda_s + r1 * h
+        lambda_s2 = lambda_s + r2 * h
+        s1 = ns.inverse_lambda(lambda_s1)
+        s2 = ns.inverse_lambda(lambda_s2)
+        log_alpha_s, log_alpha_s1, log_alpha_s2, log_alpha_t = ns.marginal_log_mean_coeff(s), ns.marginal_log_mean_coeff(s1), ns.marginal_log_mean_coeff(s2), ns.marginal_log_mean_coeff(t)
+        sigma_s, sigma_s1, sigma_s2, sigma_t = ns.marginal_std(s), ns.marginal_std(s1), ns.marginal_std(s2), ns.marginal_std(t)
+        alpha_s1, alpha_s2, alpha_t = torch.exp(log_alpha_s1), torch.exp(log_alpha_s2), torch.exp(log_alpha_t)
+
+        if self.algorithm_type == "dpmsolver++":
+            phi_11 = torch.expm1(-r1 * h)
+            phi_12 = torch.expm1(-r2 * h)
+            phi_1 = torch.expm1(-h)
+            phi_22 = torch.expm1(-r2 * h) / (r2 * h) + 1.
+            phi_2 = phi_1 / h + 1.
+            phi_3 = phi_2 / h - 0.5
+
+            if model_s is None:
+                model_s = self.model_fn(x, s)
+            if model_s1 is None:
+                x_s1 = (
+                    (sigma_s1 / sigma_s) * x
+                    - (alpha_s1 * phi_11) * model_s
+                )
+                model_s1 = self.model_fn(x_s1, s1)
+            x_s2 = (
+                (sigma_s2 / sigma_s) * x
+                - (alpha_s2 * phi_12) * model_s
+                + r2 / r1 * (alpha_s2 * phi_22) * (model_s1 - model_s)
+            )
+            model_s2 = self.model_fn(x_s2, s2)
+            if solver_type == 'dpmsolver':
+                x_t = (
+                    (sigma_t / sigma_s) * x
+                    - (alpha_t * phi_1) * model_s
+                    + (1. / r2) * (alpha_t * phi_2) * (model_s2 - model_s)
+                )
+            elif solver_type == 'taylor':
+                D1_0 = (1. / r1) * (model_s1 - model_s)
+                D1_1 = (1. / r2) * (model_s2 - model_s)
+                D1 = (r2 * D1_0 - r1 * D1_1) / (r2 - r1)
+                D2 = 2. * (D1_1 - D1_0) / (r2 - r1)
+                x_t = (
+                    (sigma_t / sigma_s) * x
+                    - (alpha_t * phi_1) * model_s
+                    + (alpha_t * phi_2) * D1
+                    - (alpha_t * phi_3) * D2
+                )
+        else:
+            phi_11 = torch.expm1(r1 * h)
+            phi_12 = torch.expm1(r2 * h)
+            phi_1 = torch.expm1(h)
+            phi_22 = torch.expm1(r2 * h) / (r2 * h) - 1.
+            phi_2 = phi_1 / h - 1.
+            phi_3 = phi_2 / h - 0.5
+
+            if model_s is None:
+                model_s = self.model_fn(x, s)
+            if model_s1 is None:
+                x_s1 = (
+                    (torch.exp(log_alpha_s1 - log_alpha_s)) * x
+                    - (sigma_s1 * phi_11) * model_s
+                )
+                model_s1 = self.model_fn(x_s1, s1)
+            x_s2 = (
+                (torch.exp(log_alpha_s2 - log_alpha_s)) * x
+                - (sigma_s2 * phi_12) * model_s
+                - r2 / r1 * (sigma_s2 * phi_22) * (model_s1 - model_s)
+            )
+            model_s2 = self.model_fn(x_s2, s2)
+            if solver_type == 'dpmsolver':
+                x_t = (
+                    (torch.exp(log_alpha_t - log_alpha_s)) * x
+                    - (sigma_t * phi_1) * model_s
+                    - (1. / r2) * (sigma_t * phi_2) * (model_s2 - model_s)
+                )
+            elif solver_type == 'taylor':
+                D1_0 = (1. / r1) * (model_s1 - model_s)
+                D1_1 = (1. / r2) * (model_s2 - model_s)
+                D1 = (r2 * D1_0 - r1 * D1_1) / (r2 - r1)
+                D2 = 2. * (D1_1 - D1_0) / (r2 - r1)
+                x_t = (
+                    (torch.exp(log_alpha_t - log_alpha_s)) * x
+                    - (sigma_t * phi_1) * model_s
+                    - (sigma_t * phi_2) * D1
+                    - (sigma_t * phi_3) * D2
+                )
+
+        if return_intermediate:
+            return x_t, {'model_s': model_s, 'model_s1': model_s1, 'model_s2': model_s2}
+        else:
+            return x_t
+
+    def multistep_dpm_solver_second_update(self, x, model_prev_list, t_prev_list, t, solver_type="dpmsolver"):
+        """
+        Multistep solver DPM-Solver-2 from time `t_prev_list[-1]` to time `t`.
+
+        Args:
+            x: A pytorch tensor. The initial value at time `s`.
+            model_prev_list: A list of pytorch tensor. The previous computed model values.
+            t_prev_list: A list of pytorch tensor. The previous times, each time has the shape (1,)
+            t: A pytorch tensor. The ending time, with the shape (1,).
+            solver_type: either 'dpmsolver' or 'taylor'. The type for the high-order solvers.
+                The type slightly impacts the performance. We recommend to use 'dpmsolver' type.
+        Returns:
+            x_t: A pytorch tensor. The approximated solution at time `t`.
+        """
+        if solver_type not in ['dpmsolver', 'taylor']:
+            raise ValueError("'solver_type' must be either 'dpmsolver' or 'taylor', got {}".format(solver_type))
+        ns = self.noise_schedule
+        model_prev_1, model_prev_0 = model_prev_list[-2], model_prev_list[-1]
+        t_prev_1, t_prev_0 = t_prev_list[-2], t_prev_list[-1]
+        lambda_prev_1, lambda_prev_0, lambda_t = ns.marginal_lambda(t_prev_1), ns.marginal_lambda(t_prev_0), ns.marginal_lambda(t)
+        log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)
+        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)
+        alpha_t = torch.exp(log_alpha_t)
+
+        h_0 = lambda_prev_0 - lambda_prev_1
+        h = lambda_t - lambda_prev_0
+        r0 = h_0 / h
+        D1_0 = (1. / r0) * (model_prev_0 - model_prev_1)
+        if self.algorithm_type == "dpmsolver++":
+            phi_1 = torch.expm1(-h)
+            if solver_type == 'dpmsolver':
+                x_t = (
+                    (sigma_t / sigma_prev_0) * x
+                    - (alpha_t * phi_1) * model_prev_0
+                    - 0.5 * (alpha_t * phi_1) * D1_0
+                )
+            elif solver_type == 'taylor':
+                x_t = (
+                    (sigma_t / sigma_prev_0) * x
+                    - (alpha_t * phi_1) * model_prev_0
+                    + (alpha_t * (phi_1 / h + 1.)) * D1_0
+                )
+        else:
+            phi_1 = torch.expm1(h)
+            if solver_type == 'dpmsolver':
+                x_t = (
+                    (torch.exp(log_alpha_t - log_alpha_prev_0)) * x
+                    - (sigma_t * phi_1) * model_prev_0
+                    - 0.5 * (sigma_t * phi_1) * D1_0
+                )
+            elif solver_type == 'taylor':
+                x_t = (
+                    (torch.exp(log_alpha_t - log_alpha_prev_0)) * x
+                    - (sigma_t * phi_1) * model_prev_0
+                    - (sigma_t * (phi_1 / h - 1.)) * D1_0
+                )
+        return x_t
+
+    def multistep_dpm_solver_third_update(self, x, model_prev_list, t_prev_list, t, solver_type='dpmsolver'):
+        """
+        Multistep solver DPM-Solver-3 from time `t_prev_list[-1]` to time `t`.
+
+        Args:
+            x: A pytorch tensor. The initial value at time `s`.
+            model_prev_list: A list of pytorch tensor. The previous computed model values.
+            t_prev_list: A list of pytorch tensor. The previous times, each time has the shape (1,)
+            t: A pytorch tensor. The ending time, with the shape (1,).
+            solver_type: either 'dpmsolver' or 'taylor'. The type for the high-order solvers.
+                The type slightly impacts the performance. We recommend to use 'dpmsolver' type.
+        Returns:
+            x_t: A pytorch tensor. The approximated solution at time `t`.
+        """
+        ns = self.noise_schedule
+        model_prev_2, model_prev_1, model_prev_0 = model_prev_list
+        t_prev_2, t_prev_1, t_prev_0 = t_prev_list
+        lambda_prev_2, lambda_prev_1, lambda_prev_0, lambda_t = ns.marginal_lambda(t_prev_2), ns.marginal_lambda(t_prev_1), ns.marginal_lambda(t_prev_0), ns.marginal_lambda(t)
+        log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)
+        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)
+        alpha_t = torch.exp(log_alpha_t)
+
+        h_1 = lambda_prev_1 - lambda_prev_2
+        h_0 = lambda_prev_0 - lambda_prev_1
+        h = lambda_t - lambda_prev_0
+        r0, r1 = h_0 / h, h_1 / h
+        D1_0 = (1. / r0) * (model_prev_0 - model_prev_1)
+        D1_1 = (1. / r1) * (model_prev_1 - model_prev_2)
+        D1 = D1_0 + (r0 / (r0 + r1)) * (D1_0 - D1_1)
+        D2 = (1. / (r0 + r1)) * (D1_0 - D1_1)
+        if self.algorithm_type == "dpmsolver++":
+            phi_1 = torch.expm1(-h)
+            phi_2 = phi_1 / h + 1.
+            phi_3 = phi_2 / h - 0.5
+            x_t = (
+                (sigma_t / sigma_prev_0) * x
+                - (alpha_t * phi_1) * model_prev_0
+                + (alpha_t * phi_2) * D1
+                - (alpha_t * phi_3) * D2
+            )
+        else:
+            phi_1 = torch.expm1(h)
+            phi_2 = phi_1 / h - 1.
+            phi_3 = phi_2 / h - 0.5
+            x_t = (
+                (torch.exp(log_alpha_t - log_alpha_prev_0)) * x
+                - (sigma_t * phi_1) * model_prev_0
+                - (sigma_t * phi_2) * D1
+                - (sigma_t * phi_3) * D2
+            )
+        return x_t
+
+    def singlestep_dpm_solver_update(self, x, s, t, order, return_intermediate=False, solver_type='dpmsolver', r1=None, r2=None):
+        """
+        Singlestep DPM-Solver with the order `order` from time `s` to time `t`.
+
+        Args:
+            x: A pytorch tensor. The initial value at time `s`.
+            s: A pytorch tensor. The starting time, with the shape (1,).
+            t: A pytorch tensor. The ending time, with the shape (1,).
+            order: A `int`. The order of DPM-Solver. We only support order == 1 or 2 or 3.
+            return_intermediate: A `bool`. If true, also return the model value at time `s`, `s1` and `s2` (the intermediate times).
+            solver_type: either 'dpmsolver' or 'taylor'. The type for the high-order solvers.
+                The type slightly impacts the performance. We recommend to use 'dpmsolver' type.
+            r1: A `float`. The hyperparameter of the second-order or third-order solver.
+            r2: A `float`. The hyperparameter of the third-order solver.
+        Returns:
+            x_t: A pytorch tensor. The approximated solution at time `t`.
+        """
+        if order == 1:
+            return self.dpm_solver_first_update(x, s, t, return_intermediate=return_intermediate)
+        elif order == 2:
+            return self.singlestep_dpm_solver_second_update(x, s, t, return_intermediate=return_intermediate, solver_type=solver_type, r1=r1)
+        elif order == 3:
+            return self.singlestep_dpm_solver_third_update(x, s, t, return_intermediate=return_intermediate, solver_type=solver_type, r1=r1, r2=r2)
+        else:
+            raise ValueError("Solver order must be 1 or 2 or 3, got {}".format(order))
+
+    def multistep_dpm_solver_update(self, x, model_prev_list, t_prev_list, t, order, solver_type='dpmsolver'):
+        """
+        Multistep DPM-Solver with the order `order` from time `t_prev_list[-1]` to time `t`.
+
+        Args:
+            x: A pytorch tensor. The initial value at time `s`.
+            model_prev_list: A list of pytorch tensor. The previous computed model values.
+            t_prev_list: A list of pytorch tensor. The previous times, each time has the shape (1,)
+            t: A pytorch tensor. The ending time, with the shape (1,).
+            order: A `int`. The order of DPM-Solver. We only support order == 1 or 2 or 3.
+            solver_type: either 'dpmsolver' or 'taylor'. The type for the high-order solvers.
+                The type slightly impacts the performance. We recommend to use 'dpmsolver' type.
+        Returns:
+            x_t: A pytorch tensor. The approximated solution at time `t`.
+        """
+        if order == 1:
+            return self.dpm_solver_first_update(x, t_prev_list[-1], t, model_s=model_prev_list[-1])
+        elif order == 2:
+            return self.multistep_dpm_solver_second_update(x, model_prev_list, t_prev_list, t, solver_type=solver_type)
+        elif order == 3:
+            return self.multistep_dpm_solver_third_update(x, model_prev_list, t_prev_list, t, solver_type=solver_type)
+        else:
+            raise ValueError("Solver order must be 1 or 2 or 3, got {}".format(order))
+
+    def dpm_solver_adaptive(self, x, order, t_T, t_0, h_init=0.05, atol=0.0078, rtol=0.05, theta=0.9, t_err=1e-5, solver_type='dpmsolver'):
+        """
+        The adaptive step size solver based on singlestep DPM-Solver.
+
+        Args:
+            x: A pytorch tensor. The initial value at time `t_T`.
+            order: A `int`. The (higher) order of the solver. We only support order == 2 or 3.
+            t_T: A `float`. The starting time of the sampling (default is T).
+            t_0: A `float`. The ending time of the sampling (default is epsilon).
+            h_init: A `float`. The initial step size (for logSNR).
+            atol: A `float`. The absolute tolerance of the solver. For image data, the default setting is 0.0078, followed [1].
+            rtol: A `float`. The relative tolerance of the solver. The default setting is 0.05.
+            theta: A `float`. The safety hyperparameter for adapting the step size. The default setting is 0.9, followed [1].
+            t_err: A `float`. The tolerance for the time. We solve the diffusion ODE until the absolute error between the 
+                current time and `t_0` is less than `t_err`. The default setting is 1e-5.
+            solver_type: either 'dpmsolver' or 'taylor'. The type for the high-order solvers.
+                The type slightly impacts the performance. We recommend to use 'dpmsolver' type.
+        Returns:
+            x_0: A pytorch tensor. The approximated solution at time `t_0`.
+
+        [1] A. Jolicoeur-Martineau, K. Li, R. PichÃ©-Taillefer, T. Kachman, and I. Mitliagkas, "Gotta go fast when generating data with score-based models," arXiv preprint arXiv:2105.14080, 2021.
+        """
+        ns = self.noise_schedule
+        s = t_T * torch.ones((1,)).to(x)
+        lambda_s = ns.marginal_lambda(s)
+        lambda_0 = ns.marginal_lambda(t_0 * torch.ones_like(s).to(x))
+        h = h_init * torch.ones_like(s).to(x)
+        x_prev = x
+        nfe = 0
+        if order == 2:
+            r1 = 0.5
+            lower_update = lambda x, s, t: self.dpm_solver_first_update(x, s, t, return_intermediate=True)
+            higher_update = lambda x, s, t, **kwargs: self.singlestep_dpm_solver_second_update(x, s, t, r1=r1, solver_type=solver_type, **kwargs)
+        elif order == 3:
+            r1, r2 = 1. / 3., 2. / 3.
+            lower_update = lambda x, s, t: self.singlestep_dpm_solver_second_update(x, s, t, r1=r1, return_intermediate=True, solver_type=solver_type)
+            higher_update = lambda x, s, t, **kwargs: self.singlestep_dpm_solver_third_update(x, s, t, r1=r1, r2=r2, solver_type=solver_type, **kwargs)
+        else:
+            raise ValueError("For adaptive step size solver, order must be 2 or 3, got {}".format(order))
+        while torch.abs((s - t_0)).mean() > t_err:
+            t = ns.inverse_lambda(lambda_s + h)
+            x_lower, lower_noise_kwargs = lower_update(x, s, t)
+            x_higher = higher_update(x, s, t, **lower_noise_kwargs)
+            delta = torch.max(torch.ones_like(x).to(x) * atol, rtol * torch.max(torch.abs(x_lower), torch.abs(x_prev)))
+            norm_fn = lambda v: torch.sqrt(torch.square(v.reshape((v.shape[0], -1))).mean(dim=-1, keepdim=True))
+            E = norm_fn((x_higher - x_lower) / delta).max()
+            if torch.all(E <= 1.):
+                x = x_higher
+                s = t
+                x_prev = x_lower
+                lambda_s = ns.marginal_lambda(s)
+            h = torch.min(theta * h * torch.float_power(E, -1. / order).float(), lambda_0 - lambda_s)
+            nfe += order
+        print('adaptive solver nfe', nfe)
+        return x
+
+    def add_noise(self, x, t, noise=None):
+        """
+        Compute the noised input xt = alpha_t * x + sigma_t * noise. 
+
+        Args:
+            x: A `torch.Tensor` with shape `(batch_size, *shape)`.
+            t: A `torch.Tensor` with shape `(t_size,)`.
+        Returns:
+            xt with shape `(t_size, batch_size, *shape)`.
+        """
+        alpha_t, sigma_t = self.noise_schedule.marginal_alpha(t), self.noise_schedule.marginal_std(t)
+        if noise is None:
+            noise = torch.randn((t.shape[0], *x.shape), device=x.device)
+        x = x.reshape((-1, *x.shape))
+        xt = expand_dims(alpha_t, x.dim()) * x + expand_dims(sigma_t, x.dim()) * noise
+        if t.shape[0] == 1:
+            return xt.squeeze(0)
+        else:
+            return xt
+
+    def inverse(self, x, steps=20, t_start=None, t_end=None, order=2, skip_type='time_uniform',
+        method='multistep', lower_order_final=True, denoise_to_zero=False, solver_type='dpmsolver',
+        atol=0.0078, rtol=0.05, return_intermediate=False,
+    ):
+        """
+        Inverse the sample `x` from time `t_start` to `t_end` by DPM-Solver.
+        For discrete-time DPMs, we use `t_start=1/N`, where `N` is the total time steps during training.
+        """
+        t_0 = 1. / self.noise_schedule.total_N if t_start is None else t_start
+        t_T = self.noise_schedule.T if t_end is None else t_end
+        assert t_0 > 0 and t_T > 0, "Time range needs to be greater than 0. For discrete-time DPMs, it needs to be in [1 / N, 1], where N is the length of betas array"
+        return self.sample(x, steps=steps, t_start=t_0, t_end=t_T, order=order, skip_type=skip_type,
+            method=method, lower_order_final=lower_order_final, denoise_to_zero=denoise_to_zero, solver_type=solver_type,
+            atol=atol, rtol=rtol, return_intermediate=return_intermediate)
+
+
+    # modified by DiffuSeq from DPM-solver++
+    def sample(self, x, steps=20, t_start=None, t_end=None, order=2, skip_type='time_uniform',
+        method='multistep', lower_order_final=True, denoise_to_zero=False, solver_type='dpmsolver',
+        atol=0.0078, rtol=0.05, return_intermediate=False, x_start=None, input_ids_mask=None,
+    ):
+        """
+        Compute the sample at time `t_end` by DPM-Solver, given the initial `x` at time `t_start`.
+
+        =====================================================
+
+        We support the following algorithms for both noise prediction model and data prediction model:
+            - 'singlestep':
+                Singlestep DPM-Solver (i.e. "DPM-Solver-fast" in the paper), which combines different orders of singlestep DPM-Solver. 
+                We combine all the singlestep solvers with order <= `order` to use up all the function evaluations (steps).
+                The total number of function evaluations (NFE) == `steps`.
+                Given a fixed NFE == `steps`, the sampling procedure is:
+                    - If `order` == 1:
+                        - Denote K = steps. We use K steps of DPM-Solver-1 (i.e. DDIM).
+                    - If `order` == 2:
+                        - Denote K = (steps // 2) + (steps % 2). We take K intermediate time steps for sampling.
+                        - If steps % 2 == 0, we use K steps of singlestep DPM-Solver-2.
+                        - If steps % 2 == 1, we use (K - 1) steps of singlestep DPM-Solver-2 and 1 step of DPM-Solver-1.
+                    - If `order` == 3:
+                        - Denote K = (steps // 3 + 1). We take K intermediate time steps for sampling.
+                        - If steps % 3 == 0, we use (K - 2) steps of singlestep DPM-Solver-3, and 1 step of singlestep DPM-Solver-2 and 1 step of DPM-Solver-1.
+                        - If steps % 3 == 1, we use (K - 1) steps of singlestep DPM-Solver-3 and 1 step of DPM-Solver-1.
+                        - If steps % 3 == 2, we use (K - 1) steps of singlestep DPM-Solver-3 and 1 step of singlestep DPM-Solver-2.
+            - 'multistep':
+                Multistep DPM-Solver with the order of `order`. The total number of function evaluations (NFE) == `steps`.
+                We initialize the first `order` values by lower order multistep solvers.
+                Given a fixed NFE == `steps`, the sampling procedure is:
+                    Denote K = steps.
+                    - If `order` == 1:
+                        - We use K steps of DPM-Solver-1 (i.e. DDIM).
+                    - If `order` == 2:
+                        - We firstly use 1 step of DPM-Solver-1, then use (K - 1) step of multistep DPM-Solver-2.
+                    - If `order` == 3:
+                        - We firstly use 1 step of DPM-Solver-1, then 1 step of multistep DPM-Solver-2, then (K - 2) step of multistep DPM-Solver-3.
+            - 'singlestep_fixed':
+                Fixed order singlestep DPM-Solver (i.e. DPM-Solver-1 or singlestep DPM-Solver-2 or singlestep DPM-Solver-3).
+                We use singlestep DPM-Solver-`order` for `order`=1 or 2 or 3, with total [`steps` // `order`] * `order` NFE.
+            - 'adaptive':
+                Adaptive step size DPM-Solver (i.e. "DPM-Solver-12" and "DPM-Solver-23" in the paper).
+                We ignore `steps` and use adaptive step size DPM-Solver with a higher order of `order`.
+                You can adjust the absolute tolerance `atol` and the relative tolerance `rtol` to balance the computatation costs
+                (NFE) and the sample quality.
+                    - If `order` == 2, we use DPM-Solver-12 which combines DPM-Solver-1 and singlestep DPM-Solver-2.
+                    - If `order` == 3, we use DPM-Solver-23 which combines singlestep DPM-Solver-2 and singlestep DPM-Solver-3.
+
+        =====================================================
+
+        Some advices for choosing the algorithm:
+            - For **unconditional sampling** or **guided sampling with small guidance scale** by DPMs:
+                Use singlestep DPM-Solver or DPM-Solver++ ("DPM-Solver-fast" in the paper) with `order = 3`.
+                e.g., DPM-Solver:
+                    >>> dpm_solver = DPM_Solver(model_fn, noise_schedule, algorithm_type="dpmsolver")
+                    >>> x_sample = dpm_solver.sample(x, steps=steps, t_start=t_start, t_end=t_end, order=3,
+                            skip_type='time_uniform', method='singlestep')
+                e.g., DPM-Solver++:
+                    >>> dpm_solver = DPM_Solver(model_fn, noise_schedule, algorithm_type="dpmsolver++")
+                    >>> x_sample = dpm_solver.sample(x, steps=steps, t_start=t_start, t_end=t_end, order=3,
+                            skip_type='time_uniform', method='singlestep')
+            - For **guided sampling with large guidance scale** by DPMs:
+                Use multistep DPM-Solver with `algorithm_type="dpmsolver++"` and `order = 2`.
+                e.g.
+                    >>> dpm_solver = DPM_Solver(model_fn, noise_schedule, algorithm_type="dpmsolver++")
+                    >>> x_sample = dpm_solver.sample(x, steps=steps, t_start=t_start, t_end=t_end, order=2,
+                            skip_type='time_uniform', method='multistep')
+
+        We support three types of `skip_type`:
+            - 'logSNR': uniform logSNR for the time steps. **Recommended for low-resolutional images**
+            - 'time_uniform': uniform time for the time steps. **Recommended for high-resolutional images**.
+            - 'time_quadratic': quadratic time for the time steps.
+
+        =====================================================
+        Args:
+            x: A pytorch tensor. The initial value at time `t_start`
+                e.g. if `t_start` == T, then `x` is a sample from the standard normal distribution.
+            steps: A `int`. The total number of function evaluations (NFE).
+            t_start: A `float`. The starting time of the sampling.
+                If `T` is None, we use self.noise_schedule.T (default is 1.0).
+            t_end: A `float`. The ending time of the sampling.
+                If `t_end` is None, we use 1. / self.noise_schedule.total_N.
+                e.g. if total_N == 1000, we have `t_end` == 1e-3.
+                For discrete-time DPMs:
+                    - We recommend `t_end` == 1. / self.noise_schedule.total_N.
+                For continuous-time DPMs:
+                    - We recommend `t_end` == 1e-3 when `steps` <= 15; and `t_end` == 1e-4 when `steps` > 15.
+            order: A `int`. The order of DPM-Solver.
+            skip_type: A `str`. The type for the spacing of the time steps. 'time_uniform' or 'logSNR' or 'time_quadratic'.
+            method: A `str`. The method for sampling. 'singlestep' or 'multistep' or 'singlestep_fixed' or 'adaptive'.
+            denoise_to_zero: A `bool`. Whether to denoise to time 0 at the final step.
+                Default is `False`. If `denoise_to_zero` is `True`, the total NFE is (`steps` + 1).
+
+                This trick is firstly proposed by DDPM (https://arxiv.org/abs/2006.11239) and
+                score_sde (https://arxiv.org/abs/2011.13456). Such trick can improve the FID
+                for diffusion models sampling by diffusion SDEs for low-resolutional images
+                (such as CIFAR-10). However, we observed that such trick does not matter for
+                high-resolutional images. As it needs an additional NFE, we do not recommend
+                it for high-resolutional images.
+            lower_order_final: A `bool`. Whether to use lower order solvers at the final steps.
+                Only valid for `method=multistep` and `steps < 15`. We empirically find that
+                this trick is a key to stabilizing the sampling by DPM-Solver with very few steps
+                (especially for steps <= 10). So we recommend to set it to be `True`.
+            solver_type: A `str`. The taylor expansion type for the solver. `dpmsolver` or `taylor`. We recommend `dpmsolver`.
+            atol: A `float`. The absolute tolerance of the adaptive step size solver. Valid when `method` == 'adaptive'.
+            rtol: A `float`. The relative tolerance of the adaptive step size solver. Valid when `method` == 'adaptive'.
+            return_intermediate: A `bool`. Whether to save the xt at each step.
+                When set to `True`, method returns a tuple (x0, intermediates); when set to False, method returns only x0.
+        Returns:
+            x_end: A pytorch tensor. The approximated solution at time `t_end`.
+
+        """
+        t_0 = 1. / self.noise_schedule.total_N if t_end is None else t_end
+        t_T = self.noise_schedule.T if t_start is None else t_start
+        assert t_0 > 0 and t_T > 0, "Time range needs to be greater than 0. For discrete-time DPMs, it needs to be in [1 / N, 1], where N is the length of betas array"
+        if return_intermediate:
+            assert method in ['multistep', 'singlestep', 'singlestep_fixed'], "Cannot use adaptive solver when saving intermediate values"
+        if self.correcting_xt_fn is not None:
+            assert method in ['multistep', 'singlestep', 'singlestep_fixed'], "Cannot use adaptive solver when correcting_xt_fn is not None"
+        device = x.device
+        intermediates = []
+        with torch.no_grad():
+            if method == 'adaptive':
+                x = self.dpm_solver_adaptive(x, order=order, t_T=t_T, t_0=t_0, atol=atol, rtol=rtol, solver_type=solver_type)
+            elif method == 'multistep':
+                assert steps >= order
+                timesteps = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=steps, device=device)
+                assert timesteps.shape[0] - 1 == steps
+                # Init the initial values.
+                step = 0
+                t = timesteps[step]
+                t_prev_list = [t]
+
+                x = self.model_fn(x, t)
+                x = torch.where(input_ids_mask==0, x_start, x)
+
+                model_prev_list = [x]
+                if self.correcting_xt_fn is not None:
+                    x = self.correcting_xt_fn(x, t, step)
+                if return_intermediate:
+                    intermediates.append(x)
+                # Init the first `order` values by lower order multistep DPM-Solver.
+                for step in range(1, order):
+                    t = timesteps[step]
+                    x = self.multistep_dpm_solver_update(x, model_prev_list, t_prev_list, t, step, solver_type=solver_type)
+                    x = torch.where(input_ids_mask==0, x_start, x)
+                    if self.correcting_xt_fn is not None:
+                        x = self.correcting_xt_fn(x, t, step)
+                    if return_intermediate:
+                        intermediates.append(x)
+                    t_prev_list.append(t)
+                    x = self.model_fn(x, t)
+                    x = torch.where(input_ids_mask==0, x_start, x)
+                    model_prev_list.append(x)
+                # Compute the remaining values by `order`-torch order multistep DPM-Solver.
+                for step in range(order, steps + 1):
+                    t = timesteps[step]
+                    # We only use lower order for steps < 10
+                    if lower_order_final and steps < 10:
+                        step_order = min(order, steps + 1 - step)
+                    else:
+                        step_order = order
+                    x = self.multistep_dpm_solver_update(x, model_prev_list, t_prev_list, t, step_order, solver_type=solver_type)
+                    x = torch.where(input_ids_mask==0, x_start, x)
+                    if self.correcting_xt_fn is not None:
+                        x = self.correcting_xt_fn(x, t, step)
+                    if return_intermediate:
+                        intermediates.append(x)
+                    for i in range(order - 1):
+                        t_prev_list[i] = t_prev_list[i + 1]
+                        model_prev_list[i] = model_prev_list[i + 1]
+                    t_prev_list[-1] = t
+                    # We do not need to evaluate the final model value.
+                    if step < steps:
+                        x = self.model_fn(x, t)
+                        x = torch.where(input_ids_mask==0, x_start, x)
+                        model_prev_list[-1] = x
+            elif method in ['singlestep', 'singlestep_fixed']:
+                if method == 'singlestep':
+                    timesteps_outer, orders = self.get_orders_and_timesteps_for_singlestep_solver(steps=steps, order=order, skip_type=skip_type, t_T=t_T, t_0=t_0, device=device)
+                elif method == 'singlestep_fixed':
+                    K = steps // order
+                    orders = [order,] * K
+                    timesteps_outer = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=K, device=device)
+                for step, order in enumerate(orders):
+                    s, t = timesteps_outer[step], timesteps_outer[step + 1]
+                    timesteps_inner = self.get_time_steps(skip_type=skip_type, t_T=s.item(), t_0=t.item(), N=order, device=device)
+                    lambda_inner = self.noise_schedule.marginal_lambda(timesteps_inner)
+                    h = lambda_inner[-1] - lambda_inner[0]
+                    r1 = None if order <= 1 else (lambda_inner[1] - lambda_inner[0]) / h
+                    r2 = None if order <= 2 else (lambda_inner[2] - lambda_inner[0]) / h
+                    x = self.singlestep_dpm_solver_update(x, s, t, order, solver_type=solver_type, r1=r1, r2=r2)
+                    if self.correcting_xt_fn is not None:
+                        x = self.correcting_xt_fn(x, t, step)
+                    if return_intermediate:
+                        intermediates.append(x)
+            else:
+                raise ValueError("Got wrong method {}".format(method))
+            if denoise_to_zero:
+                t = torch.ones((1,)).to(device) * t_0
+                x = self.denoise_to_zero_fn(x, t)
+                if self.correcting_xt_fn is not None:
+                    x = self.correcting_xt_fn(x, t, step + 1)
+                if return_intermediate:
+                    intermediates.append(x)
+        if return_intermediate:
+            return x, intermediates
+        else:
+            return x
+
+
+
+#############################################################
+# other utility functions
+#############################################################
+
+def interpolate_fn(x, xp, yp):
+    """
+    A piecewise linear function y = f(x), using xp and yp as keypoints.
+    We implement f(x) in a differentiable way (i.e. applicable for autograd).
+    The function f(x) is well-defined for all x-axis. (For x beyond the bounds of xp, we use the outmost points of xp to define the linear function.)
+
+    Args:
+        x: PyTorch tensor with shape [N, C], where N is the batch size, C is the number of channels (we use C = 1 for DPM-Solver).
+        xp: PyTorch tensor with shape [C, K], where K is the number of keypoints.
+        yp: PyTorch tensor with shape [C, K].
+    Returns:
+        The function values f(x), with shape [N, C].
+    """
+    N, K = x.shape[0], xp.shape[1]
+    all_x = torch.cat([x.unsqueeze(2), xp.unsqueeze(0).repeat((N, 1, 1))], dim=2)
+    sorted_all_x, x_indices = torch.sort(all_x, dim=2)
+    x_idx = torch.argmin(x_indices, dim=2)
+    cand_start_idx = x_idx - 1
+    start_idx = torch.where(
+        torch.eq(x_idx, 0),
+        torch.tensor(1, device=x.device),
+        torch.where(
+            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,
+        ),
+    )
+    end_idx = torch.where(torch.eq(start_idx, cand_start_idx), start_idx + 2, start_idx + 1)
+    start_x = torch.gather(sorted_all_x, dim=2, index=start_idx.unsqueeze(2)).squeeze(2)
+    end_x = torch.gather(sorted_all_x, dim=2, index=end_idx.unsqueeze(2)).squeeze(2)
+    start_idx2 = torch.where(
+        torch.eq(x_idx, 0),
+        torch.tensor(0, device=x.device),
+        torch.where(
+            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,
+        ),
+    )
+    y_positions_expanded = yp.unsqueeze(0).expand(N, -1, -1)
+    start_y = torch.gather(y_positions_expanded, dim=2, index=start_idx2.unsqueeze(2)).squeeze(2)
+    end_y = torch.gather(y_positions_expanded, dim=2, index=(start_idx2 + 1).unsqueeze(2)).squeeze(2)
+    cand = start_y + (x - start_x) * (end_y - start_y) / (end_x - start_x)
+    return cand
+
+
+def expand_dims(v, dims):
+    """
+    Expand the tensor `v` to the dim `dims`.
+
+    Args:
+        `v`: a PyTorch tensor with shape [N].
+        `dim`: a `int`.
+    Returns:
+        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.
+    """
+    return v[(...,) + (None,)*(dims - 1)]
\ No newline at end of file
diff --git a/tools/dot/lib/ema.py b/tools/dot/lib/ema.py
new file mode 100644
index 0000000..0bb31b5
--- /dev/null
+++ b/tools/dot/lib/ema.py
@@ -0,0 +1,46 @@
+import copy
+import torch
+from contextlib import contextmanager
+
+class EMA:
+    """
+    Usage:
+    ema = EMA(my_model, decay=0.999)
+    for i in range(N):
+        ...
+        loss.backward()
+        opt.step()
+        ema.step()
+    with ema.enabled():
+        y_pred = model(X_test)
+    """
+    def __init__(self, module, decay):
+        super().__init__()
+        self.decay = decay
+        if self.decay > 0:
+            self.original = module
+            self.ema = copy.deepcopy(module)
+
+    def _param_pairs(self):
+        original_params = sorted(list(self.original.named_parameters()))
+        ema_params = sorted(list(self.ema.named_parameters()))
+        return [(p1,p2) for (_,p1), (_,p2) in zip(original_params, ema_params)]
+
+    def step(self):
+        if self.decay > 0:
+            for p_orig, p_ema in self._param_pairs():
+                p_ema.data.mul_(self.decay)
+                p_ema.data.add_((1 - self.decay) * p_orig.data)
+
+
+    @contextmanager
+    def enabled(self):
+        if self.decay > 0:
+            prev_orig_params = [p.data.clone() for p,_ in self._param_pairs()]
+            for p_orig, p_ema in self._param_pairs():
+                p_orig.data.copy_(p_ema.data)
+            yield
+            for i, (p_orig, _) in enumerate(self._param_pairs()):
+                p_orig.data.copy_(prev_orig_params[i])
+        else:
+            yield
\ No newline at end of file
diff --git a/tools/dot/lib/models.py b/tools/dot/lib/models.py
new file mode 100644
index 0000000..35daa00
--- /dev/null
+++ b/tools/dot/lib/models.py
@@ -0,0 +1,318 @@
+import apex.normalization
+# try:
+#     import flash_attn.flash_attn_interface
+#     import flash_attn.ops.fused_dense
+#     use_flash_attn = True
+# except:
+import xformers.ops
+    # use_flash_attn = False
+
+import lib.utils
+import mup
+import numpy as np
+import lib.rotary
+import torch
+import torch.utils.checkpoint
+import torch.nn.functional as F
+from einops import rearrange
+from torch import nn, optim
+
+
+class MLP(nn.Module):
+    def __init__(
+        self,
+        in_features,
+        hidden_features=None,
+        out_features=None,
+        bias1=False, 
+        bias2=False
+    ):
+        super().__init__()
+        self.fc1 = nn.Linear(in_features, hidden_features, bias=bias1)
+        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias2)
+
+    def forward(self, inputs: torch.Tensor) -> torch.Tensor:
+        return self.fc2(F.gelu(self.fc1(inputs),approximate="tanh"))
+
+
+class LayerNorm(nn.Module):
+    def __init__(self, dim):
+        super().__init__()
+        self.weight = nn.Parameter(torch.ones([dim]))
+        self.dim = dim
+    def forward(self, x):
+        with torch.cuda.amp.autocast(enabled=False):
+            x = F.layer_norm(x.float(), [self.dim])
+        return x * self.weight[None,None,:]
+
+def residual_linear(x, W, x_skip, residual_scale):
+    """x_skip + residual_scale * W @ x"""
+    dim_out, dim_in = W.shape[0], W.shape[1]
+    return torch.addmm(
+        x_skip.view(-1, dim_out),
+        x.view(-1, dim_in),
+        W.T,
+        alpha=residual_scale
+    ).view(*x.shape[:-1], dim_out)
+
+
+class TransformerBlock(nn.Module):
+    def __init__(self, dim, n_heads, causal, residual_scale):
+        super().__init__()
+
+        self.causal = causal
+        self.dim = dim
+        self.n_heads = n_heads
+        self.residual_scale = residual_scale
+
+        self.rmsnorm1 = apex.normalization.FusedRMSNorm(dim)
+        self.attn_qkv = nn.Linear(dim, 3*dim, bias=False)
+        self.attn_out = nn.Linear(dim, dim, bias=False)
+
+        self.rmsnorm2 = apex.normalization.FusedRMSNorm(dim)
+        # if use_flash_attn:
+        #     self.mlp = flash_attn.ops.fused_dense.FusedMLP(
+        #         dim, 4*dim, bias1=False, bias2=False, checkpoint_lvl=1)
+        # else:
+        self.mlp = MLP(dim, 4*dim, dim, bias1=False, bias2=False)
+
+    def forward(self, x, rotary_cos_sin, cu_seqlens=None, attn_mask=None):
+        batch_size, seq_len = x.shape[0], x.shape[1]
+
+        # Self-attention block
+        x_skip = x
+        x = self.rmsnorm1(x)
+        qkv = self.attn_qkv(x)
+        qkv = rearrange(
+            qkv,
+            'b s (three h d) -> b s three h d',
+            three=3, h=self.n_heads
+        )
+        half_dtype = qkv.dtype
+        with torch.cuda.amp.autocast(enabled=False):
+            cos, sin = rotary_cos_sin
+            qkv = lib.rotary.apply_rotary_pos_emb(
+                qkv, cos.to(half_dtype), sin.to(half_dtype)
+            )
+        # flash attention doesn't support attention mask 
+        # if use_flash_attn:
+        #     qkv = rearrange(qkv, 'b s ... -> (b s) ...')
+        #     if cu_seqlens is None:
+        #         cu_seqlens = torch.arange(
+        #             0, (batch_size + 1) * seq_len, step=seq_len,
+        #             dtype=torch.int32, device=qkv.device
+        #         )
+        #     x = flash_attn.flash_attn_interface.flash_attn_varlen_qkvpacked_func(
+        #         qkv, cu_seqlens, seq_len, 0., causal=self.causal)
+        #     x = rearrange(x, '(b s) h d -> b s (h d)', b=batch_size)
+        # else:
+
+        if attn_mask is None:
+            attn_bias = None
+        else:
+            # b s, ensure memory is aligned by slicing a bigger tensor.
+            round_seq_len = (seq_len//8 + 1)*8
+            attn_bias = torch.zeros((batch_size, round_seq_len), dtype=qkv.dtype)
+            attn_bias[:, :seq_len].masked_fill_(~attn_mask , float("-inf"))
+            # b h s s 
+            attn_bias = attn_bias.unsqueeze(1).unsqueeze(-1).repeat(1, self.n_heads, 1, round_seq_len)
+            attn_bias = attn_bias[:,:,:seq_len,:seq_len]
+
+        x = xformers.ops.memory_efficient_attention(
+                qkv[:,:,0], qkv[:,:,1], qkv[:,:,2], attn_bias=attn_bias
+            )
+        x = rearrange(x, 'b s h d -> b s (h d)', b=batch_size)
+        
+        x = residual_linear(
+            x, self.attn_out.weight, x_skip, self.residual_scale
+        )
+
+        # Feedforward block
+        x_skip = x
+        x = self.rmsnorm2(x)
+        x = self.mlp(x)
+        x = torch.add(x_skip, x, alpha=self.residual_scale)
+
+        return x
+
+class EmbeddingMatrix(nn.Module):
+    def __init__(self, vocab_size, embed_dim):
+        super().__init__()
+        self.matrix = nn.Parameter(torch.randn(vocab_size, embed_dim))
+        self.matrix.data /= self.matrix.data.norm(p=2, dim=1, keepdim=True)
+    def forward(self):
+        norm = torch.linalg.norm(self.matrix, dim=1, keepdim=True)
+        return (self.matrix / (norm + 1e-8))
+
+class NoiseSchedule(nn.Module):
+    def __init__(self):
+        super().__init__()
+        self.W1 = nn.Parameter(torch.randn(1024, 1))
+        self.b1 = nn.Parameter(torch.randn(1024))
+        self.W2 = nn.Parameter(torch.randn(1, 1024))
+    def forward(self, t):
+        """t.shape: [n]"""
+        W1 = F.softplus(self.W1.double())
+        W2 = 0.01 * F.softplus(self.W2.double())
+        def gamma_tilde(t):
+            h = t[:,None] - 0.5
+            h = (h @ W1.T) + self.b1[None,:].double()
+            h = torch.tanh(h)
+            h = (h @ W2.T)[:,0]
+            return h
+        gamma_tilde_0 = gamma_tilde(torch.tensor([0.], device='cuda'))
+        gamma_tilde_1 = gamma_tilde(torch.tensor([1.], device='cuda'))
+        gamma_tilde_t = gamma_tilde(t)
+        return (
+            (gamma_tilde_t - gamma_tilde_0) /
+            (gamma_tilde_1 - gamma_tilde_0)
+        )
+
+class GammaBounds(nn.Module):
+    def __init__(self, gamma_0, gamma_1):
+        super().__init__()
+        self.gamma_0 = nn.Parameter(torch.tensor(float(gamma_0)))
+        self.gamma_1 = nn.Parameter(torch.tensor(float(gamma_1)))
+    def forward(self):
+        return self.gamma_0.clone().double(), self.gamma_1.clone().double()
+
+class DiffusionModel(nn.Module):
+    def __init__(self, dim, embed_dim, n_blocks, n_heads, vocab_size):
+        super().__init__()
+
+        self.input_linear = nn.Linear(embed_dim, dim, bias=False)
+        self.selfcond_linear = nn.Linear(embed_dim, dim, bias=False)
+        self.selfcond_linear.weight.data.zero_()
+        self.gamma_linear = nn.Linear(64, dim, bias=False)
+        self.gamma_linear.weight.data.zero_()
+
+        self.rotary_emb = lib.rotary.Rotary(dim // n_heads)
+
+        residual_scale = float(1./np.sqrt(n_blocks))
+        self.blocks = nn.ModuleList([
+            TransformerBlock(dim, n_heads, False, residual_scale)
+            for i in range(n_blocks)
+        ])
+
+        self.output_norm = lib.models.LayerNorm(dim)
+        self.output_linear = mup.MuReadout(dim, vocab_size)
+        self.output_linear.weight.data.zero_()
+        self.output_linear.bias.data.zero_()
+
+        self.dim = dim
+        self.embed_dim = embed_dim
+        self.vocab_size = vocab_size
+
+    def forward(self, z, gamma, embedding_matrix, bias_scale, x_selfcond,
+        selfcond_mask=None, cu_seqlens=None, attn_mask=None, x_embed=None, src_mask=None):
+
+        if selfcond_mask is None:
+            selfcond_mask = torch.ones(z.shape[0], device='cuda')
+
+        alpha_squared = torch.sigmoid(-gamma)[:,None,None]
+        sigma_squared = torch.sigmoid(gamma)[:,None,None]
+        alpha = alpha_squared.sqrt()
+
+        # Rescale input to stdev 1
+        z_variance = (alpha_squared / self.embed_dim) + sigma_squared
+        x = z / z_variance.sqrt().float()
+
+        # assume src part has already been recovered
+        if x_embed is not None:
+            if src_mask is not None:
+                src_mask = src_mask.unsqueeze(-1) # bs, seq, 1
+                x = torch.where(src_mask, x_embed, x)
+                x_selfcond = torch.where(src_mask, x_embed, x_selfcond)
+            else:
+                x = x_embed
+                x_selfcond = x_embed
+
+        x = self.input_linear(x)
+
+        x = x + self.selfcond_linear(
+            x_selfcond * float(np.sqrt(self.embed_dim))
+        )
+
+        gamma_embed = torch.linspace(-5., 5., 64 // 2, device='cuda')
+        gamma_embed = gamma_embed.exp()[None,:] * gamma[:,None]
+        gamma_embed = torch.cat([gamma_embed.sin(), gamma_embed.cos()], dim=1)
+        gamma_embed = self.gamma_linear(gamma_embed.float())[:,None,:]
+        x = x + gamma_embed
+
+        rotary_cos_sin = self.rotary_emb(x)
+        # with torch.cuda.amp.autocast(dtype=torch.bfloat16):  # v100 not support
+        with torch.cuda.amp.autocast(dtype=torch.float16):
+            for i in range(len(self.blocks)):
+                x = self.blocks[i](x, rotary_cos_sin, cu_seqlens=cu_seqlens, attn_mask=attn_mask)
+
+        x = self.output_norm(x.float())
+
+        x *= self.output_linear.output_mult/self.output_linear.width_mult()
+
+        W = torch.cat([
+            self.output_linear.weight.T,
+            embedding_matrix.T,
+            embedding_matrix.T.detach()
+        ], dim=0)
+        z_scaled_for_bias = bias_scale * (alpha/sigma_squared).float() * z
+        x = torch.cat([
+            x,
+            z_scaled_for_bias * (1 - selfcond_mask.float()[:,None,None]),
+            z_scaled_for_bias * selfcond_mask.float()[:,None,None]
+        ], dim=2)
+        logits = torch.addmm(
+            self.output_linear.bias.view(1, self.vocab_size),
+            x.view(-1, self.dim + 2*self.embed_dim),
+            W.view(self.dim + 2*self.embed_dim, self.vocab_size)
+        ).view(x.shape[0], x.shape[1], self.vocab_size)
+
+        # Comment for 'no categorical reparameterization' ablation
+        x_reconst = F.softmax(logits, dim=2)
+        x_reconst = x_reconst @ torch.cat([
+            embedding_matrix, embedding_matrix.detach()], dim=1)
+        x_reconst = torch.lerp(
+            x_reconst[:,:,:self.embed_dim],
+            x_reconst[:,:,self.embed_dim:],
+            selfcond_mask.float()[:,None,None]
+        )
+        
+        # if x_embed is not None:
+        #     x_reconst = torch.where(src_mask, x_embed, x_reconst)
+        
+        return logits, x_reconst
+
+class AutoregressiveModel(nn.Module):
+    def __init__(self, dim, n_blocks, n_heads, vocab_size, tie_embeddings):
+        super().__init__()
+        self.tie_embeddings = tie_embeddings
+        if not tie_embeddings:
+            self.input_embedding = nn.Embedding(vocab_size, dim)
+        self.rotary_emb = lib.rotary.Rotary(dim // n_heads)
+
+        residual_scale = float(1./np.sqrt(n_blocks))
+        self.blocks = nn.ModuleList([
+            TransformerBlock(dim, n_heads, True, residual_scale)
+            for i in range(n_blocks)
+        ])
+        self.output_norm = apex.normalization.FusedRMSNorm(dim)
+        self.output_linear = mup.MuReadout(dim, vocab_size)
+        self.first_token_logits = nn.Parameter(torch.zeros(vocab_size))
+
+    def forward(self, x):
+        if self.tie_embeddings:
+            x = F.embedding(x, self.output_linear.weight) * float(np.sqrt(3*256))
+        else:
+            x = self.input_embedding(x)
+        rotary_cos_sin = self.rotary_emb(x)
+        # with torch.cuda.amp.autocast(dtype=torch.bfloat16):
+        with torch.cuda.amp.autocast(dtype=torch.float16):
+            for i in range(len(self.blocks)):
+                x = self.blocks[i](x, rotary_cos_sin)
+        x = x.float()
+        x = self.output_norm(x)
+        logits = self.output_linear(x)
+        logits = torch.cat([
+            self.first_token_logits[None,None,:].expand(x.shape[0],-1,-1),
+            logits[:,:-1,:]
+        ], dim=1)
+        return logits
\ No newline at end of file
diff --git a/tools/dot/lib/ops.py b/tools/dot/lib/ops.py
new file mode 100644
index 0000000..95e5c5c
--- /dev/null
+++ b/tools/dot/lib/ops.py
@@ -0,0 +1,24 @@
+"""
+Miscellaneous PyTorch functions
+"""
+
+import torch
+import torch.nn.functional as F
+
+def cross_entropy(logits, targets):
+    # Much faster than F.cross_entropy by avoiding the transpose
+    logprobs = F.log_softmax(logits, dim=2)
+    return -logprobs[
+        torch.arange(logits.shape[0])[:,None],
+        torch.arange(logits.shape[1])[None,:],
+        targets
+    ]
+
+@torch.jit.script
+def gaussian_kl(mu_p, sigma_p, mu_q, sigma_q):
+    """KL(p||q)"""
+    return (
+        sigma_q.log() - sigma_p.log()
+        + (sigma_p**2 + (mu_p - mu_q)**2)/(2*sigma_q**2)
+        - 0.5
+    )
diff --git a/tools/dot/lib/rotary.py b/tools/dot/lib/rotary.py
new file mode 100644
index 0000000..0b9697c
--- /dev/null
+++ b/tools/dot/lib/rotary.py
@@ -0,0 +1,48 @@
+import torch
+from torch import nn
+
+class Rotary(torch.nn.Module):
+    def __init__(self, dim, base=10_000):
+        super().__init__()
+        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
+        self.register_buffer("inv_freq", inv_freq)
+        self.seq_len_cached = None
+        self.cos_cached = None
+        self.sin_cached = None
+
+    def forward(self, x, seq_dim=1):
+        seq_len = x.shape[seq_dim]
+        if seq_len != self.seq_len_cached:
+            self.seq_len_cached = seq_len
+            t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq)
+            freqs = torch.einsum("i,j->ij", t, self.inv_freq.clone())
+            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)
+            # dims are: batch, seq_len, qkv, head, dim
+            self.cos_cached = emb.cos()[None, :, None, None, :].repeat(1,1,3,1,1)
+            self.sin_cached = emb.sin()[None, :, None, None, :].repeat(1,1,3,1,1)
+            # This makes the transformation on v an identity.
+            self.cos_cached[:,:,2,:,:].fill_(1.)
+            self.sin_cached[:,:,2,:,:].fill_(0.)
+
+        return self.cos_cached, self.sin_cached
+
+def rotate_half(x):
+    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]
+    return torch.cat(
+        (-x2, x1), dim=-1
+    )
+
+@torch.jit.script
+def _apply_rotary_pos_emb_torchscript(qkv, cos, sin):
+    return (qkv * cos) + (rotate_half(qkv) * sin)
+
+def apply_rotary_pos_emb(qkv, cos, sin):
+    try:
+        import flash_attn.layers.rotary
+        cos = cos[0,:,0,0,:cos.shape[-1]//2]
+        sin = sin[0,:,0,0,:sin.shape[-1]//2]
+        return flash_attn.layers.rotary.apply_rotary_emb_qkv_(
+            qkv, cos, sin
+        )
+    except:
+        return _apply_rotary_pos_emb_torchscript(qkv, cos, sin)
\ No newline at end of file
diff --git a/tools/dot/lib/scalinglaw_utils.py b/tools/dot/lib/scalinglaw_utils.py
new file mode 100644
index 0000000..31eb440
--- /dev/null
+++ b/tools/dot/lib/scalinglaw_utils.py
@@ -0,0 +1,53 @@
+import numpy as np
+
+SEQ_LEN = 256
+VAL_FREQ = 1000
+
+def params(n_blocks, dim):
+    return (12 * n_blocks * dim**2)# + (dim * 32768)
+
+def flops_per_step(n_blocks, dim, batch_size, model_type, seq_len=SEQ_LEN):
+    flops_per_token = 6 * (params(n_blocks, dim))
+    if model_type == 'autoregressive':
+        # Self-attn FLOPs are halved because of causal mask
+        flops_per_token += (n_blocks * seq_len * dim)
+    elif model_type == 'diffusion':
+        flops_per_token += 2 * (n_blocks * seq_len * dim)
+        # Extra forward pass on 25% of the data for self-conditioning
+        flops_per_token *= (1. + (0.33 * 0.25))
+    else:
+        raise Exception()
+    tokens_per_step = seq_len * batch_size
+    return flops_per_token * tokens_per_step
+
+def chinchilla_tokens_given_params(params):
+    return 8e9 * (params / 400e6)
+
+def chinchilla_params_given_flops(flops):
+    return 400e6 * np.sqrt(flops / 1.92e19)
+
+def chinchilla_tokens_given_flops(flops):
+    return 8e9 * np.sqrt(flops / 1.92e19)
+
+def power_law_fit(x, y):
+    """y = a * x ^ b"""
+    fit = np.polyfit(np.log(x), np.log(y), 1)
+    def fit_fn(x_):
+        return np.exp(np.poly1d(fit)(np.log(x_)))
+    a = np.exp(fit[1])
+    b = fit[0]
+    return a, b, fit_fn
+
+def power_law_plus_constant_fit(x, y):
+    """y = y0 + a*x^b"""
+    best_mse = float('inf')
+    best_fit = None
+    for y0 in np.linspace(0., np.min(y), 10_000):
+        a, b, fit_fn = power_law_fit(x, y - y0)
+        def new_fit_fn(x_, fit_fn_=fit_fn, y0_=y0):
+            return fit_fn_(x_) + y0_
+        mse = np.mean((y - new_fit_fn(x))**2)
+        if mse < best_mse:
+            best_mse = mse
+            best_fit = (a, b, y0, new_fit_fn)
+    return best_fit
\ No newline at end of file
diff --git a/tools/dot/lib/utils.py b/tools/dot/lib/utils.py
new file mode 100644
index 0000000..eea22d0
--- /dev/null
+++ b/tools/dot/lib/utils.py
@@ -0,0 +1,160 @@
+import argparse
+import collections
+import contextlib
+import functools
+import lib.ddp
+import numpy as np
+import time
+import torch
+import types
+import warnings
+from torch import nn, optim
+import logging
+
+class AttributeDict(dict):
+    def __getattr__(self, attr):
+        return self[attr]
+    def __setattr__(self, attr, value):
+        self[attr] = value
+
+def print_args(args, title=None):
+    if isinstance(args, argparse.Namespace):
+        args = vars(args)
+    if title:
+        logging.info(f'{title} args:')
+    else:
+        logging.info('Args:')
+    for k, v in sorted(args.items()):
+        logging.info(f'\t{k}: {v}')
+
+def print_model(model):
+    logging.info('Parameters:')
+    total_params = 0
+    for name, param in model.named_parameters():
+        logging.info(f"\t{name}: {list(param.shape)}, std {param.std()}")
+        if len(list(param.shape)) == 0:
+            total_params += 1
+        else:
+            total_params += functools.reduce(
+                (lambda x,y: x*y), list(param.shape))
+    logging.info(f'Total parameters: {total_params:,}')
+
+def print_tensor(label, tensor):
+    """Print a tensor with a given label."""
+    torch.set_printoptions(precision=3, linewidth=119, sci_mode=False)
+    logging.info(f'{label}:')
+    for line in str(tensor).splitlines():
+        logging.info(f"\t{line}")
+    torch.set_printoptions(profile='default')
+
+def print_row(*row, colwidth=10):
+    """Print a row of values."""
+    def format_val(x):
+        if isinstance(x, torch.Tensor):
+            x = x.item()
+        if np.issubdtype(type(x), np.floating):
+            x = "{:.4f}".format(x)
+        return str(x).ljust(colwidth)[:colwidth]
+    logging.info("  ".join([format_val(x) for x in row]))
+
+def train_loop(
+    forward,
+    opt,
+    steps,
+    names=[],
+    hook=None,
+    print_freq=1000,
+    first_step=0,
+    lr_warmup_steps=0,
+    lr_decay=False,
+    amp_grad_scaler=True,
+    grad_accum_steps=1,
+    ddp_models=[],
+    clip_params=[],
+    clip_quantile=0.95
+    ):
+
+    def lr_fn(step):
+        if (step - first_step) < 10:
+            # Zero LR for the first 10 steps to warm up Adam
+            return 0.
+        elif step < lr_warmup_steps:
+            return float(step) / lr_warmup_steps
+        elif lr_decay:
+            # Linear to zero
+            return 1. - (float(step-lr_warmup_steps) / (1e-8+steps-lr_warmup_steps))
+        else:
+            return 1.
+    scheduler = optim.lr_scheduler.LambdaLR(opt, lr_fn)
+
+    print_row('step', 'step time', 'loss', *names, 'grad norm', 'mem')
+    histories = collections.defaultdict(lambda: [])
+    scaler = torch.cuda.amp.GradScaler(enabled=amp_grad_scaler)
+    start_time = time.time()
+    prev_grad_norms = torch.full([1000], 1e8, device='cuda')
+    for step in range(steps):
+
+        if step < first_step:
+            with warnings.catch_warnings():
+                warnings.simplefilter("ignore")
+                scheduler.step()
+            continue
+
+        for accum_step in range(grad_accum_steps):
+
+            with contextlib.ExitStack() as stack:
+                if accum_step < grad_accum_steps - 1:
+                    for m in ddp_models:
+                        stack.enter_context(m.no_sync())
+                forward_vals = forward(
+                    step,
+                    (accum_step * lib.ddp.world_size()) + lib.ddp.rank(),
+                    lib.ddp.world_size() * grad_accum_steps
+                )
+                if not isinstance(forward_vals, tuple):
+                    forward_vals = (forward_vals,)
+
+                scaled_loss = forward_vals[0] / grad_accum_steps
+                scaler.scale(scaled_loss).backward()
+
+            histories['loss'].append(forward_vals[0].item())
+            for name, val in zip(names, forward_vals[1:]):
+                histories[name].append(val.item())
+
+            del forward_vals
+
+        scaler.unscale_(opt)
+        with torch.no_grad():
+            threshold = torch.quantile(prev_grad_norms, clip_quantile)
+            grad_norm = nn.utils.clip_grad_norm_(clip_params, threshold)
+            histories['grad_norm'].append(grad_norm.item())
+            prev_grad_norms[step % len(prev_grad_norms)] = grad_norm
+        scaler.step(opt)
+        scaler.update()
+        opt.zero_grad(set_to_none=True)
+  
+        with warnings.catch_warnings():
+            warnings.simplefilter("ignore")
+            scheduler.step()
+
+        if (step==0) or (step % print_freq == (print_freq - 1)):
+            means = {
+                name: lib.ddp.reduce_mean(np.mean(histories[name]))
+                for name in histories.keys()
+            }
+            means['step_time'] = (time.time() - start_time) / max(step - first_step, 1)
+            print_row(
+                step,
+                means['step_time'],
+                means['loss'],
+                *[means[name] for name in names],
+                means['grad_norm'],
+                torch.cuda.max_memory_allocated() / (1024**3)
+            )
+            histories.clear()
+
+        if hook is not None:
+            hook(step)
+
+        if step == 0:
+            start_time = time.time()
\ No newline at end of file
