# ğŸ”„ åŠ¨æ€ç¨€ç–åº¦è°ƒåº¦ - ä½¿ç”¨æŒ‡å—

## æ¦‚è¿°

åŠ¨æ€ç¨€ç–åº¦è°ƒåº¦æ˜¯åŸºäº **Cosine Annealing with Warm Restarts (SGDR)** è®ºæ–‡å®ç°çš„ä¸€ç§è‡ªé€‚åº”å‰ªæç­–ç•¥ï¼Œé€šè¿‡å‘¨æœŸæ€§åœ°è°ƒæ•´å‰ªæç¨€ç–åº¦ï¼Œå¢å¼ºè¿›åŒ–ç®—æ³•åœ¨ä¸åŒç¨€ç–ç”Ÿæ€ä½ä¸Šçš„æ¢ç´¢èƒ½åŠ›ã€‚

**è®ºæ–‡**: [SGDR: Stochastic Gradient Descent with Warm Restarts](https://arxiv.org/abs/1608.03983)

**æ ¸å¿ƒä¿®æ”¹**: æˆ‘ä»¬ä½¿ç”¨ `sin(Ï€/2)` æ›¿ä»£åŸæ–‡çš„ä½™å¼¦å‡½æ•°ï¼Œå®ç°ä»ä½ç¨€ç–åº¦åˆ°é«˜ç¨€ç–åº¦çš„ **warm-up** æ•ˆæœã€‚

---

## è®¾è®¡æ€è·¯

### 1. ä¸ºä»€ä¹ˆéœ€è¦åŠ¨æ€ç¨€ç–åº¦ï¼Ÿ

åœ¨ä¼ ç»Ÿçš„å›ºå®šç¨€ç–åº¦å‰ªæä¸­ï¼š
- ç¨€ç–åº¦è®¾ç½®è¿‡é«˜ï¼šå¯èƒ½è¿‡æ—©åœ°ç ´åæœ‰æ½œåŠ›çš„æ¨¡å‹
- ç¨€ç–åº¦è®¾ç½®è¿‡ä½ï¼šæ¢ç´¢ç©ºé—´å—é™ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨ç¨€ç–æ€§ä¼˜åŠ¿

åŠ¨æ€ç¨€ç–åº¦è°ƒåº¦é€šè¿‡**å‘¨æœŸæ€§åœ°å˜åŒ–ç¨€ç–åº¦**ï¼Œè§£å†³è¿™ä¸ªä¸¤éš¾é—®é¢˜ï¼š
- **åˆæœŸä½ç¨€ç–åº¦**ï¼šä¿æŠ¤é«˜fitnessä½†ä½sparsityçš„ä¼˜ç§€ä¸ªä½“
- **é€æ¸å¢åŠ ç¨€ç–åº¦**ï¼šæ¢ç´¢æ›´ç¨€ç–çš„æ¨¡å‹ç©ºé—´
- **å‘¨æœŸæ€§é‡å¯**ï¼šé¿å…é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œå¢åŠ ç§ç¾¤å¤šæ ·æ€§

### 2. æ•°å­¦å…¬å¼

åŠ¨æ€ç¨€ç–åº¦åœ¨ç¬¬ `t` æ¬¡è¿­ä»£æ—¶çš„å€¼ä¸ºï¼š

```
Î·_t = Î·_min + 0.5 Ã— (Î·_max - Î·_min) Ã— (1 + sin(T_cur/T_i Ã— Ï€/2))
```

å…¶ä¸­ï¼š
- `Î·_min`: ç¨€ç–åº¦æœ€å°å€¼ï¼ˆä¾‹å¦‚ 0.1ï¼‰
- `Î·_max`: ç¨€ç–åº¦æœ€å¤§å€¼ï¼ˆä¾‹å¦‚ 0.6ï¼‰
- `T_cur`: å½“å‰å‘¨æœŸå†…å·²ç»è¿‡çš„è¿­ä»£æ¬¡æ•°
- `T_i`: å½“å‰å‘¨æœŸçš„æ€»è¿­ä»£æ¬¡æ•°

æ¯ä¸ªå‘¨æœŸç»“æŸåï¼š
- ç¨€ç–åº¦é‡å¯åˆ° `Î·_min`
- ä¸‹ä¸€å‘¨æœŸé•¿åº¦ `T_i` ä¹˜ä»¥ `T_mult`ï¼ˆä¾‹å¦‚ 2 è¡¨ç¤ºæ¯æ¬¡ç¿»å€ï¼‰

### 3. è°ƒåº¦ç¤ºä¾‹

å‡è®¾é…ç½®ä¸ºï¼š
- `Î·_min = 0.1`
- `Î·_max = 0.6`
- `T_0 = 100`ï¼ˆç¬¬ä¸€å‘¨æœŸ100æ¬¡è¿­ä»£ï¼‰
- `T_mult = 2`ï¼ˆæ¯æ¬¡å‘¨æœŸç¿»å€ï¼‰

ç¨€ç–åº¦å˜åŒ–å¦‚ä¸‹ï¼š

```
è¿­ä»£ 0-99:    å‘¨æœŸ1 (100æ¬¡)  ç¨€ç–åº¦: 0.10 â†’ 0.60
è¿­ä»£ 100-299:  å‘¨æœŸ2 (200æ¬¡)  ç¨€ç–åº¦: 0.10 â†’ 0.60  [é‡å¯!]
è¿­ä»£ 300-699:  å‘¨æœŸ3 (400æ¬¡)  ç¨€ç–åº¦: 0.10 â†’ 0.60  [é‡å¯!]
è¿­ä»£ 700-1499: å‘¨æœŸ4 (800æ¬¡)  ç¨€ç–åº¦: 0.10 â†’ 0.60  [é‡å¯!]
è¿­ä»£ 1500-...: å‘¨æœŸ5 (1600æ¬¡) ç¨€ç–åº¦: 0.10 â†’ 0.60  [é‡å¯!]
```

---

## ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1: ä½¿ç”¨é¢„é…ç½®è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# ç›´æ¥è¿è¡ŒåŠ¨æ€ç¨€ç–åº¦å®éªŒ
bash run_bfcl_dynamic_sparsity.sh
```

### æ–¹æ³•2: è‡ªå®šä¹‰é…ç½®

```bash
export USE_DYNAMIC_SPARSITY=true
export SPARSITY_MIN=0.1
export SPARSITY_MAX=0.6
export SPARSITY_T0=100
export SPARSITY_T_MULT=2

bash scripts/experiments/run_bfcl_single_node.sh
```

### æ–¹æ³•3: å‘½ä»¤è¡Œå‚æ•°

```bash
python main_sparsity_aware.py \
  --model1_path models/Qwen2.5-0.5B-Instruct \
  --model2_path models/Qwen2.5-0.5B-Instruct \
  --pop_size 8 \
  --total_forward_passes 3000 \
  --runs 1 \
  --omega 0.9 \
  --beta 0.1 \
  --use_dynamic_sparsity \
  --sparsity_min 0.1 \
  --sparsity_max 0.6 \
  --sparsity_t0 100 \
  --sparsity_t_mult 2 \
  --eval_subset_size 20 \
  --use_bfcl_eval \
  --bfcl_data_path bfcl/data/bfcl_test_200.json \
  --gsm8k_weight 0.5 \
  --bfcl_weight 0.5 \
  --output_dir results_dynamic
```

---

## å‚æ•°è¯´æ˜

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|-----|------|--------|------|
| `--use_dynamic_sparsity` | flag | False | å¯ç”¨åŠ¨æ€ç¨€ç–åº¦è°ƒåº¦ï¼ˆä¼šè¦†ç›– `--pruning_sparsity`ï¼‰ |
| `--sparsity_min` | float | 0.1 | ç¨€ç–åº¦æœ€å°å€¼ï¼ˆå‘¨æœŸå¼€å§‹æ—¶çš„å€¼ï¼‰ |
| `--sparsity_max` | float | 0.6 | ç¨€ç–åº¦æœ€å¤§å€¼ï¼ˆå‘¨æœŸç»“æŸæ—¶çš„å€¼ï¼‰ |
| `--sparsity_t0` | int | 100 | ç¬¬ä¸€ä¸ªå‘¨æœŸçš„è¿­ä»£æ¬¡æ•° |
| `--sparsity_t_mult` | int | 2 | å‘¨æœŸé•¿åº¦ä¹˜æ•°ï¼ˆ1=å›ºå®šå‘¨æœŸï¼Œ2=æ¯æ¬¡ç¿»å€ï¼‰ |

### å‚æ•°é€‰æ‹©å»ºè®®

#### 1. `sparsity_min` å’Œ `sparsity_max`
- **å°æ¨¡å‹ï¼ˆ<1Bå‚æ•°ï¼‰**: `[0.1, 0.5]`
- **ä¸­ç­‰æ¨¡å‹ï¼ˆ1-7Bå‚æ•°ï¼‰**: `[0.1, 0.6]`
- **å¤§æ¨¡å‹ï¼ˆ>7Bå‚æ•°ï¼‰**: `[0.2, 0.7]`

#### 2. `sparsity_t0`ï¼ˆç¬¬ä¸€å‘¨æœŸé•¿åº¦ï¼‰
- **å¿«é€Ÿæ¢ç´¢**: `50-100` æ¬¡è¿­ä»£
- **å¹³è¡¡æ¨¡å¼**: `100-200` æ¬¡è¿­ä»£
- **ç¨³å®šæ”¶æ•›**: `200-500` æ¬¡è¿­ä»£

å»ºè®®ï¼š`T_0 â‰ˆ total_iterations / 30`

#### 3. `sparsity_t_mult`ï¼ˆå‘¨æœŸä¹˜æ•°ï¼‰
- `T_mult = 1`: å›ºå®šå‘¨æœŸï¼ˆæ¯ä¸ªå‘¨æœŸé•¿åº¦ç›¸åŒï¼‰
  - ä¼˜ç‚¹ï¼šé‡å¯é¢‘ç¹ï¼Œæ¢ç´¢å¤šæ ·
  - ç¼ºç‚¹ï¼šåæœŸå¯èƒ½ä¸ç¨³å®š
- `T_mult = 2`: æŒ‡æ•°å¢é•¿ï¼ˆæ¨èï¼‰
  - ä¼˜ç‚¹ï¼šåˆæœŸå¿«é€Ÿæ¢ç´¢ï¼ŒåæœŸç¨³å®šæ”¶æ•›
  - ç¼ºç‚¹ï¼šåæœŸé‡å¯æ¬¡æ•°å‡å°‘
- `T_mult = 3`: å¿«é€Ÿå¢é•¿
  - ä¼˜ç‚¹ï¼šåæœŸéå¸¸ç¨³å®š
  - ç¼ºç‚¹ï¼šé‡å¯æ¬¡æ•°å¾ˆå°‘

---

## å®éªŒå¯¹æ¯”

### å¯¹æ¯”1: åŠ¨æ€ vs å›ºå®šç¨€ç–åº¦

**å›ºå®šç¨€ç–åº¦ (pruning_sparsity=0.3)**:
```bash
export PRUNING_SPARSITY=0.3
export USE_DYNAMIC_SPARSITY=false
bash scripts/experiments/run_bfcl_single_node.sh
```

**åŠ¨æ€ç¨€ç–åº¦ (0.1â†’0.6)**:
```bash
export USE_DYNAMIC_SPARSITY=true
export SPARSITY_MIN=0.1
export SPARSITY_MAX=0.6
bash scripts/experiments/run_bfcl_single_node.sh
```

### å¯¹æ¯”2: ä¸åŒå‘¨æœŸç­–ç•¥

**å›ºå®šå‘¨æœŸ (T_mult=1)**:
```bash
export SPARSITY_T_MULT=1  # æ¯100æ¬¡è¿­ä»£é‡å¯
```

**æŒ‡æ•°å¢é•¿ (T_mult=2)**:
```bash
export SPARSITY_T_MULT=2  # 100 â†’ 200 â†’ 400 â†’ ...
```

---

## æ—¥å¿—è¾“å‡º

å¯ç”¨åŠ¨æ€ç¨€ç–åº¦åï¼Œæ‚¨ä¼šçœ‹åˆ°å¦‚ä¸‹æ—¥å¿—ï¼š

```
ğŸ”„ Dynamic Sparsity ENABLED: method=wanda
   Sparsity range: [0.10, 0.60]
   First cycle: 100 iterations
   Cycle multiplier: 2x

ğŸ”„ [Iter 0] Dynamic sparsity: 0.1000
ğŸ”„ [Iter 10] Dynamic sparsity: 0.1244
...
ğŸ”„ [Iter 90] Dynamic sparsity: 0.5756
ğŸ”„ [Iter 100] Dynamic sparsity: 0.1000  # é‡å¯ï¼
```

---

## å¸¸è§é—®é¢˜

### Q1: åŠ¨æ€ç¨€ç–åº¦ä¼šè¦†ç›– `--pruning_sparsity` å—ï¼Ÿ

**æ˜¯çš„**ã€‚å½“å¯ç”¨ `--use_dynamic_sparsity` æ—¶ï¼Œ`--pruning_sparsity` å‚æ•°ä¼šè¢«å¿½ç•¥ã€‚

### Q2: å¦‚ä½•é€‰æ‹©åˆé€‚çš„ç¨€ç–åº¦èŒƒå›´ï¼Ÿ

å»ºè®®ä»è¾ƒå¤§èŒƒå›´å¼€å§‹ï¼ˆä¾‹å¦‚ `[0.1, 0.6]`ï¼‰ï¼Œè§‚å¯Ÿå®éªŒç»“æœåå†è°ƒæ•´ï¼š
- å¦‚æœæ¨¡å‹åœ¨é«˜ç¨€ç–åº¦æ—¶è¡¨ç°å´©æºƒï¼Œå‡å° `sparsity_max`
- å¦‚æœä½ç¨€ç–åº¦é˜¶æ®µæ”¶ç›Šä¸æ˜æ˜¾ï¼Œå¢å¤§ `sparsity_min`

### Q3: å‘¨æœŸé•¿åº¦åº”è¯¥å¦‚ä½•è®¾ç½®ï¼Ÿ

ç»éªŒæ³•åˆ™ï¼š
```
T_0 = total_iterations / (10 Ã— log2(total_iterations))
```

ä¾‹å¦‚ï¼š
- 1000æ¬¡è¿­ä»£: `T_0 â‰ˆ 100`
- 3000æ¬¡è¿­ä»£: `T_0 â‰ˆ 200`
- 10000æ¬¡è¿­ä»£: `T_0 â‰ˆ 300`

### Q4: åŠ¨æ€ç¨€ç–åº¦é€‚ç”¨äºæ‰€æœ‰ä»»åŠ¡å—ï¼Ÿ

ä¸ä¸€å®šã€‚å»ºè®®å…ˆåœ¨å°è§„æ¨¡å®éªŒä¸Šå¯¹æ¯”ï¼š
- ä»»åŠ¡å¯¹ç¨€ç–åº¦æ•æ„Ÿï¼šåŠ¨æ€è°ƒåº¦æ•ˆæœå¥½
- ä»»åŠ¡å¯¹ç¨€ç–åº¦ä¸æ•æ„Ÿï¼šå›ºå®šç¨€ç–åº¦å¯èƒ½æ›´ç¨³å®š

---

## å®ç°ç»†èŠ‚

### æ ¸å¿ƒå‡½æ•°

```python
def calculate_dynamic_sparsity(
    current_iteration: int,
    eta_min: float,
    eta_max: float,
    t0: int,
    t_mult: int
) -> float:
    """
    è®¡ç®—å½“å‰è¿­ä»£çš„åŠ¨æ€ç¨€ç–åº¦
    """
    t_i = float(t0)
    t_cur = float(current_iteration)
    
    # æ‰¾åˆ°å½“å‰å‘¨æœŸ
    while t_cur >= t_i:
        t_cur -= t_i
        t_i *= t_mult
    
    # æ­£å¼¦warm-upå…¬å¼
    sparsity_ratio = 0.5 * (1 + math.sin((t_cur / t_i) * math.pi / 2))
    return eta_min + (eta_max - eta_min) * sparsity_ratio
```

### é›†æˆä½ç½®

åœ¨ `natural_niches_sparsity_aware_fn.py` çš„ä¸»å¾ªç¯ä¸­ï¼ˆç¬¬1240-1253è¡Œï¼‰ï¼š

```python
if enable_pruning:
    # åŠ¨æ€è®¡ç®—å½“å‰ç¨€ç–åº¦
    if use_dynamic_sparsity:
        current_pruning_sparsity = calculate_dynamic_sparsity(
            current_iteration=i,
            eta_min=sparsity_min,
            eta_max=sparsity_max,
            t0=sparsity_t0,
            t_mult=sparsity_t_mult
        )
    else:
        current_pruning_sparsity = pruning_sparsity
    
    # ä½¿ç”¨åŠ¨æ€ç¨€ç–åº¦è¿›è¡Œå‰ªæ
    prune_with_wanda(..., current_pruning_sparsity, ...)
```

---

## ç›¸å…³æ–‡ä»¶

- `natural_niches_sparsity_aware_fn.py`: æ ¸å¿ƒå®ç°ï¼ˆç¬¬119-193è¡Œï¼‰
- `main_sparsity_aware.py`: å‘½ä»¤è¡Œå‚æ•°ï¼ˆç¬¬80-90è¡Œï¼‰
- `run_bfcl_dynamic_sparsity.sh`: é¢„é…ç½®è¿è¡Œè„šæœ¬
- `scripts/experiments/run_bfcl_single_node.sh`: é€šç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ”¯æŒåŠ¨æ€ç¨€ç–åº¦ï¼‰

---

## å¼•ç”¨

å¦‚æœæ‚¨åœ¨ç ”ç©¶ä¸­ä½¿ç”¨äº†åŠ¨æ€ç¨€ç–åº¦è°ƒåº¦ï¼Œè¯·å¼•ç”¨ä»¥ä¸‹è®ºæ–‡ï¼š

```bibtex
@inproceedings{loshchilov2017sgdr,
  title={SGDR: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  booktitle={International Conference on Learning Representations},
  year={2017}
}
```

---

**ç¥å®éªŒé¡ºåˆ©ï¼** ğŸš€

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹æ—¥å¿—è¾“å‡ºæˆ–è”ç³»å¼€å‘è€…ã€‚


